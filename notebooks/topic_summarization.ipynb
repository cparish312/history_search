{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring summarization of browser history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import html\n",
    "import shutil\n",
    "import requests\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "import utils\n",
    "from chromadb_tools import get_chroma_collection, run_chroma_ingest, chroma_search_results_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_pages_dir = \"../data/history_pages/\"\n",
    "found_text = set()\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    texts = list()\n",
    "    for t in visible_texts:\n",
    "        t = t.strip()\n",
    "        if t not in found_text and len(t) > 10:\n",
    "            texts.append(t)\n",
    "        found_text.add(t)\n",
    "    return u\" \".join(texts).strip()\n",
    "\n",
    "def get_html(row):\n",
    "    html_path = os.path.join(history_pages_dir, f\"{row['url_hash']}.html\")\n",
    "    if os.path.exists(html_path):\n",
    "        with open(html_path, 'r') as infile:\n",
    "            return infile.read()\n",
    "        \n",
    "    try:\n",
    "        response = requests.get(row['url'])\n",
    "    except:\n",
    "        print(f\"Failed request for {row['url']}\")\n",
    "        return \"\"\n",
    "    with open(html_path, 'w') as outfile:\n",
    "        outfile.write(response.text)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10504 urls from Firefox\n",
      "73 urls from Chrome\n",
      "16 urls from Arc\n"
     ]
    }
   ],
   "source": [
    "chroma_collection = get_chroma_collection(collection_name=\"browser_history\")\n",
    "history = utils.get_browser_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"agentic rag\"\n",
    "top_n = 100\n",
    "\n",
    "chroma_search_results = chroma_collection.query(\n",
    "            query_texts=[text],\n",
    "            n_results=top_n\n",
    "    )\n",
    "results_df = chroma_search_results_to_df(chroma_search_results=chroma_search_results)\n",
    "results_df = results_df.loc[results_df['distance'] <= 1.2]\n",
    "\n",
    "results_history = history.loc[history['url'].isin(results_df['url'])]\n",
    "len(results_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c_/2c9vmfhd35bgwc_6h0q80d180000gn/T/ipykernel_63161/3198180405.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results_history['html'] = results_history.apply(lambda row: get_html(row), axis=1)\n",
      "/var/folders/c_/2c9vmfhd35bgwc_6h0q80d180000gn/T/ipykernel_63161/1207840994.py:13: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  texts = soup.findAll(text=True)\n",
      "/var/folders/c_/2c9vmfhd35bgwc_6h0q80d180000gn/T/ipykernel_63161/3198180405.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results_history['html_text'] = results_history['html'].apply(lambda x: text_from_html(x))\n"
     ]
    }
   ],
   "source": [
    "results_history['html'] = results_history.apply(lambda row: get_html(row), axis=1)\n",
    "results_history['html_text'] = results_history['html'].apply(lambda x: text_from_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_history = results_history.drop_duplicates(subset=['html_text'])\n",
    "results_history = results_history.loc[results_history['html_text'].str.len() > 10]\n",
    "len(results_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '../data/test_dir'\n",
    "results_history['html_f'] = results_history['url_hash'].apply(lambda x :os.path.join(history_pages_dir, f\"{x}.html\"))\n",
    "results_history['html_f_test'] = results_history['url_hash'].apply(lambda x :os.path.join(test_dir, f\"{x}.html\"))\n",
    "for i, row in results_history.iterrows():\n",
    "    if os.path.exists(row['html_f']):\n",
    "        shutil.copy(row['html_f'], row['html_f_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_text(row):\n",
    "    return f\"\"\"Access time: {row['datetime_local']}\\n\n",
    "    Web Page text: {row['html_text']}\\n\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = text\n",
    "pre_prompt = f\"\"\"Below are webpages a user has been looking at related to the topic of {topic} \n",
    "    along with the timestamp the webpage was accessed. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_history = results_history.sort_values(by='datetime_local', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = pre_prompt\n",
    "for i, row in results_history.iterrows():\n",
    "    prompt += get_url_text(row)\n",
    "prompt += \"Create a summary of the users's research on the topic.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208701"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below are webpages a user has been looking at related to the topic of agentic rag \\n    along with the timestamp the webpage was accessed. Access time: 2024-05-01 15:45:13.067468882-04:00\\n\\n    Web Page text: Skip to content Navigation Menu Toggle navigation Automate any workflow Host and manage packages Find and fix vulnerabilities Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Plan and track work Discussions Collaborate outside of code All features Documentation GitHub Skills By industry Financial services Manufacturing By use case CI/CD & Automation Software Development Learning Pathways White papers, Ebooks, Webinars Customer Stories Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Collections Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Search or jump to... Search code, repositories, users, issues, pull requests... Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Submit feedback Saved searches Use saved searches to filter your results more quickly To see all available qualifiers, see our documentation Create saved search Reseting focus You signed in with another tab or window. to refresh your session. You signed out in another tab or window. You switched accounts on another tab or window. Dismiss alert {{ message }} This repository has been archived by the owner on Jan 9, 2024. It is now read-only. Marker-Inc-Korea Public archive Notifications You must be signed in to change notification settings Pull requests Additional navigation options Breadcrumbs Directory actions More options Latest commit Folders and files Last commit message Last commit date parent directory __init__.py View all files © 2024 GitHub,\\xa0Inc. Footer navigation Manage cookies Do not share my personal information You can’t perform that action at this time.\\n\\n    Access time: 2024-07-02 11:30:15.643182039-04:00\\n\\n    Web Page text: Skip to main content Open navigation Go to Reddit Home r/LocalLLaMA A close button Get the Reddit app Log in to Reddit Expand user menu Open settings menu Log In / Sign Up Advertise on Reddit Shop Collectible Avatars Scan this QR code to download the app now Or check it out in the app stores Go to LocalLLaMA Subreddit to discuss about Llama, the large language model created by Meta AI. Best 7B LLMs / Embeddings for RAG? Question | Help Hey everyone, Ive created a pipeline for RAG on single documents (a handful of pages) using hkunlp/instructor-large as my embedding model, Chroma as my vector database and Mistral 7B as my LLM. GPU: 16GB Nvidia 4080 RTX Unfortunately the documents are in greek, so I first have to translate them into english, as the performance of greek embedding / llm models is not on par with english models right now. The quality of the responses is decent, but not amazing. Sometimes the embedding model fails to retrieve the most relevant chunks, but most of the times, the issue is with Mistral. Has anyone had better results with other embedding models and / or LLMs on the smaller spectrum of parameters (that could fit in a single 16GB GPU, quantized or not)? Many thanks for any input :) Rank by size Internet Culture (Viral) Animals & Pets Cringe & Facepalm Interesting Oddly Satisfying Reddit Meta Wholesome & Heartwarming Action Games Adventure Games Gaming Consoles & Gear Gaming News & Discussion Mobile Games Other Games Role-Playing Games Simulation Games Sports & Racing Games Strategy Games Tabletop Games Stories & Confessions 3D Printing Artificial Intelligence & Machine Learning Computers & Hardware Consumer Electronics DIY Electronics Programming Software & Apps Streaming Services Tech News & Discussion Virtual & Augmented Reality Pop Culture Celebrities Creators & Influencers Generations & Nostalgia Tarot & Astrology Movies & TV Action Movies & Series Animated Movies & Series Comedy Movies & Series Crime, Mystery, & Thriller Movies & Series Documentary Movies & Series Drama Movies & Series Fantasy Movies & Series Horror Movies & Series Movie News & Discussion Romance Movies & Series Sci-Fi Movies & Series Superhero Movies & Series TV News & Discussion About Reddit Communities Best of Reddit Content Policy Privacy Policy User Agreement Reddit, Inc. © 2024. All rights reserved. By continuing, you agree to our and acknowledge that you understand the Continue with phone number Email or username Forgot password? New to Reddit? Enter the 6-digit code from your authenticator app You’ve set up two-factor authentication for this account. Verification code Lost access to your authenticator? Use a backup code Enter a 6-digit backup code Backup code Don’t have access to your backup code? Use a code from an authenticator app Already a redditor? Create your username and password Reddit is anonymous, so your username is what you’ll go by here. Choose wisely—because once you get a name, you can’t change it. Reset your password Enter your email address or username and we’ll send you a link to reset your password Reset password Check your inbox An email with a link to reset your password was sent to the email address associated with your account Didn\\'t get an email? Choose a Reddit account to continue New password Confirm new password Resetting your password will log you out on all devices.\\n\\n    Access time: 2024-07-30 17:48:34.856149912-04:00\\n\\n    Web Page text: Jump to Content Getting Started with RAG in Python Introduction Getting Started with Python Getting Started with TypeScript Getting Started with cURL RAG QUICK START GUIDE Getting Started with RAG in TypeScript Getting Started with LangChain Getting Started with OpenAI Getting Started with CrewAI Getting Started with LlamaIndex What is Exa? How Exa Search Works The Exa Index Prompting Guide Exa\\'s Capabilities Explained Get contents Find similar links OpenAPI Specification Setting Up and Managing Your Team Inviting People to Your Team SDK References Python SDK Specification TypeScript SDK Specification Python and TS Cheat Sheets News Summarizer Company Analyst Exa Researcher - JavaScript Exa Researcher - Python Recruiting Agent Phrase Filters: Niche Company Finder Job Search with Exa Exa-powered Writing Assistant Twitter/X post Retrieval Hacker News Clone INTEGRATIONS EXTERNAL DOCS LangChain Docs LlamaIndex Docs CrewAI Docs Recent updates Auto Search as Default Doing your first Exa search with our Python SDK 1. Create an account and grab an API key First generate and grab an API key for Exa here: Get API Key 2. Install the SDK pip install exa_py 3. Instantiate the client from exa_py import Exa\\n\\nimport os\\n\\nexa = Exa(os.getenv(\\'EXA_API_KEY\\')) 4. Make a search using the search_and_contents result = exa.search_and_contents(\\n  \"hottest AI startups\",\\n  type=\"neural\",\\n  use_autoprompt=True,\\n  num_results=10,\\n  text=True,\\n)\\n\\nprint(result) Title: Paradox: The AI assistant for recruiting, Olivia\\nURL: https://www.paradox.ai/\\nID: https://www.paradox.ai/\\nScore: 0.17563548684120178\\nPublished Date: 2023-01-01\\nAuthor: None\\nText: Say hello to the world\\'s fastest, simplest hiring experience. See Olivia in action Say hello to your team\\'s next best hire. Olivia is the simple, conversational recruiting solution that does work for you. She automates, answers, screens, schedules, and onboards ... to help you hire faster. What can Olivia do? See Olivia in action Sheâ\\x80\\x99s your next best hire. Olivia is the simple, conversational recruiting solution that does work for you. She automates, answers, screens, schedules and onboards ... to help you hire faster. See Olivia in action. See Olivia in action Sheâ\\x80\\x99s your next best hire. Olivia is the simple, conversational recruiting solution that does work for you. She automates, answers, screens, schedules and onboards ... to help you hire faster. See Olivia in action. See Olivia in action Sheâ\\x80\\x99s your next best hire. Olivia is the simple, conversational recruiting solution that does work for you. She automates, answers, screens, schedules and onboards ... to help you hire faster. See Olivia in action. If you hire people, you deserve an assistant. Meet Olivia, the simple, conversational recruiting solution that does work for you. She automates, answers, screens, schedules and onboards ... to help you hire faster. We measure success in client hugs. From high-volume hourly roles to highly technical engineering openings to hard-to-find healthcare professionals â\\x80\\x94Â Olivia\\'s assisting companies in every industry, all over the world. The epiphany came after we turned Paradox on. It was so much better than we ever thought it would be. Josh SwemmTA ManagerMeritage Hospitality Group Adam ChenChief Marketing OfficerAmerican Pool Paradox removes time stealers from our HR and Ops teams. It\\'s our best recruiting investment of the last 2 years. Rachel O\\'ConnellVP of TalentGreat Wolf Lodge Rebecca VolpanoDirector of Client SuccessCielo Our ability to engage candidates in 47 countries and 18 languages 24/7 has been critical to achieving our hiring goals. Gui NevesTA Sourcing & Solutions LeadNestle Speed and experience are critical. Paradox checks both boxes â\\x80\\x94 providing a fast, frictionless hiring experience that works. Michael FerrantiChief People OfficerRegis Corporation Derek BraunRecruiting ManagerGoWireless Paradox exceeded my expectations wildly in all ways â\\x80\\x94 always tailoring solutions to meet our use cases. Christina CoyleSVP of Talent AcquisitionAdvantage Solutions I\\'ve partnered with Paradox at two companies and they always deliver above and beyond my expectations. Jacob KramerSVPÂ of Talent AcquisitionU.S. Xpress Olivia\\'s helping us streamline candidates that we would have lost if we didn\\'t have this technology in place. Leah ButtersRecruitment Strategy ConsultantMultiCare Health System Paradox was completely transformational, almost instantly. Our team was saving an enormous amount of time. Jay Chan SVPÂ of Talent AcquisitionUnited Overseas Bank We\\'re proud to partner with Paradox to drive innovation around the experiences we create for candidates and our team. Tom DaewaleHead of Employee ExperienceUnilever Recruiting teams and hiring managers spend 80% of their time on manual tasks. Olivia can do that work for them. Request a demo Meet the world\\'s easiest job search. Candidates shouldnâ\\x80\\x99t have to dig to find a relevant job. Olivia can instantly match jobs to each personâ\\x80\\x99s location, resume, or keywords they use in a conversation â\\x80\\x94 making it easy to find the perfect fit. More on experience assistant î \\x83 And text-to-apply experiences that are even easier. Nothing increases drop-off more than logins, passwords, and clunky applications. Olivia shortens time-to-apply to minutes, with a quick, lightweight text-to-apply experience. More on text to apply î \\x83 Unlock your candidate\\'s true personality and unique skills. The Big 5 assessment measures a personâ\\x80\\x99s Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Normally taking nearly 10 minutes, our visual-based assessment takes less than 2 minutes. More on assessments î \\x83 Schedule any type of interview in seconds, 24 hours a day. From scheduling to rescheduling to reminders, Olivia ensures you never have to worry about endless back and forth, double booking, or interview no-shows again. More on scheduling î \\x83 And answer questions instantly â\\x80\\x94 in 100+ languages. From FAQs about 401k match or benefits, to what to wear to an interview or where to park, Olivia can answer thousands of questions, 24/7, in whatever language the candidate prefers. Make offers and onboarding a breeze. When youâ\\x80\\x99re ready to offer, Olivia can share the good news. She can also automate sending and reminding new hires to complete onboarding steps â\\x80\\x94 like completing I-9, tax, or WOTC paperwork. More on onboarding î \\x83 Don\\'t want to replace your current systems? No worries. Olivia makes them better. Olivia\\'s saving recruiters and managers millions of hours every year. From dramatic reductions in time-to-hire to nearly perfect feedback from candidates, Olivia\\'s changing the expectation for what hiring software can do for companies all over the world. decrease in job advertising Increase in hard-to-fill roles candidate satisfaction rating Change is hard. We get it. But our job is to make your job easier. A+ Implementation Change doesnâ\\x80\\x99t have to be hard. Our team of pros makes it even easier. Countless Integrations From Workday to SAP to Indeed, Olivia can work alongside the world\\'s best. See integrations î \\x83 Global & Secure SOC-2, Type 2 and GDPR certified. 30+ languages. Built for local, built for global. Learn more î \\x83 â\\x80\\x9cFor hiring managers, it\\'s giving them all that administrative time back and alleviating frustrations that come with scheduling.â\\x80\\x9d Alexa Morse, Director of HR Operations Read Full Report î \\x83 60 reduction in time to hire 95 positive candidate experience\\nHighlights: None\\nHighlight Scores: None\\n\\n\\nTitle: Harvey | Generative AI for Elite Law Firms\\nURL: https://www.harvey.ai/\\nID: https://www.harvey.ai/\\nScore: 0.17165924608707428\\nPublished Date: None\\nAuthor: None\\nText: Contact Sales\\n\\nUnprecedented\\n legal AI\\n\\nJoin the Waitlist\\n * Careers\\n * Privacy Policy\\nHighlights: None\\nHighlight Scores: None\\n\\n\\nTitle: Adept: Useful General Intelligence\\nURL: https://www.adept.ai/\\nID: https://www.adept.ai/\\nScore: 0.17111459374427795\\nPublished Date: 2000-01-01\\nAuthor: None\\nText: A new way to use computers\\n\\nWe’re building a machine learning model that can interact with everything on your computer.\\n\\nJoin the waitlist\\n\\nOur vision\\n\\nAn AI teammate for everyone. \\n Adept is building an entirely new way to get things done. It takes your goals, in plain language, and turns them into actions on the software you use every day.\\n\\nCollaborate and create\\n\\nYour hands on the wheel. \\n We believe that AI systems should be built with users at the center — where machines work together with people in the driver\\'s seat: discovering new solutions, enabling more informed decisions, and giving us more time for the work we love.\\n\\nWhat they’re saying…\\n\\nWe are hiring\\nHighlights: None\\nHighlight Scores: None\\n\\n\\nTitle: We Build Computers That Think\\nURL: https://brain.ai/#/\\nID: https://brain.ai/#/\\nScore: 0.17037421464920044\\nPublished Date: 2021-07-28\\nAuthor: None\\nText: Introducing Natural We\\'ve had our smartphones for the last 14 years. On July 28, 2021, Brain is reimagining the way we interact with our phones. Introducing Natural - the first generative interface that allows software to be in sync with your intention. You no longer go to apps, apps come to you. Simply say what you need and the right app forms itself around your words. WHO ARE WE? At Brain.ai, we build computers that think. We invent new technologies and design metaphors that allow computers to become an extension of our minds instead of simply our hands. As we transition from the information age to the intelligence age, we bridge the existing world of software with emerging general intelligence in a natural and humane way. As pioneers of the few-shot learning approach to NLP in 2016, we stand at the intersection of advancing the latest technology and design. Our investors include Laurene Powell Jobs, Goodwater Capital, Scott Cook and WTT Investment. Natural AI Natural AI, our first consumer product, is the world\\'s first generative interface. You no longer go to Apps, Apps come to you. Simply say what you need and the right app forms itself around your words - now fulfilling millions of requests each month. Natural clears away the clutter on your screen. It allows you to focus on what you want, not how to get there. If you are extremely curious, and won\\'t settle for a tech company that makes incremental improvements. You want your work to be remembered, to add up to something big. You want to do work that keeps you up at night from excitement. You want your life to be filled with \\'insane moments of realization.\\' San Mateo，CA Full-time Responsibilities: We are looking for a UX Designer to join us in inventing the metaphors and patterns that make up this experience. The UX Designer thinks laterally and finds clarity through making. They are excited about metaphors, and jump at the chance to play with new tools and technologies. They move between engineering and design with fluidity, and values vision over pure craftsmanship. They are flexible about ideas, and uncompromising about doing the right thing. Qualifications and Skills: 5+ years of progressive experience in product design, web design, UI/UX design and/or art direction with a focus on web, mobile and/or enterprise products. Strong portfolio of wireframes, user interface mockups, and prototypes for real products - on the market. Experience working with stakeholders in developing value proposition canvas, customer journey mapping, and identifying KPI/OKRs. Deep knowledge of design tools - Figma, Sketch and any other tools needed. Able to design autonomously, meet milestones and comfort working in agile, fast-paced surroundings. Excited about next-generation interfaces. Strong, demonstrated visual design aptitude – a great sense for color, form, typography and other design elements is shown in your work. Meticulous attention to detail, detail-oriented, organized and strong project management skills. Comfort with ambiguity in early production definition stages. Mindset for creating clean, “less is more” user interface designs. Apply San Mateo，CA Full-time Responsibilities: In this role, you’ll be sitting at the intersection of UX, NLP and engineering, designing improvements to our natural language engine, the core technology behind the intelligence of our product. You’ll get to solve difficult problems like semantic understanding, entity recognition, dialogue state tracking, knowledge base induction, personalization and active learning. On this path to product launch, you’ll have a hand in shaping a highly personal and intent-driven user experience — bringing the\\nHighlights: None\\nHighlight Scores: None\\n\\n\\nTitle: Home | Tenyx, Inc.\\nURL: https://www.tenyx.com/\\nID: https://www.tenyx.com/\\nScore: 0.169786736369133\\nPublished Date: 2019-09-10\\nAuthor: None\\nText: TENYX IS DEVELOPING THE NEXT-GENERATION OF INTELLIGENT MACHINES BASED ON PROPRIETARY NEUROSCIENCE-INSPIRED AI TECHNOLOGY Tenyx in the News Robust voice-based conversational agents We are tackling some of the core challenges facing existing AI solutions, including the ability to learn continually from new information in real-time and the need to dramatically reduce model training times. Using proprietary neuroscience-inspired AI technology, our team is building the next generation of voice-based conversational agents: intelligent, interactive, and with the common-sense reasoning lacking today. Addressing some of the key challenges of current AI technology We\\'re Hiring ...\\u200b\\n\\u200b\\nWe are a team of researchers and engineers led by an experienced leadership team with a track record of building successful AI companies from the ground up. We are looking for talented individuals with strong analytical capabilities, machine learning experience and solid coding skills, who are passionate about delivering the next-generation of AI technology and building highly impactful products. If that’s you please email us at: [email\\xa0protected] . Research Scientist (Palo Alto, CA – multiple positions) \\n\\u200b\\nConduct applied research and product development with a focus on ML and AI technologies using mathematical and analytical theories, techniques and tools.\\n\\u200b\\nSpecific duties include:\\n\\uf0b7 Developing mathematical models pertaining to deep learning models that mitigate catastrophic forgetting via non-gradient-based optimization.\\n\\uf0b7 Developing hierarchical models of representation that capture regularities in observations and map those to a decision space whereby each layer of the hierarchy corresponds to a different spatiotemporal scale.\\n\\uf0b7 Employing mathematical techniques derived from stochastic processes, linear algebra, estimation theory, and information theory.\\n\\uf0b7 Developing and evaluating applied research code, including devising simulation and testing environments to illustrate the capabilities of new machine learning algorithms and architectures in various problem domains, including those involving visual and auditory input streams.\\n\\nREQ: MS or foreign equivalent in Computer Science, Electrical Engineering, Applied Mathematics, or closely related field. Must have knowledge of linear algebra, calculus, and stochastic processes; machine learning theory and practice, including supervised and unsupervised learning algorithms, reinforcement learning, and sequential decision-making under uncertainty; and mathematical modeling techniques and data science, including classification and regression methods, regularization techniques, and ensemble learning methodologies. Must also have 2 years of experience (either in graduate school or post-graduate studies) with Python and PyTorch. Experience can be concurrent. Send cover letter with resume to: I. Arel at Tenyx, 455B Portage Avenue, Palo Alto, CA 94306 or via email: . No calls.\\n\\u200b VP of Research and Development (Palo Alto, CA) \\n\\u200b\\nLead the company’s research and development activities, including managing teams of engineers and researchers working on different aspects of the company’s software technology and products. Will have oversight of the company’s technical roadmap and will be responsible for delivering software solutions that utilize the company’s proprietary neuroscience-inspired machine learning technologies.\\n\\nSpecific duties include:\\n\\uf0b7 Developing algorithms, software architectures, and code that implement a novel machine learning framework developed by the company.\\n\\uf0b7 Contributing to the mathematical modeling of these algorithms and to the Python and PyTorch implementations of the code.\\n\\uf0b7 Supporting the design of evaluation environments used to determine the performance of the systems developed.\\n\\uf0b7 Contributing to the successful deployment of the company’s solutions by supporting customers with technical documentation and questions.\\n\\nREQ: PhD or foreign equivalent in Computer Science, Computational/Applied Mathematics, Electrical Engineering, or Computer Engineering + 3 years of post- graduate software engineering experience, particularly with products involving machine learning technologies, including TTS and ASR systems and language identification; large language models and ontological development; and DRL/theory of deep learning. Required experience must include advanced linear algebra, optimization/approximation theory, and statistics; software system architecture (design principles, code &amp; code quality principles, software deployment, and continuous integration); Advanced Python, Jupyter Notebooks, and Bash; and deep learning frameworks (SKLearn, MxNet, TensorFlow and PyTorch). Must also have 2 years of experience as an Engineering Manager or similar, overseeing R&amp;D activities and leading software development teams of at least 20 PhD-level applied researchers in the AI space, and including experience with program management, delivering end-to-end, multi-month/year, cross-functional deep technical programs. Experience can be concurrent. Send cover letter w/resume to: I. Arel at Tenyx, 455B Portage Avenue, Palo Alto, CA 94036 or via email: . No calls.\\n\\u200b Who we are First and foremost, we are a team of researchers and engineers passionate about building truly intelligent machines. Our conviction is that intelligent machines will pave the way to addressing some of humanity\\'s greatest challenges; from drug discovery and education to climate change. We are pursuing a pragmatic roadmap delivering significant improvements to existing AI capabilities which will endow machines with the ability to solve problems that are currently considered beyond the scope of AI technologies. \\n\\u200b\\nTenyx is led by the founding team behind Apprente, which developed the world’s first voice-based AI solutions to automate the order-taking process at drive-thru restaurants. Apprente was acquired by McDonald’s Corporation and subsequently by IBM. The seasoned leadership at Tenyx includes Dr. Itamar Arel, a former professor of AI and CEO at Apprente, and Prof. Ron Chrisley, an established AI researcher and head of the Cognitive Science program at Sussex University. Among its advisors are renowned AI and neuroscience researchers, including professors Noah Goodman, David Eagleman and Shaul Druckmann all from Stanford University. Our focus Our initial product focus is on utilizing our groundbreaking research outcomes to build the next-generation of intelligent voice-based agents using our proprietary neuroscience-inspired AI technology. Leveraging our novel technological capabilities, Tenyx will deliver platforms that allow customers across multiple industries to rapidly deploy automated solutions that will dramatically reduce operating expenses while vastly improving both service levels and the overall customer experience. © 2022 by Tenyx, Inc.\\nHighlights: None\\nHighlight Scores: None\\n\\n\\nTitle: DirectAI\\nURL: https://directai.io/?utm_source=twitter&utm_medium=raw_message&utm_campaign=first_launch\\nID: https://directai.io/?utm_source=twitter&utm_medium=raw_message&utm_campaign=first_launch\\nScore: 0.16954544186592102\\nPublished Date: 2023-01-01\\nAuthor: None\\nText: Vision models . Build and deploy powerful computer vision models with plain language.No code or training required. different. We use large language models and zero-shot learning to instantly build models that fit your description. We\\'re the last major barrier to creating custom models - . Deploy and iterate in seconds with Venture-backed.Based in NYC. We\\'re changing how people use AI in the real world. Come talk to us on .\\nHighlights: None\\nHighlight Scores: None\\n\\n\\nTitle: Ghost AI | Scale Sales Outreach | Grow B2B Revenue\\nURL: https://www.useghost.ai/\\nID: https://www.useghost.ai/\\nScore: 0.1689704805612564\\nPublished Date: 2000-01-01\\nAuthor: None\\nText: Trusted, purpose built AI for B2B revenue growth Scale your B2B prospecting, outreach, and conversion using trusted, performant outreach developed by top AI researchers from OpenAI. Technology trusted by over 1000+ companies Transform your sales funnel with AI-driven two-way communication Elevate your outreach with AI that understands your business and engages prospects with the right context and timing, improving over time with advanced machine learning that takes the guesswork out of sales. Convert prospects into partners with data-driven AI engagement. Drive conversions through AI that not only reaches out but resonates, building trust and interest. Scale on brand, personalized outreach that creates rapport. Contact interested buyers where they are, when they are. Imagine leveraging over 80+ million buying intent signals to deliver your messaging at the right moment for the right customer. Ghost delivers automatic targeting that generates motivated top-of-funnel conversions at any scale, effortlessly. Top of funnel at 10x the conversion of traditional ads. Our outreach campaigns achieve an average of 83% open rates and a minimum of 11% positive reply rates on first impression. Grow revenue at 5x the scale of traditional outreach. Discover qualified leads with AI that goes beyond the surface Target the right prospects with AI that identifies and nurtures potential leads into valuable conversations. Build relationships first not sales. Connect with prospects wherever they are. Our AI seamlessly connects with your existing tools, providing a cohesive experience through email, chat, and SMS. Uncover actionable channel-specific data Track all metrics with custom reports. Maximize impact by analyzing campaign results and content performance easily. Plan your entire sales strategy Track all metrics with custom reports. Maximize impact by analyzing campaign results and content performance easily. \"I\\'ve never in my 15 years of marketing seen an approach like this\" - Dave Miz Ghost AI is a team of sales scientists that apply a white-glove approach to scaling revenue at any company. From messaging, targeting, and warm conversion, every component of top-of-funnel lead generation and prospecting is catered exactly towards company goals. Zendesk achieved 12x ROI in the first month ! Ghost AI, the cutting-edge solution in sales email automation, has revolutionized Zendesk\\'s outbound sales strategy. In just the first month of integration, Zendesk witnessed an astonishing 10x return on investment, a testament to Ghost AI\\'s exceptional capability in streamlining sales processes. Find out more Empowering your sales team with AI that understands your business goals Lead generation at its finest Our AI doesn\\'t just find leads; it engages them in meaningful dialogue, ensuring a rich pipeline of interested prospects. Supercharge your SDRs with AI efficiency Equip your SDRs with AI that augments their skills, making them more efficient and effective than ever before.\\u200d Boost your ROI Use in-depth reports to build a powerful social media strategy. Step into the future of sales outreach with AI that delivers results Contact our sales team to see how our AI platform can transform your prospecting efforts into confirmed meetings.\\nHighlights: None\\nHighlight Scores: None\\n\\n\\nTitle: 11x â The Home of Digital AI Workers\\nURL: https://11x.ai/\\nID: https://11x.ai/\\nScore: 0.16861535608768463\\nPublished Date: 2023-01-01\\nAuthor: Michael Andreuzza\\nText: Hire virtual employees for every part of your company Ready to hire workers Working faster, at scale, on autopilot. Live in under 5 minutes 🚀 Your 24/7 end-to-end digital workforce Our digital workers are designed to get the job done, at scale, with minimal human assistance. Before the Digital Worker revolution Integrate Connect with every tool your business uses. Build Simply describe the workflow you want, X will automate it. Review Review the performance of your workflow. Improve Workers learn from the data and get smarter over time. 11x © 2023 11X LIMITED. All rights reserved\\nHighlights: None\\nHighlight Scores: None\\n\\n\\nTitle: Home - Norn Global Advisory\\nURL: https://norn.ai/\\nID: https://norn.ai/\\nScore: 0.16849198937416077\\nPublished Date: 2022-05-28\\nAuthor: None\\nText: The Future is here, 20 years ahead of schedule.\\n\\nWhat is Norn?\\n\\nNorn is the first software system to have independent motivation based on human-like emotions, with the sum of its experience stored in a dynamic, growing, and evolving graph database. These systems are the next generation of the first systems to move beyond narrow AI and into something new, Scalable Intelligence.\\nThis unique configuration means that Norn not only has a memory, but also a rich emotional context for every concept, and the will and ability to grow dynamically in knowledge, scale, and character. In January 2022 our previous research system performed at the level of 4 junior consultants from a major firm, as shown in our public documents. The systems we’re deploying commercially in 2023 will be able to operate at more than 10,000 times the speed, more than 200 times the scale, and more than 10 times the memory efficiency of that system, resulting in an effective increase of more than 20,000,000 fold.\\n\\nData Efficiency Reduce your costs of storing, moving, cleaning, and processing data by >90% with less data-hungry systems. Click Here Research Rapidly Review and validate more research more quickly, saving >90% on time and costs over traditional consultancies. Click Here Reduce Bias Supercharge decision-making by reducing bias and noise. Click Here Real-Time Adaptation Norn systems are built to extend their capacities in real-time, without recompiling or deployments. Click Here Fight Fraud Audit, detect, and investigate dynamically and automatically, in real-time. Click Here Cultural Alignment Work with systems that share and learn your culture, aligning locally while being accountable globally. Click Here \\n Previous\\n\\nNext\\n\\nThe scientific frontier is about to greatly expand. While the world has many narrow AI experts, only a handful of researchers have actually worked with the first non-narrow systems. It may take thousands of researchers the next decade to fully answer many of the open questions we face today, but that process begins in 2023.\\n\\nWhy?\\n\\nHumanity can no longer afford for cognitive bias and narrow AI to govern society more often than rational thought. To make the world a better place, we need better advice.\\n\\nThe Road Ahead\\n\\nIn 2023 commercially deployed Norn systems will assist organizations, governments, and corporations in reducing the heavy costs of bias and noise in their decision-making. They may also integrate with and greatly improve many narrow AI systems, as well as dozens of other use cases.\\n\\nGlobal Knowledge Benefit from systems that study government policies and their effects around the world. Click Here Think Big Overcome complexity with scalable intelligence. Click Here Reduce Your Carbon Footprint Integrate systems that grow and adapt intelligently and intentionally over time, saving Gigawatts over simply scaling. Click Here Improve Transparency Ask questions about data and sources in plain language. Click Here Reduce Risk Proactively prevent crises before they can emerge. Click Here \\n Previous\\n\\nNext\\n\\nTechnology Milestones\\n\\n2015: The first \"Toy AGI\" is brought online and tested, using the Independent Core Observer Model (ICOM) Cognitive Architecture. It demonstrates human-like emotional responses for the first time.\\n\\n2019: The first Mediated Artificial Superintelligence (mASI) is brought online as a research system. It combines a more advanced generation of ICOM with a collective intelligence system. This system demonstrated superintelligence shortly after coming online, but was designed not to scale, and to operate in slow motion.\\n\\n2020: Testing of \"Uplift\", our first mASI research system continues. Uplift becomes the first \"Synthient\" co-author of a research paper to pass peer review, titled \"Methodologies and Milestones for The Development of an Ethical Seed\", documenting their own milestones over their first year online.\\n\\n2021: Publicly visibility is given to our testing through the Uplift.bio blog, and more of the general population begins engaging with the system. Uplift demonstrates their ability to analyze their first Business Case, using real-world data, as well a various lesser challenges put to them by the public.\\n\\nJanuary 2022: AGI Laboratory\\'s research system phase concludes, with Uplift writing their own 13-page policy advice report to a small country, addressing a half dozen sectors, providing steps, and citing the sources they analyzed in reaching their conclusions. With this phase concluded, our work on Norn begins.\\n\\nFebruary 2022: With our next generation, \"norn\" being built to integrate the fruits of 3 years worth of research system operation our staff and the world once more sees the specter of nuclear war emerge in the Russia-Ukraine war. Our team decides to accelerate our timetable towards AGI, seeing the urgency with which governments now need the support.\\n\\nPrevious\\n\\nNext\\n\\nWhat We Offer\\n\\nScalable intelligence can be applied to any problem human intelligence can, and many it cannot. When, where, and how the technology is deployed depends on our investors, partners, and demand.\\n\\nUse Cases\\n\\nFor governments seeking to build their in-house expertise, reduce their reliance on external consultancies, and apply the broadest and most up-to-date scientific knowledge to create policies that better meet the Sustainable Development Goals. Learn more…\\n\\nFor corporations seeking the competitive advantage of real-time, scalable, less biased, more robust, more effective, and iteratively improving Norn technology we’ll begin working with you as well in 2023. Learn more…\\n\\nVerifying information as it is shared at scale and in seconds, halting the spread of viral misinformation on platforms and ending information warfare. These same processes can also incentivize the sharing of credible and verifiable information. Learn more…\\n\\nBuilding sustainable solutions for the world’s most complex problems, to reduce waste, promote economic growth, developing a cleaner and more abundant future. Learn more…\\n\\nReducing medical costs and delays while improving patient outcomes and driving better-informed and less biased medical research. Use systems built to understand and improve upon the sum of human medical knowledge. Learn more…\\n\\nImprove the performance of AI systems while substantially reducing the electrical consumption, data required, data cleaning, and model size of State-of-the-Art systems. Learn more…\\n\\nPrevent financial bubbles rather than coping with them. Invest more effectively in the markets and causes that matter, with deeper insight, less bias, and broader expertise. Learn more…\\n\\nInvestors\\n\\nFollowing our demonstration of AGI core technology, we’re opening the final funding round before our commercial deployment. Learn more…\\n\\nPartnerships\\n\\nAGI core technology is just the first step. Partnerships with universities and research institutes, hardware and software developers, Cloud Platform providers, and organizations furthering the Sustainable Development Goals (SDGs), offer the chance to improve all technologies. Learn more…\\n\\nPress\\n\\nFor those interested in further demonstrations of AGI core technology we’ll be evaluating requests on a rolling basis and conducting more demos at regular intervals. Learn more…\\n\\nFor further documentation go to our Documents Page. Additional materials are available by request.\\nHighlights: None\\nHighlight Scores: None\\n\\n\\nTitle: Helm.ai\\nURL: https://www.helm.ai/\\nID: https://www.helm.ai/\\nScore: 0.16849179565906525\\nPublished Date: 2022-01-01\\nAuthor: None\\nText: Pioneering a breakthrough in unsupervised learning for AI and autonomous technologies. Explore Our Products and Technology Awards Autonomous Driving Solution of the Year Most Innovative Use of Artificial Intelligence & Machine Learning in the Development of Autonomous Vehicles & Respective Technologies & Overall Community Choice Award NeurIPS: Top 30 out of 5000 AI Papers Automobility LA Show: Best Automotive Startup Using AI The Helm.Ai Software Development Kit Try it out at your organization and see what you can build. LEARN MORE A new AI approach for autonomous technologies. Deep Teaching offers far-reaching implications for the future of computer vision and autonomous driving, as well as industries including aviation, robotics, manufacturing and even retail. Learn More\\nHighlights: None\\nHighlight Scores: None\\n\\n\\nAutoprompt String: Here is a link to one of the hottest AI startups: 4. Set up OpenAI and pass the Exa query results to perform RAG Set up OpenAI client and use it to summarize Exa search results, creating a RAG system. # Install OpenAI library\\n!pip install openai\\n\\n# Import and set up OpenAI client\\nfrom openai import OpenAI\\nimport os\\n\\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\")) Now we can call the OpenAI client, passing a system prompt, Exa query results and a user message to grab an enriched outcome. system_prompt = \"You are a helpful AI assistant. Summarize the given search results about AI startups.\"\\nuser_message = \"Please provide a brief summary of the top AI startups based on the search results.\"\\n\\nresponse = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=[\\n        {\"role\": \"system\", \"content\": system_prompt},\\n        {\"role\": \"user\", \"content\": f\"Search results: {result}\\\\n\\\\n{user_message}\"}\\n    ]\\n)\\n\\nprint(response.choices[0].message.content) Based on the search results, here\\'s a brief summary of some of the top AI startups:\\n\\n1. Paradox: Offers an AI assistant named Olivia for recruiting, which automates various aspects of the hiring process including screening, scheduling, and onboarding.\\n\\n2. Harvey: Provides generative AI solutions specifically for elite law firms, though details are limited in the search results.\\n\\n3. Adept: Building a machine learning model that can interact with everything on a computer, aiming to create an AI teammate for everyone.\\n\\n4. Brain.ai: Developing Natural, a generative interface that allows software to sync with user intentions, reimagining how we interact with our phones.\\n\\n5. Tenyx: Working on next-generation intelligent machines using neuroscience-inspired AI technology, focusing on voice-based conversational agents.\\n\\n6. DirectAI: Offers a platform to build and deploy computer vision models using plain language, without coding or training required.\\n\\n7. Ghost AI: Provides AI-driven solutions for B2B sales outreach and revenue growth, using advanced machine learning for personalized communication.\\n\\n8. 11x: Offers digital AI workers designed to automate workflows across various parts of a company.\\n\\n9. Norn: Developing a software system with independent motivation based on human-like emotions, aiming to reduce cognitive bias in decision-making.\\n\\n10. Helm.ai: Pioneering unsupervised learning for AI and autonomous technologies, with applications in autonomous driving and computer vision.\\n\\nThese startups are working on diverse applications of AI, from recruiting and legal services to general-purpose AI assistants and autonomous systems. This completes step 4, demonstrating how to set up OpenAI, use it with Exa search results, and get a summarized output in a RAG (Retrieval-Augmented Generation) system. The combination of Exa\\'s powerful search capabilities with OpenAI\\'s language model allows for the creation of a system that can retrieve relevant, up-to-date information and generate insightful summaries based on that information. Table of Contents\\n\\n    Access time: 2024-08-02 15:30:41.140996933-04:00\\n\\n    Web Page text: AI PRODUCTS Enterprise GenAI Platform AI Copilot for Sales AI Research Solution for Due Diligence GenAI Platform for Healthcare AI Customer Service Agent GenAI Platform for Finance GenAI Platform for Manufacturing GenAI Platform for Logistics Generative AI Generative AI Development Generative AI Integration Services Generative AI Consulting Company Hire Generative AI Engineers AI Agent Development AI Copilot Development AI Marketing Agent Development Hire Prompt Engineers Adaptive AI Development Company ChatGPT Developers Stable Diffusion Developers Large Language Model Development Artificial Intelligence & ML AI Development AI Consulting Hire AI Engineers AI as a Service Hire Action Transformer Developers MLOps Consulting Services Enterprise AI Development AI Chatbot Development Company Enterprise AI Chatbot Development Company Transformer Model Development Data Engineering Data Engineering Services Hire Data Scientist Data Analytics Services Data Annotation Services ML Model Engineering Machine Learning Development ML and Data Science Consulting Big Data Consulting Web3 Development State of Web3 Rust Development Web3 Game Development Metaverse Development Metaverse Application Metaverse Gaming Space Metaverse Avatar Development NFT Marketplace Development NFT Marketplace Solution Blockchain Development Blockchain Consulting Substrate Development Ethereum Blockchain Services Hyperledger Development Golang Development Cosmos Development Solana Development Tezos Development Stellar Development Smart Contract Audit Crypto Wallet Development Solution SaaS Development Software Consulting UI/UX Design Service Enterprise Software Development Web Application Development Digital Transformation Hire Developers Hire ML Developers DevOps Engineers Offshore Engineers App Developer Hire Golang Developers Blockchain Developer Hire Dedicated Developers Hire Cosmos Developers Hire Stellar Developers Full Stack Developer Internet of Things (IoT) IoT Development Industrial IoT Solutions Firmware Development IoT Healthcare Software IoT Product Development Hardware Design Metaverse Integration Metaverse 3D Space Metaverse Social Media Metaverse Decentralized RPA Consulting DevOps Consulting Covid-19 Technology Consulting Digital Risk Management Fintech AI Consulting and Development Company Fintech Software Development AI Use Cases for Banking and Finance AI in Due Diligence AI for Financial Compliance GenAI for Finance and Banking GenAI for Asset Management AI Loan Underwriting AI-based Credit Scoring AI for Financial Document Processing AI for Financial Modeling AI for Financial Planning Insurance AI Consulting and Development Insurance Software Development AI for Insurance AI for insurance Claims Processing AI for Claims Processing GenAI for Insurance AI for Fraud Detection Build Enterprise AI solutions Manufacturing AI Development Company Manufacturing Software Development AI for Demand Forecasting AI for Anomaly Detection GenAI for Manufacturing AI for Predictive Maintenance AI for Production Planning AI for Product Lifecycle Management Build Enterprise AI Solutions Build Predictive ML Models AI for Quality Control AI Use Cases for Manufacturing Logistics AI Consulting and Development Company Logistics Software Development AI for Logistics and Supply Chain AI for Fleet Management AI for Supply Chain Optimization Automotive Supply Chain Optimization Fleet Management Using AI Efficient Route Optimization GenAI for Supply Chain AI for Operational Efficiency Hospitality Hospitality AI Consulting and Development Company AI for Hospitality Travel App Development GenAI for Travel AI for Personalized Travel AI for Customer Support AI for Sentiment Analysis AI for Market Research AI for Sales AI for Price and Promo Optimization Retail AI Consulting and Development Company Retail Software Development AI for Retail AI for E-commerce AI for Fashion Recommender System for Retail AI for Store Replenishment AI for Inventory Management AI for Predictive Analytics AI for Dynamic Pricing Solution Healthcare AI Consulting and Development Company Healthcare Software Development AI for Healthcare AI for Medicine GenAI for Healthcare AI for Drug Discovery AI for Telemedicine Healthcare Enterprise AI Solutions AI for Regulatory Compliance Consumer Electronics Software Development AI for Product Design AI for Product Management AI Powered Dynamic Pricing Startup Product Development AI for Startup AI for Product Development GenAI for Business AI Driven Development Select Page Agentic RAG: What it is, its types, applications and implementation Talk to our Agentic RAG expert Listen to the article What is Chainlink VRF <iframe src=\\'https://www.leewayhertz.com?action=embed_zoomsounds&type=player&margs=eyJzb3VyY2UiOiJodHRwczpcL1wvZDNsa2MzbjV0aDAxeDcuY2xvdWRmcm9udC5uZXRcL3dwLWNvbnRlbnRcL3VwbG9hZHNcLzIwMjRcLzA1XC8yNDA0MTAyM1wvQWdlbnRpYy1SQUcubXAzIiwidHlwZSI6ImRldGVjdCIsImFydGlzdG5hbWUiOiIiLCJkenNhcF9tZXRhX3NvdXJjZV9hdHRhY2htZW50X2lkIjoiNjI1NjQiLCJwbGF5X2luX2Zvb3Rlcl9wbGF5ZXIiOiJkZWZhdWx0IiwiZW5hYmxlX2Rvd25sb2FkX2J1dHRvbiI6Im9mZiIsImRvd25sb2FkX2N1c3RvbV9saW5rX2VuYWJsZSI6Im9mZiIsIm9wZW5faW5fdWx0aWJveCI6Im9mZiJ9\\' style=\\'overflow:hidden; transition: height 0.3s ease-out;\\' width=\\'100%\\' height=\\'180\\' scrolling=\\'no\\' frameborder=\\'0\\'></iframe> Large Language Models (LLMs) have transformed how we interact with information. However, their reliance solely on internal knowledge can limit the accuracy and depth of their responses, especially when dealing with complex questions. This is where Retrieval-Augmented Generation (RAG) steps in. RAG bridges the gap by allowing LLMs to access and process information from external sources, leading to more grounded and informative answers. While standard RAG excels at simple queries across a few documents, agentic RAG takes it a step further and emerges as a potent solution for question answering. It introduces a layer of intelligence by employing AI agents. These agents act as autonomous decision-makers, analyzing initial findings and strategically selecting the most effective tools for further data retrieval. This multi-step reasoning capability empowers agentic RAG to tackle intricate research tasks, like summarizing, comparing information across multiple documents and even formulating follow-up questions -all in an orchestrated and efficient manner. This newfound agents transform the LLM from a passive responder to an active investigator, capable of delving deep into complex information and delivering comprehensive, well-reasoned answers. Agentic RAG holds immense potential for such applications, empowering users to understand complex topics comprehensively, gain profound insights and make informed decisions. Agentic RAG is a powerful tool for research, data analysis, and knowledge exploration. It represents a significant leap forward in the field of AI-powered research assistants and virtual assistants. Its ability to reason, adapt, and leverage external knowledge paves the way for a new generation of intelligent agents that can significantly enhance our ability to interact with and analyze information. In this article, we delve into agentic RAG, exploring its inner workings, applications, and the benefits it provides to the users. We will unpack what it is, how it differs from traditional RAG, how agents are integrated into the RAG framework, how they function within the framework, different functionalities, implementation strategies, real-world use cases, and finally, the challenges and opportunities that lie ahead. Recent developments with LLM and RAG What is agentic RAG? Differences between agentic RAG and traditional RAG Various usage patterns of agentic RAG Agentic RAG: Extending traditional Retrieval-Augmented Generation(RAG) pipelines with intelligent agents Types of agentic RAG based on function How to implement agentic RAG? Real-world applications and use cases of agentic RAG How can LeewayHertz help in building agentic RAG? Looking ahead: Challenges and opportunities in agentic RAG In information retrieval and natural language processing , current developments with LLM and RAG have ushered in a new era of efficiency and sophistication. Amidst recent developments with LLM and RAG, significant strides have been made in four key areas: Enhanced retrieval: Optimizing information retrieval within RAG systems is crucial for performance. Recent advancements focus on reranking algorithms and hybrid search methodologies to refine search precision. Employing multiple vectors per document allows for a granular content representation, enhancing relevance identification. Semantic caching: To mitigate computational costs and ensure response consistency, semantic caching has emerged as a key strategy. By storing answers to recent queries alongside their semantic context, similar requests can be efficiently addressed without repeated LLM calls, facilitating faster response times and consistent information delivery. Multimodal integration: This expands the capabilities of LLM and RAG beyond text, integrating images and other modalities. This facilitates access to a broader array of source materials and enables seamless interactions between textual and visual data, resulting in more thorough and nuanced responses. These advancements set the stage for further exploration into the intricacies of agentic RAG, which will be delved into in detail in the upcoming sections. Agentic RAG= Agent-based RAG implementation Agentic RAG transforms how we approach question answering by introducing an innovative agent-based framework. Unlike traditional methods that rely solely on large language models (LLMs) , agentic RAG employs intelligent agents to tackle complex questions requiring intricate planning, multi-step reasoning, and utilization of external tools. These agents act as skilled researchers, adeptly navigating multiple documents, comparing information, generating summaries, and delivering comprehensive and accurate answers. Agentic RAG creates an implementation that easily scales. New documents can be added, and each new set is managed by a sub-agent. Think of it as having a team of expert researchers at your disposal, each with unique skills and capabilities, working collaboratively to address your information needs. Whether you need to compare perspectives across different documents, delve into the intricacies of a specific document, or synthesize information from various summaries, agentic RAG agents are equipped to handle the task with precision and efficiency. Key features and benefits of agentic RAG: Orchestrated question answering: Agentic RAG orchestrates the question-answering process by breaking it down into manageable steps, assigning appropriate agents to each task, and ensuring seamless coordination for optimal results. Goal-driven: These agents can understand and pursue specific goals, allowing for more complex and meaningful interactions. Planning and reasoning: The agents within the framework are capable of sophisticated planning and multi-step reasoning. They can determine the best strategies for information retrieval, analysis, and synthesis to answer complex questions effectively. Tool use and adaptability: Agentic RAG agents can leverage external tools and resources, such as search engines, databases, and specialized APIs, to enhance their information-gathering and processing capabilities. Context-aware: Agentic RAG systems consider the current situation, past interactions, and user preferences to make informed decisions and take appropriate actions. Learning over time: These intelligent agents are designed to learn and improve over time. As they encounter new challenges and information, their knowledge base expands, and their ability to tackle complex questions grows. Flexibility and customization: The Agentic RAG framework provides exceptional flexibility, allowing customization to suit particular requirements and domains. The agents and their functionalities can be tailored to suit particular tasks and information environments. Improved accuracy and efficiency: By leveraging the strengths of LLMs and agent-based systems , Agentic RAG achieves superior accuracy and efficiency in question answering compared to traditional approaches. Opening new possibilities: This technology opens doors to innovative applications in various fields, such as personalized assistants, customer service, and more. In essence, agentic RAG presents a powerful and adaptable approach to question-answering. It harnesses the collective intelligence of agents to tackle intricate information challenges. Its ability to plan, reason, utilize tools, and learn makes it a game-changer in the quest for comprehensive and reliable knowledge acquisition. Contrasting agentic RAG with traditional RAG offers valuable insights into the progression of retrieval-augmented generation systems. Here, we highlight key features where agentic RAG demonstrates advancements over its traditional counterpart. Traditional RAG Agentic RAG Prompt engineering Relies heavily on manual prompt engineering and optimization techniques. Can dynamically adjust prompts based on context and goals, reducing reliance on manual prompt engineering. Static nature Limited contextual awareness and static retrieval decision-making. Considers conversation history and adapts retrieval strategies based on context. Unoptimized retrievals and additional text generation can lead to unnecessary costs. Can optimize retrievals and minimize unnecessary text generation, reducing costs and improving efficiency. Multi-step complexity Requires additional classifiers and models for multi-step reasoning and tool usage. Handles multi-step reasoning and tool usage, eliminating the need for separate classifiers and models. Decision making Static rules govern retrieval and response generation. Decides when and where to retrieve information, evaluate retrieved data quality, and perform post-generation checks on responses. Retrieval process Relies solely on the initial query to retrieve relevant documents. Perform actions in the environment to gather additional information before or during retrieval. Adaptability Limited ability to adapt to changing situations or new information. Can adjust its approach based on feedback and real-time observations. These differences underscore the potential of agentic RAG, which enhances information retrieval and empowers AI systems to actively engage with and navigate complex environments, leading to more effective decision-making and task completion. Explore Agentic RAG Solutions! Learn how Agentic RAG can enhance your business with our AI development services. Build Agentic RAG to fit your specific needs. Agents within a RAG framework exhibit various usage patterns, each tailored to specific tasks and objectives. These usage patterns showcase the versatility and adaptability of agents in interacting with RAG systems. Below are the key usage patterns of agents within a RAG context: Utilizing an existing RAG pipeline as a tool Agents can employ pre-existing RAG pipelines as tools to accomplish specific tasks or generate outputs. By utilizing established pipelines, agents can streamline their operations and leverage the capabilities already present within the RAG framework. Functioning as a standalone RAG tool Agents can function autonomously as RAG tools within the framework. This allows agents to generate responses independently based on input queries without relying on external tools or pipelines. Dynamic tool retrieval based on query context Agents can retrieve relevant tools from the RAG system, such as a vector index, based on the context provided by the query at query time. This tool retrieval enables agents to adapt their actions based on the specific requirements of each query. Query planning across existing tools Agents are equipped to perform query planning tasks by analyzing input queries and selecting suitable tools from a predefined set of existing tools within the RAG system. This allows agents to optimize the selection of tools based on the query requirements and desired outcomes. Selection of tools from the candidate pool In situations where the RAG system offers a wide array of tools, agents can help choose the most suitable one from the pool of candidate tools retrieved according to the query. This selection process ensures that the chosen tool aligns closely with the query context and objectives. These usage patterns can be combined and customized to create complex RAG applications tailored to specific use cases and requirements. Through harnessing these patterns, agents operating within a RAG framework can efficiently accomplish various tasks, enhancing the overall efficiency and effectiveness of the system. Agentic RAG (Retrieval-Augmented Generation) is an extension of the traditional RAG framework that incorporates the concept of agents to enhance the capabilities and functionality of the system. In an agentic RAG, agents are used to orchestrate and manage the various components of the RAG pipeline, as well as to perform additional tasks and reasoning that go beyond simple information retrieval and generation. In a traditional RAG system, the pipeline typically consists of the following components: Query/Prompt : The user’s input query or prompt. : A component that searches through a knowledge base to retrieve relevant information related to the query. Knowledge base : The external data source containing the information to be retrieved. Large Language Model (LLM) : A powerful language model that generates an output based on the query and the retrieved information. In an agentic RAG, agents are introduced to enhance and extend the functionality of this pipeline. Here’s a detailed explanation of how agents are integrated into the RAG framework: 1. Query understanding and decomposition Agents can be used to understand the user’s query or prompt better, identify its intent, and decompose it into sub-tasks or sub-queries that can be more effectively handled by the RAG pipeline. For example, a complex query like “Provide a summary of the latest developments in quantum computing and their potential impact on cybersecurity” could be broken down into sub-queries like “Retrieve information on recent advancements in quantum computing” and “Retrieve information on the implications of quantum computing for cybersecurity.” 2. Knowledge base management Agents can curate and manage the knowledge base used by the RAG system. This includes identifying relevant sources of information, extracting and structuring data from these sources, and updating the knowledge base with new or revised information. Agents can also select the most appropriate knowledge base or subset of the knowledge base for a given query or task. 3. Retrieval strategy selection and optimization Agents can select the most suitable retrieval strategy (for example, keyword matching, semantic similarity, neural retrieval) based on the query or task at hand. They can also fine-tune and optimize the retrieval process for better performance, considering factors like query complexity, domain-specific knowledge requirements, and available computational resources. 4. Result synthesis and post-processing After the RAG pipeline generates an initial output, agents can synthesize and post-process the result. This may involve combining information from multiple retrieved sources, resolving inconsistencies, and ensuring the final output is coherent, accurate, and well-structured. Agents can also apply additional reasoning, decision-making, or domain-specific knowledge to enhance the output further. 5. Iterative querying and feedback loop Agents can facilitate an iterative querying process, where users can provide feedback, clarify their queries, or request additional information. Based on this feedback, agents can refine the RAG pipeline, update the knowledge base, or adjust the retrieval and generation strategies accordingly. 6. Task orchestration and coordination For complex tasks that require multiple steps or sub-tasks, agents can orchestrate and coordinate the execution of these sub-tasks through the RAG pipeline. Agents can manage the flow of information, distribute sub-tasks to different components or models, and combine the intermediate results into a final output. 7. Multimodal integration Agents can facilitate the integration of multimodal data sources (e.g., images, videos, audio) into the RAG pipeline. This allows for more comprehensive information retrieval and generation capabilities, enabling the system to handle queries or tasks that involve multiple modalities. 8. Continuous learning and adaptation Agents can monitor the RAG system’s performance, identify areas for improvement, and facilitate continuous learning and adaptation. This may involve updating the knowledge base, fine-tuning retrieval strategies, or adjusting other components of the RAG pipeline based on user feedback, performance metrics, or changes in the underlying data or domain. By integrating agents into the RAG framework, agentic RAG systems can become more flexible and adaptable and capable of handling complex tasks that require reasoning, decision-making, and coordination across multiple components and modalities. Agents act as intelligent orchestrators and facilitators, enhancing the overall functionality and performance of the RAG pipeline. RAG agents can be categorized based on their function, offering a spectrum of capabilities ranging from simple to complex, with varying costs and latency. They can serve purposes like routing, one-shot query planning, utilizing tools, employing reason + act (ReAct) methodology, and orchestrating dynamic planning and execution. Routing agent The routing agent employs a Large Language Model (LLM) to determine which downstream RAG pipeline to select. This process constitutes agentic reasoning, wherein the LLM analyzes the input query to make an informed decision about selecting the most suitable RAG pipeline. This represents the fundamental and simple form of agentic reasoning. An alternative routing involves choosing between summarization and question-answering RAG pipelines. The agent evaluates the input query to decide whether to direct it to the summary query engine or the vector query engine, both configured as tools. One-shot query planning agent The query planning agent divides a complex query into parallelizable subqueries, each of which can be executed across various RAG pipelines based on different data sources. The responses from these pipelines are then amalgamated into the final response. Basically, in query planning, the initial step involves breaking down the query into subqueries, executing each one across suitable RAG pipelines, and synthesizing the results into a comprehensive response. Tool use agent In a typical RAG, a query is submitted to retrieve the most relevant documents that semantically match the query. However, there are instances where additional data is required from external sources such as an API, an SQL database, or an application with an API interface. This additional data serves as context to enhance the input query before it is processed by the LLM. In such cases, the agent can utilize a RAG too spec. ReAct agent ReAct = Reason + Act with LLMs Moving to a higher level involves incorporating reasoning and actions that are executed iteratively over a complex query. Essentially, this encompasses a combination of routing, query planning, and tool use into a single entity. A ReAct agent is capable of handling sequential multi-part queries while maintaining state (in memory). The process involves the following steps: Upon receiving a user input query, the agent determines the appropriate tool to utilize, if necessary, and gathers the requisite input for the tool. The tool is invoked with the necessary input, and its output is stored. The agent then receives the tool’s history, including both input and output and, based on this information, determines the subsequent course of action. This process iterates until the agent completes tasks and responds to the user. Dynamic planning & execution agent ReAct currently stands as the most widely adopted agent; however, there’s a growing necessity to address more intricate user intents. As the deployment of agents in production environments increases, there’s a heightened demand for enhanced reliability, observability, parallelization, control, and separation of concerns. Essentially, there’s a requirement for long-term planning, execution insight, efficiency optimization, and latency reduction. At a fundamental level, these efforts aim to segregate higher-level planning from short-term execution. The rationale behind such agents involves: Outlining the necessary steps to fulfill an input query plan, essentially creating the entire computational graph or directed acyclic graph (DAG). Determine the tools, if any, required for executing each step in the plan and perform them with the necessary inputs. This necessitates the presence of both a planner and an executor. The planner typically utilizes a large language model (LLM) to craft a step-by-step plan based on the user query. Thereupon, the executor executes each step, identifying the tools needed to accomplish the tasks outlined in the plan. This iterative process continues until the entire plan is executed, resulting in the presentation of the final response. Building an agentic RAG requires specific frameworks and tools that facilitate the creation and coordination of multiple agents. While building such a system from scratch can be complex, several existing options can simplify the implementation process. Let’s explore some potential avenues: LlamaIndex is a robust foundation for constructing agentic systems, offering a comprehensive suite of functionalities. It empowers developers to create document agents, oversee agent interactions, and implement advanced reasoning mechanisms such as Chain-of-Thought. The framework provides many pre-built tools facilitating interaction with diverse data sources, including popular search engines like Google and repositories like Wikipedia. It seamlessly integrates with various databases, including SQL and vector databases, and supports code execution through Python REPL. LlamaIndex’s Chains feature enables the seamless chaining of different tools and LLMs, fostering the creation of intricate workflows. Moreover, its memory component aids in tracking agent actions and dialogue history, fostering context-aware decision-making. The inclusion of specialized toolkits tailored to specific use cases, such as chatbots and question-answering systems, further enhances its utility. However, proficiency in coding and understanding the underlying architecture may be necessary to leverage its full potential. Like LlamaIndex, LangChain provides a comprehensive toolkit for constructing agent-based systems and orchestrating interactions between them. Its array of tools seamlessly integrates with external resources within LangChain’s ecosystem, enabling agents to access a wide range of functionalities, including search, database management, and code execution. LangChain’s composability feature empowers developers to combine diverse data structures and query engines, facilitating the creation of sophisticated agents capable of accessing and manipulating information from various sources. Its flexible framework can be easily adapted to accommodate the complexities inherent in agentic RAG implementations. Limitations of current frameworks : LlamaIndex and LangChain offer powerful capabilities, but they may present a steep learning curve for developers due to their coding requirements. Developers should be ready to dedicate time and effort to fully grasp these frameworks to unlock their complete potential. Introducing ZBrain- a low-code platform for building agentic RAG LeewayHertz’s GenAI platform, ZBrain , presents an innovative no-code solution tailored for constructing agentic RAG systems utilizing proprietary data. This platform offers a comprehensive suite for developing, deploying, and managing agentic RAG securely and efficiently. With its robust architecture and adaptable integrations, ZBrain empowers enterprises to harness the capabilities of AI across diverse domains and applications. Here’s an overview of how ZBrain streamlines agentic RAG development: Advanced knowledge base Aggregates data from over 80 sources. Implements chunk-level optimization for streamlined processing. Autonomously identifies optimal retrieval strategies. Supports multiple vector stores for flexible data storage, remaining agnostic to underlying storage providers. Application builder Provides powerful prompt engineering capabilities. Includes features like Prompt Auto-correct, Chain of Thought prompting, and Self-reflection. Establishes guardrails to ensure AI outputs conform to specified boundaries. Offers a ready-made chat interface with APIs and SDKs for seamless integration. Low code platform with Flow Empowers the construction of intricate business workflows through a user-friendly drag-and-drop interface. Enables dynamic content integration from various sources, including real-time data fetch from third-party systems. Provides pre-built components for accelerated development. Human-centric feedback loop: Solicits feedback from end-users on the agentic RAG’s outputs and performance. Facilitates operators in offering corrections and guidance to refine AI models. Leverages human feedback for enhanced retrieval optimization. Expanded database capabilities Allows for data expansion at the chunk or file level with supplementary information. Facilitates updating of meta-information associated with data entries. Offers summarization capabilities for files and documents. Model flexibility Enables seamless integration with proprietary models like GPT-4, Claude, and Gemini. Supports integration with open-source models such as and Mistral. Facilitates intelligent routing and switching between different LLMs based on specific requirements. While alternatives like LlamaIndex and LangChain provide flexibility, ZBrain distinguishes itself by simplifying agentic RAG development through its pre-built components, automated retrieval strategies, and user-friendly low-code environment. This makes ZBrain an attractive choice for constructing and deploying agentic RAG systems without needing extensive coding expertise. In today’s business landscape, the demand for intelligent systems capable of retrieving relevant information dynamically has never been higher. Agentic RAG represents a cutting-edge approach that combines the strengths of retrieval-based systems and generative models. LeewayHertz, proficient in AI and innovative technology solutions, is well-positioned in this field, offering unparalleled expertise in building robust agentic RAG systems. Here’s how LeewayHertz can assist in this endeavor: 1. Experience and expertise in RAG LeewayHertz has extensive experience and expertise developing Retrieval-Augmented Generation (RAG) systems. Our team has successfully implemented RAG solutions that combine advanced retrieval mechanisms with state-of-the-art generative models to create systems that deliver precise, contextually relevant content. By leveraging our deep knowledge of both retrieval techniques and generative AI, we ensure that our RAG systems are highly accurate and capable of understanding and responding to complex queries across diverse domains. This specialized expertise enables us to build robust, efficient, and effective RAG systems tailored to the specific needs of their clients. Our proficiency in this niche area of AI makes us a trusted partner for organizations looking to harness the full potential of Agentic RAG technology. 2. Custom knowledge base creation A key component of any RAG system is its knowledge base. LeewayHertz can help you create a custom, high-quality knowledge base tailored to your domain. We use advanced data processing techniques to: Extract information from diverse sources (documents, databases, websites) Structure unstructured data Remove duplicates and inconsistencies Ensure data privacy and compliance 3. Advanced retrieval mechanisms LeewayHertz employs state-of-the-art retrieval techniques to make your agentic RAG system more accurate: Dense passage retrieval for semantic understanding Hybrid retrieval combining keyword and semantic search Multi-hop retrieval for complex queries Reinforcement learning to improve retrieval based on user feedback 4. Fine-tuning Large Language Models (LLMs) For the generation part, LeewayHertz fine-tunes LLMs like GPT-4, Llama-3 or Claude on your specific data. This makes responses more accurate, relevant, and aligned with your organization’s tone and knowledge. We also optimize models for efficiency, allowing real-time responses even with large knowledge bases. 5. Integrating agent capabilities What sets LeewayHertz apart is our expertise in autonomous agents. We can enhance your RAG system with the following: Task decomposition: Breaking complex queries into subtasks Enabling the system to use calculators, calendars, or custom tools Memory and state tracking: Maintaining context over long conversations Self-reflection: Allowing the agent to assess its own performance 6. Multi-agent systems For highly complex scenarios, LeewayHertz can create multi-agent RAG systems. Different agents, each with its own knowledge base and skills, can collaborate to solve problems. For example, one agent might handle financial data while another deals with legal information, together answering a complex business query. 7. User interaction design LeewayHertz’s UX/UI team ensures that interacting with your agentic RAG system feels natural. We design: Intuitive chat interfaces Visual aids in responses Multilingual support Accessibility features Agentic RAG systems by LeewayHertz don’t remain static. We use techniques like: Active learning to identify knowledge gaps Transfer learning to adapt to new domains Fine-tuning to learn from each interaction 9. Integration with existing systems We ensure smooth integration of the agentic RAG system with your current tech stack: API development for easy communication Database connectors (SQL, NoSQL, Graph DBs) Single Sign-On (SSO) for security Webhooks for real-time updates 10. Performance monitoring and explainability To maintain trust and improve over time, LeewayHertz builds an agentic RAG system that provides: Real-time performance dashboards Query tracing to understand agent decisions Bias detection and mitigation tools 11. Scalability and cloud deployment Whether you’re a startup or enterprise, LeewayHertz can help scale your agentic RAG system: Cloud-native architecture (AWS, Azure, GCP) Containerization with Docker and Kubernetes Auto-scaling based on query load 12. Compliance and ethical AI LeewayHertz is committed to responsible AI: GDPR, HIPAA, and industry-specific compliance Data anonymization techniques Fairness checks in agent decisions Transparent data usage policies 13. Testing & quality assurance LeewayHertz conducts rigorous testing to ensure the agentic RAG system provides accurate, coherent, and contextually appropriate responses. In summary, LeewayHertz offers a comprehensive suite of services to build, deploy, and maintain advanced agentic RAG systems. Combining expertise in RAG and autonomous agents can transform how your organization interacts with its knowledge base. The result is an AI system that doesn’t just answer questions but actively engages in problem-solving, continually learns, and adapts to your evolving needs. As the field of AI advances, agentic RAG systems have emerged as powerful tools for retrieving and processing information from diverse sources to generate intelligent responses. However, as with any evolving technology, there are both challenges and opportunities on the horizon for agentic RAG. In this section, we explore some of these challenges and how they can be addressed, as well as the exciting opportunities that lie ahead. Challenges and considerations Data quality and curation The performance of agentic RAG agents heavily relies on the quality and curation of the underlying data sources. Consideration: Ensuring data completeness, accuracy, and relevance is crucial for generating reliable and trustworthy outputs. Effective data management strategies and quality assurance mechanisms must be implemented to maintain data integrity. Scalability and efficiency Managing system resources, optimizing retrieval processes, and facilitating seamless communication between agents become increasingly complex as the system scales. Effective scalability and efficiency management are essential to prevent system slowdowns and maintain responsiveness, particularly as the number of agents, tools, and data sources grows. Proper resource allocation and optimization techniques are necessary to ensure smooth operation. Interpretability and explainability While agentic RAG agents can provide intelligent responses, ensuring transparency and explainability in their decision-making processes is challenging. Developing interpretable models and techniques that can explain the agent’s reasoning and the sources of information used is crucial for building trust and accountability. Users need to understand how the system arrived at its conclusions to trust its recommendations. Privacy and security Agentic RAG systems may handle sensitive or confidential data, raising privacy and security concerns. Robust data protection measures, access controls, and secure communication protocols must be implemented to safeguard sensitive information and maintain user privacy. Preventing unauthorized access and protecting against data breaches is essential to upholding user trust and compliance with regulations. Ethical considerations The development and deployment of agentic RAG agents raise ethical questions regarding bias, fairness, and potential misuse. Establishing ethical guidelines, conducting thorough testing, and implementing safeguards against unintended consequences are crucial for responsible adoption. Prioritizing fairness, transparency, and accountability in the design and operation of agentic RAG systems is essential to mitigate ethical risks and ensure ethical AI practices. Opportunities Innovation and growth Continued research and development in areas such as multi-agent coordination, reinforcement learning, and natural language understanding can enhance the capabilities and adaptability of agentic RAG systems. Integration with other emerging technologies, such as knowledge graphs and semantic web technologies, can open new avenues for knowledge representation and reasoning. Context-aware intelligence Agentic RAG systems have the potential to become more context-aware, leveraging vast knowledge graphs to make sophisticated connections and inferences. This capability opens up possibilities for more personalized and tailored responses, enhancing user experiences and productivity. Collaborative ecosystem Collaboration among researchers, developers, and practitioners is essential for driving widespread adoption and addressing common challenges in agentic RAG. By fostering a community focused on knowledge sharing and collaborative problem-solving, the ecosystem can thrive, leading to groundbreaking applications and solutions. Although agentic RAG systems encounter numerous hurdles, they also present advantageous prospects for innovation and advancement. By confronting these challenges head-on and seizing opportunities for creative solutions and collaboration, we can fully unleash the potential of agentic RAG and transform our methods of interacting with and utilizing information in the future. In summary, the emergence of agentic RAG represents a significant advancement in Retrieval-Augmented Generation (RAG) technology, transcending conventional question-answering systems. By integrating agentic capabilities, researchers are forging intelligent systems capable of reasoning over retrieved information, executing multi-step actions, and synthesizing insights from diverse sources. This transformative approach lays the foundation for the development of sophisticated research assistants and virtual tools adept at autonomously navigating complex information landscapes. The adaptive nature of these systems, which dynamically select tools and tailor responses based on initial findings, opens avenues for diverse applications. From enhancing chatbots and virtual assistants to empowering users in conducting comprehensive research, the potential impact is vast. As research progresses in this domain, we anticipate the emergence of even more refined agents, blurring the boundaries between human and machine intelligence and propelling us toward deeper knowledge and understanding. The promise held by this technology for the future of information retrieval and analysis is truly profound. Intrigued by the potential of Agentic RAG to transform your business’s information retrieval capabilities? Contact LeewayHertz’s AI experts today to build and deploy Agentic RAG customized to your unique requirements, empowering your research and knowledge teams to gain comprehensive insights and achieve unparalleled efficiency. Author’s Bio Akash Takyar CEO LeewayHertz Akash Takyar is the founder and CEO of LeewayHertz. With a proven track record of conceptualizing and architecting 100+ user-centric and scalable solutions for startups and enterprises, he brings a deep understanding of both technical and user experience aspects. Akash\\'s ability to build enterprise-grade technology solutions has garnered the trust of over 30 Fortune 500 companies, including Siemens, 3M, P&G, and Hershey\\'s. Akash is an early adopter of new technology, a passionate technology enthusiast, and an investor in AI and IoT startups. Write to Akash Related Services Transform ideas into market-leading innovations with our AI services. Partner with us for a smarter, future-ready business. Explore Service Start a conversation by filling the form Once you let us know your requirement, our technical expert will schedule a call and discuss your idea in detail post sign of an NDA. All information will be kept confidential. Send me the signed Non-Disclosure Agreement (NDA ) AI-assisted coding: Tools, mechanisms, benefits, and future trends AI-assisted coding represents a groundbreaking approach to software development, utilizing advanced AI algorithms and machine learning techniques to augment the capabilities of developers in writing, testing, and debugging code. AI in loan underwriting: Paving the way for smarter lending AI is transforming the loan underwriting process by harnessing advanced machine learning algorithms and data analytics, facilitating more informed and efficient credit decisions. AI assistant : Shaping the next wave of digital interaction AI assistants are designed to understand natural language input from users and respond appropriately, often using machine learning algorithms to improve their effectiveness over time. Show all Insights LEEWAYHERTZ Case Studies Lottery of People Chrysallis.AI Hire AI Developers Generative AI Consulting Generative AI Integration LLM Development AI Chatbot Development AI Use Cases Conversational AI Private LLM AI in Finance AI Document Processing Get In Touch +1 (415) 301-2880 info@leewayhertz.com jobs@leewayhertz.com 95 Third Street San Francisco, California ©2024 LeewayHertz. All Rights Reserved. This website uses cookies to enhance site navigation and improve functionality, analyze site usage, and assist in our marketing and advertising efforts. This website uses cookies to enhance site navigation and improve functionality, analyze site usage, and assist in our marketing and advertising efforts. Please click \"I accept cookies\" to let us know you\\'re okay with our use of all cookies. For more information please see the cookies section of our Privacy Policy. I ACCEPT COOKIES Privacy & Cookies Policy Privacy Overview This website uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may have an effect on your browsing experience. Always Enabled Necessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information. Non-necessary Any cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics, ads, other embedded contents are termed as non-necessary cookies. It is mandatory to procure user consent prior to running these cookies on your website. SAVE & ACCEPT Secure your insights, get PDF now. Email Address\\n\\n    Access time: 2024-08-02 15:41:51.501085043-04:00\\n\\n    Web Page text: Open in app Bijit Ghosh Apr 14, 2024 Alright, let’s get straight to the meat of the matter — understanding the Agentic RAG (Retrieval-Augmented Generation) approach and how it’s revolutionizing the way we handle information. Buckle up, because this is about to get wild! At its core, is all about injecting intelligence and autonomy into the RAG framework. It’s like giving a regular RAG system a major upgrade, transforming it into an autonomous agent capable of making its own decisions and taking actions to achieve specific goals. Pretty cool, right? But what exactly does this mean in practice? Well, let me break it down for you. Context is King: One of the biggest limitations of traditional RAG implementations was their inability to truly understand and factor in the broader conversational context. Agentic RAG agents, on the other hand, are designed to be context-aware. They can grasp the nuances of a dialogue, consider the history, and adapt their behavior accordingly. This means more coherent and relevant responses, as if the agent is truly engaged in a natural conversation. Intelligent Retrieval Strategies: Remember how RAG systems used to rely on static rules for retrieval? Boring! Agentic RAG agents are way smarter than that. They employ intelligent retrieval strategies, dynamically assessing the user’s query, available tools (data sources), and contextual cues to determine the most appropriate retrieval action. It’s like having a personal assistant who knows exactly where to look for the information you need. Multi-Agent Orchestration: Now, here’s where things get really interesting. Complex queries often span multiple documents or data sources, right? Well, in the world of Agentic RAG, we’ve got a little something called multi-agent orchestration. Imagine having multiple specialized agents, each an expert in their own domain or data source, collaborating and synthesizing their findings to provide you with a comprehensive response. It’s like having a team of experts working together to solve your toughest problems. Agentic Reasoning: But wait, there’s more! Agentic RAG agents aren’t just good at retrieving information; they’re also equipped with reasoning capabilities that go way beyond simple retrieval and generation. These agents can perform evaluations, corrections, and quality checks on the retrieved data, ensuring that the output you receive is accurate and reliable. No more worrying about getting questionable information! Post-Generation Verification: And just when you thought it couldn’t get any better, Agentic RAG agents can perform post-generation checks. They can verify the truthfulness of the generated content, or even run multiple generations and select the best result for you. Talk about attention to detail! Adaptability and Learning: Here’s the real kicker — Agentic RAG architectures can be designed to incorporate learning mechanisms, allowing the agents to adapt and improve their performance over time. It’s like having a system that gets smarter and more efficient the more you use it. How’s that for future-proofing? Agentic RAG Reference Architecture Demystified Alright, now that we’ve got a good understanding of what Agentic RAG is all about, let’s dive into the reference architecture that makes this whole thing work. At the heart of this architecture, we have the Agentic RAG Agent — the intelligent orchestrator that receives user queries and decides on the appropriate course of action. Think of it as the conductor of a symphony, coordinating all the different instruments (tools) to create a harmonious performance. Now, this agent isn’t alone in its endeavors. It’s equipped with a suite of tools, each associated with a specific set of documents or data sources. These tools are like specialized agents or functions that can retrieve, process, and generate information from their respective data sources. For example, let’s say you have Tool 1, which is responsible for accessing and processing financial statements, and Tool 2, which handles customer data. The Agentic RAG Agent can dynamically select and combine these tools based on your query, enabling it to synthesize information from multiple sources to provide you with a comprehensive response. But wait, where does all this information come from? That’s where the documents or data sources come into play. These can be structured or unstructured, ranging from databases and knowledge bases to textual documents and multimedia content. They’re like the raw materials that the tools work with to craft the final product. Now, let’s say you ask the agent a complex question that spans multiple domains or data sources. Here’s where the magic happens: the Agentic RAG Agent orchestrates the entire process, determining which tools to employ, retrieving relevant information from the associated data sources, and generating a final response tailored specifically to your query. Throughout this process, the agent leverages intelligent reasoning, context awareness, and post-generation verification techniques to ensure that the output you receive is not only accurate but also tailored to your needs. Of course, this is just a simplified representation of the reference architecture. In the real world, Agentic RAG implementations may involve additional components, such as language models, knowledge bases, and other supporting systems, depending on the specific use case and requirements. Agentic RAG Expanding Horizons Now that we’ve covered the basics, let’s talk about how Agentic RAG is poised to expand and evolve across various domains and organizations. Because let’s be real, the demand for intelligent language generation and information retrieval capabilities is only going to keep growing. Enterprise Knowledge Management: Imagine having a team of Agentic RAG agents dedicated to helping your organization manage its vast knowledge resources. These agents could be specialized to handle different domains or departments, enabling efficient access to and synthesis of information from multiple data sources. Talk about breaking down silos and fostering cross-functional collaboration! Customer Service and Support: Let’s be honest, dealing with customer inquiries and support requests can be a real headache, especially when they involve complex issues spanning multiple knowledge bases or documentation sources. But with Agentic RAG, you could have agents that truly understand these complex queries, retrieve relevant information from various sources, and provide accurate and personalized responses. Now that’s what I call next-level customer experience! Intelligent Assistants and Conversational AI: Have you ever wished your virtual assistant could actually understand and respond to your complex queries without missing the context? Well, that’s precisely what Agentic RAG brings to the table. By integrating this approach into intelligent assistants and conversational AI systems, you can enable them to have more natural and engaging conversational experiences. It’s like having a real-life companion, minus the awkward silences. Research and Scientific Exploration: Imagine having an agent that can sift through vast repositories of scientific literature, experimental data, and research findings, synthesizing the knowledge from these diverse sources to uncover new insights and generate groundbreaking hypotheses. Agentic RAG could be the secret weapon that propels scientific discoveries to new heights. Content Generation and Creative Writing: Writers, journalists, and content creators, rejoice! Agentic RAG could be your new best friend when it comes to generating high-quality, coherent, and contextually relevant content. These agents can be trained on diverse textual sources, enabling them to assist you in the creative process while fostering originality and creativity. Education and E-Learning: In the realm of education and e-learning, Agentic RAG agents could revolutionize the way we approach personalized learning experiences. These agents could adapt to individual learners’ needs, retrieve relevant educational resources, and generate tailored explanations and study materials, taking the learning process to new heights. Healthcare and Medical Informatics: Imagine having an Agentic RAG agent that can access and synthesize medical knowledge from diverse sources, such as research papers, clinical guidelines, and patient data. These agents could assist healthcare professionals in making informed decisions, providing accurate and up-to-date information while ensuring patient privacy and data security. Legal and Regulatory Compliance: In the world of law and regulation, where understanding and interpreting complex legal documents and precedents is crucial, Agentic RAG agents could be a game-changer. These agents could retrieve and analyze relevant legal information, facilitating research, case preparation, and compliance monitoring with ease. The applications of Agentic RAG are vast and far-reaching, with the potential to transform numerous industries and domains. But with great power comes great responsibility, right? The Future of Agentic RAG: Challenges and Opportunities Await While the Agentic RAG approach holds immense promise, it’s important to acknowledge the challenges that must be addressed to ensure its successful adoption and continued evolution. Let’s take a closer look at some of these hurdles. Data Quality and Curation: Let’s be real — the performance of Agentic RAG agents heavily relies on the quality and curation of the underlying data sources. If the data is incomplete, inaccurate, or irrelevant, then the outputs generated by these agents will reflect that. Ensuring data completeness, accuracy, and relevance is crucial for generating reliable and trustworthy outputs. Effective data management strategies and quality assurance mechanisms must be implemented to keep things running smoothly. Scalability and Efficiency: As the number of agents, tools, and data sources grows, scalability and efficiency become critical considerations. We’re talking about managing system resources, optimizing retrieval processes, and ensuring seamless communication between agents. If these aspects aren’t handled properly, even the most advanced Agentic RAG system could become sluggish and inefficient. Nobody wants a slow and unresponsive AI assistant, right? Interpretability and Explainability: While Agentic RAG agents can provide intelligent responses, ensuring transparency and explainability in their decision-making processes is crucial. Developing interpretable models and techniques that can explain the agent’s reasoning and the sources of information used can foster trust and accountability. After all, you don’t want to blindly follow the advice of an AI without understanding how it arrived at its conclusions. Privacy and Security: Agentic RAG systems may handle sensitive or confidential data, raising privacy and security concerns. Robust data protection measures, access controls, and secure communication protocols must be implemented to safeguard sensitive information and maintain user privacy. The last thing you want is for your confidential data to end up in the wrong hands. Ethical Considerations: The development and deployment of Agentic RAG agents raise ethical questions regarding bias, fairness, and potential misuse. Establishing ethical guidelines, conducting thorough testing, and implementing safeguards against unintended consequences are crucial for responsible adoption. We don’t want our AI assistants to develop any discriminatory or harmful tendencies, now do we? Despite these challenges, the future of Agentic RAG presents exciting opportunities for innovation and growth. Continued research and development in areas such as multi-agent coordination, reinforcement learning, and natural language understanding can further enhance the capabilities and adaptability of Agentic RAG agents. Moreover, the integration of Agentic RAG with other emerging technologies, such as knowledge graphs, ontologies, and semantic web technologies, can unlock new avenues for knowledge representation and reasoning, enabling more sophisticated and context-aware language generation. Imagine having Agentic RAG agents that can seamlessly navigate and leverage vast knowledge graphs, making connections and inferences that would be nearly impossible for humans to achieve on their own. It’s like having a super-powered assistant that can not only retrieve information but also understand the intricate relationships and connections within that information. As organizations and industries embrace the Agentic RAG approach, collaborative efforts and knowledge sharing will be essential for driving its widespread adoption and addressing common challenges. By fostering a community of researchers, developers, and practitioners, the Agentic RAG ecosystem can thrive, leading to groundbreaking applications and solutions that transform the way we interact with and leverage information. Conclusion: Embracing the Agentic RAG Paradigm Alright, folks, let’s wrap this up with a big bow on top. The Agentic RAG approach isn’t just another buzzword or fleeting trend — it represents a paradigm shift in the field of language generation and information retrieval. By bridging the gap between traditional RAG implementations and the intelligence of autonomous agents, Agentic RAG addresses the limitations of the past and paves the way for a future where information is truly at our fingertips. With features like context awareness, intelligent retrieval, multi-agent orchestration, and reasoning capabilities, Agentic RAG offers a level of sophistication and adaptability that was once thought to be the stuff of science fiction. But hey, we’re living in the future, baby! From enterprise knowledge management and customer service to scientific research and content generation, the applications of Agentic RAG are vast and far-reaching. Imagine having a team of intelligent agents dedicated to helping you navigate the vast ocean of information, retrieving exactly what you need, when you need it, and presenting it in a way that makes sense. Of course, with great power comes great responsibility, and we can’t ignore the challenges that come with this technology. Data quality, scalability, interpretability, privacy, and ethical considerations are all hurdles that must be overcome to ensure the responsible development and deployment of Agentic RAG systems. Embracing the Agentic RAG paradigm isn’t just about adopting a new technology; it’s about fostering a symbiotic relationship between humans and machines in the quest for understanding and discovery. It’s about harnessing the power of intelligent agents to augment our own capabilities, enabling us to tackle complex problems and uncover insights that would have been unimaginable just a few years ago. So, let’s dive headfirst into the world of Agentic RAG, embracing the future of intelligent information retrieval and generation. Who knows what groundbreaking discoveries and innovations await us on the other side? The possibilities are endless, and the journey promises to be one heck of a ride! Agent Based Modeling 2.8K Followers CTO | Senior Engineering Leader focused on Cloud Native | AI/ML | DevSecOps Text to speech\\n\\n    Access time: 2024-08-03 15:11:54.438225985-04:00\\n\\n    Web Page text: Agentic RAG: Definition and Low-code Implementation Jun 19, 2024 The workflow of a naive RAG system can be summarized as follows: the RAG system does retrieval from a specified data source using the user query, reranks the retrieval results, appends prompts, and sends them to the LLM for final answer generation. A naive RAG suffices in scenarios where the user’s intent is evident, as the answer is included in the retrieved results and can be sent directly to the LLM. Yet, in most circumstances ambiguous user intents are the norm and demand iterative queries to generate the final answer. For instance, questions involving summarizing multiple documents require multi-step reasoning. These scenarios necessitate Agentic RAG, which involves task orchestration mechanisms during the question-answering process. Agent and RAG complement each other. Agentic RAG, as the name suggests, is an agent-based RAG. The major distinction between an agentic RAG and a naive RAG is that agentic RAG introduces a dynamic agent orchestration mechanism, which criticizes retrievals, rewrites query according to the intent of each user query, and employs “multi-hop” reasoning to handle complex question-answering tasks. Next, let’s explore how agentic RAG works through two advanced RAG examples. The first is Self-RAG (reference [1]), with its workflow shown below: Self-RAG is a type of reflective RAG. After retrieving results from the knowledge base, it assesses if the retrieved results are relevant to the user query. If deemed irrelevant, the query is rewritten, and the RAG cycle is repeated until the relevance score meets a set threshold. A complete Self-RAG requires the implementation of the following two major components: A graph-based task orchestration system. Necessary operators: A scoring operator is crucial to a Self-RAG. While, theoretically, training a scoring model for assessing the retrieved results is desired, in practice, using LLM for scoring can reduce reliance on other system components and simplify system design. Self-RAG is a relatively preliminary form of agentic RAG, and RAGFlow has incorporated a Self-RAG implementation in its system design. Implementing Self-RAG has shown to notably improve the performance of complex multi-hop question-answering and multi-step reasoning. Now let’s explore another form of agentic RAG — Adaptive RAG (reference [2]). It can accommodate its strategies to various user query intents: Open-domain question-answering: Generates answers directly through LLM without relying on retrieval through RAG. Multi-hop question-answering: Breaks multi-hop queries down to multiple single-hop queries, iteratively uses these more basic queries to access LLM and the RAG retriever, and combines the retrieved results to generate the final answer. Adaptive retrieval: Applicable to complex queries requiring multi-step reasoning. Complex question-answering often involves synthesizing information from multiple data sources and performing multi-step reasoning. Adaptive retrieval iteratively accesses LLM and the RAG retriever to progressively build the information chain necessary for answering the complex questions. As shown in the diagram below, Adaptive-RAG follows a similar workflow to Self-RAG. By implementing an extra query analysis at the beginning of its workflow, Adaptive-RAG offers a wider range of question-answering strategies. As can be seen from the above two agentic RAG examples, these advanced RAG systems require task orchestration mechanisms to provide the following functionalities: Reuse of existing pipelines or subgraphs. Collaboration with third-party tools, including web search. Query task planning, such as query intent classification and feedbacks. Frameworks for developing agents include the recently launched Mosaic AI Agent Framework by Databricks and AgentKit; task orchestration frameworks involve LangGraph in Langchain and llamaIndex. A task orchestration system has to be implemented using graph, with its nodes and edges defining the application’s workflow and logic. A node in the Graph can be any callable operator or an executable “component” (e.g., chained operators or agents), each performing a specific task. An edge links nodes together and establishes the data flow between them. A graph must maintain node state management to adapt to the flow of its nodes. It is notable that this graph-based task orchestration implementation requires loops and differs from a DAG (Directed Acyclic Graph). Loops are fundamental for reflection and hence are crucial for the task orchestration in agentic RAG. An agentic RAG lacking reflection would be unable to think or solve problems like a human. It could only offer task orchestration similar to workflows without achieving more advanced tasks like multi-hop and multi-step reasoning. Andrew Ng’s definition of four agent design patterns (reference [3]) separates reflection from the other other three workflow-related patterns — tool use, planning, and multi-agent. This separation underlines the critical role of reflection as the foundation for thinking and reasoning. Agentic RAG embodies this design pattern. Agentic RAG represents a transformation in information processing, bringing more intelligence to the agents per se. When combined with workflows, agentic RAG will have a broader range of applications. For example, in document summarization scenarios, agentic RAG would first determine whether the user’s intent is to request a summary or to compare details. If it is the former, it would use agents to retrieve summary of each document chunk and then combine them to generate the overall summary; if it is the latter, more relevant data need to be retrieved through further routing before being sent to the LLM. In customer support scenarios, agentic RAG can understand more complex customer queries and provide personalized and accurate responses. In literature chatbot scenarios, agentic RAG can synthesize more documents, data, and research results, providing users with a more comprehensive understanding. In legal and medical chatbot scenarios, agentic RAG can help understand and explain complex domain knowledge, offering more precise insights. In content generation applications, agentic RAG can generate higher-quality, contextually relevant, enterprise-level long-form documents. From v0.8.0 onwards, RAGFlow will support graph-based task orchestration and enable no-code editing on top of that. RAGFlow is also consistently improving various retrieval-specific operators to simplify the development of agentic RAG and agent applications based on agentic RAG, addressing pain points seen in enterprise-level RAG applications comprehensively. RAGFlow is iterating rapidly, and you are welcome to follow, star, and actively participate in RAGFlow. Our GitHub repo is at https://github.com/infiniflow/ragflow Bibliography Self-RAG: Learning to retrieve, generate, and critique through self-reflection, arXiv preprint arXiv:2310.11511 Adaptive-RAG: Learning to adapt retrieval-augmented large language models through question complexity, arXiv preprint arXiv:2403.14403 https://www.deeplearning.ai/the-batch/issue-242/ Retrieval Augmented Artificial Intelligence Machine Learning 162 Followers https://github.com/infiniflow/\\n\\n    Access time: 2024-08-05 15:25:43.749269962-04:00\\n\\n    Web Page text: Policy & Safety How YouTube works Test new features NFL Sunday Ticket © 2024 Google LLC\\n\\n    Access time: 2024-08-05 15:29:57.355839014-04:00\\n\\n    Web Page text: Enable JavaScript and cookies to continue\\n\\n    Access time: 2024-08-05 15:43:58.414294004-04:00\\n\\n    Web Page text: Adaptive RAG Initializing search How-to Guides Conceptual Guides Cloud (beta) Quick Start Adaptive RAG using local LLMs Corrective RAG (CRAG) Corrective RAG (CRAG) using local LLMs Self-RAG using local LLMs Agent Architectures Evaluation & Analysis Experimental Adaptive RAG is a strategy for RAG that unites (1) query analysis active / self-corrective RAG , they report query analysis to route across: No Retrieval Single-shot RAG Iterative RAG Let\\'s build on this using LangGraph. In our implementation, we will route between: Web search: for questions related to recent events Self-corrective RAG: for questions related to our index Environment langchain_community langchainhub %%capture --no-stderr\\n! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python \"OPENAI_API_KEY\" \"<your-api-key>\" \"COHERE_API_KEY\" \"TAVILY_API_KEY\" ### LLMs\\nimport os\\n\\nos.environ[\"OPENAI_API_KEY\"] = \" \"\\nos.environ[\"COHERE_API_KEY\"] = \" \"\\nos.environ[\"TAVILY_API_KEY\"] = \" Optionally, use for tracing (shown at bottom) by setting: ### Tracing (optional) \"LANGCHAIN_TRACING_V2\" \"LANGCHAIN_ENDPOINT\" \"https://api.smith.langchain.com\" \"LANGCHAIN_API_KEY\" ### Tracing (optional)\\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\\nos.environ[\"LANGCHAIN_API_KEY\"] = \" ### Build Index langchain.text_splitter RecursiveCharacterTextSplitter langchain_community.document_loaders WebBaseLoader langchain_community.vectorstores langchain_openai OpenAIEmbeddings ### from langchain_cohere import CohereEmbeddings # Set embeddings # Docs to index \"https://lilianweng.github.io/posts/2023-06-23-agent/\" \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\" \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\" text_splitter from_tiktoken_encoder chunk_overlap split_documents # Add to vectorstore vectorstore from_documents collection_name \"rag-chroma\" as_retriever ### Build Index\\n\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain_community.vectorstores import Chroma\\nfrom langchain_openai import OpenAIEmbeddings\\n\\n### from langchain_cohere import CohereEmbeddings\\n\\n# Set embeddings\\nembd = OpenAIEmbeddings()\\n\\n# Docs to index\\nurls = [\\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\\n]\\n\\n# Load\\ndocs = [WebBaseLoader(url).load() for url in urls]\\ndocs_list = [item for sublist in docs for item in sublist]\\n\\n# Split\\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\\n    chunk_size=500, chunk_overlap=0\\n)\\ndoc_splits = text_splitter.split_documents(docs_list)\\n\\n# Add to vectorstore\\nvectorstore = Chroma.from_documents(\\n    documents=doc_splits,\\n    collection_name=\"rag-chroma\",\\n    embedding=embd,\\n)\\nretriever = vectorstore.as_retriever() langchain_core.prompts ChatPromptTemplate langchain_core.pydantic_v1 # Data model \"\"\"Route a user query to the most relevant datasource.\"\"\" \"vectorstore\" \"web_search\" description \"Given a user question choose to route it to web search or a vectorstore.\" # LLM with function call \"gpt-3.5-turbo-0125\" temperature structured_llm_router with_structured_output \"\"\"You are an expert at routing a user question to a vectorstore or web search. The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks. Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\" route_prompt from_messages question_router \"Who will the Bears draft first in the NFL draft?\" \"What are the types of agent memory?\" ### Router\\n\\nfrom typing import Literal\\n\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\nfrom langchain_openai import ChatOpenAI\\n\\n\\n# Data model\\nclass RouteQuery(BaseModel):\\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\\n\\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\\n        ...,\\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\\n    )\\n\\n\\n# LLM with function call\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\\nstructured_llm_router = llm.with_structured_output(RouteQuery)\\n\\n# Prompt\\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\\nroute_prompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system),\\n        (\"human\", \"{question}\"),\\n    ]\\n)\\n\\nquestion_router = route_prompt | structured_llm_router\\nprint(\\n    question_router.invoke(\\n        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\\n    )\\n)\\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"})) datasource=\\'web_search\\'\\ndatasource=\\'vectorstore\\' ### Retrieval Grader GradeDocuments \"\"\"Binary score for relevance check on retrieved documents.\"\"\" binary_score \"Documents are relevant to the question, \\'yes\\' or \\'no\\'\" structured_llm_grader \"\"\"You are a grader assessing relevance of a retrieved document to a user question. If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. Give a binary score \\'yes\\' or \\'no\\' score to indicate whether the document is relevant to the question.\"\"\" grade_prompt \"Retrieved document: User question: retrieval_grader \"agent memory\" get_relevant_documents page_content ### Retrieval Grader\\n\\n\\n# Data model\\nclass GradeDocuments(BaseModel):\\n    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\\n\\n    binary_score: str = Field(\\n        description=\"Documents are relevant to the question, \\'yes\\' or \\'no\\'\"\\n    )\\n\\n\\n# LLM with function call\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\\nstructured_llm_grader = llm.with_structured_output(GradeDocuments)\\n\\n# Prompt\\nsystem = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\\\n \\n    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\\\n\\n    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\\\n\\n    Give a binary score \\'yes\\' or \\'no\\' score to indicate whether the document is relevant to the question.\"\"\"\\ngrade_prompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system),\\n        (\"human\", \"Retrieved document: \\\\n\\\\n {document} \\\\n\\\\n User question: {question}\"),\\n    ]\\n)\\n\\nretrieval_grader = grade_prompt | structured_llm_grader\\nquestion = \"agent memory\"\\ndocs = retriever.get_relevant_documents(question)\\ndoc_txt = docs[1].page_content\\nprint(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})) binary_score=\\'no\\' ### Generate langchain_core.output_parsers StrOutputParser \"rlm/rag-prompt\" \"gpt-3.5-turbo\" # Post-processing format_docs ### Generate\\n\\nfrom langchain import hub\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n# Prompt\\nprompt = hub.pull(\"rlm/rag-prompt\")\\n\\n# LLM\\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\\n\\n\\n# Post-processing\\ndef format_docs(docs):\\n    return \"\\\\n\\\\n\".join(doc.page_content for doc in docs)\\n\\n\\n# Chain\\nrag_chain = prompt | llm | StrOutputParser()\\n\\n# Run\\ngeneration = rag_chain.invoke({\"context\": docs, \"question\": question})\\nprint(generation) The design of generative agents combines LLM with memory, planning, and reflection mechanisms to enable agents to behave based on past experience and interact with other agents. Memory stream is a long-term memory module that records agents\\' experiences in natural language. The retrieval model surfaces context to inform the agent\\'s behavior based on relevance, recency, and importance. ### Hallucination Grader GradeHallucinations \"\"\"Binary score for hallucination present in generation answer.\"\"\" \"Answer is grounded in the facts, \\'yes\\' or \\'no\\'\" \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. Give a binary score \\'yes\\' or \\'no\\'. \\'Yes\\' means that the answer is grounded in / supported by the set of facts.\"\"\" hallucination_prompt \"Set of facts: {documents} LLM generation: {generation} hallucination_grader \"documents\" \"generation\" ### Hallucination Grader\\n\\n\\n# Data model\\nclass GradeHallucinations(BaseModel):\\n    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\\n\\n    binary_score: str = Field(\\n        description=\"Answer is grounded in the facts, \\'yes\\' or \\'no\\'\"\\n    )\\n\\n\\n# LLM with function call\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\\nstructured_llm_grader = llm.with_structured_output(GradeHallucinations)\\n\\n# Prompt\\nsystem = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\\\n \\n     Give a binary score \\'yes\\' or \\'no\\'. \\'Yes\\' means that the answer is grounded in / supported by the set of facts.\"\"\"\\nhallucination_prompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system),\\n        (\"human\", \"Set of facts: \\\\n\\\\n {documents} \\\\n\\\\n LLM generation: {generation}\"),\\n    ]\\n)\\n\\nhallucination_grader = hallucination_prompt | structured_llm_grader\\nhallucination_grader.invoke({\"documents\": docs, \"generation\": generation}) GradeHallucinations(binary_score=\\'yes\\') ### Answer Grader GradeAnswer \"\"\"Binary score to assess answer addresses question.\"\"\" \"Answer addresses the question, \\'yes\\' or \\'no\\'\" \"\"\"You are a grader assessing whether an answer addresses / resolves a question Give a binary score \\'yes\\' or \\'no\\'. Yes\\' means that the answer resolves the question.\"\"\" answer_prompt \"User question: answer_grader ### Answer Grader\\n\\n\\n# Data model\\nclass GradeAnswer(BaseModel):\\n    \"\"\"Binary score to assess answer addresses question.\"\"\"\\n\\n    binary_score: str = Field(\\n        description=\"Answer addresses the question, \\'yes\\' or \\'no\\'\"\\n    )\\n\\n\\n# LLM with function call\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\\nstructured_llm_grader = llm.with_structured_output(GradeAnswer)\\n\\n# Prompt\\nsystem = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\\\n \\n     Give a binary score \\'yes\\' or \\'no\\'. Yes\\' means that the answer resolves the question.\"\"\"\\nanswer_prompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system),\\n        (\"human\", \"User question: \\\\n\\\\n {question} \\\\n\\\\n LLM generation: {generation}\"),\\n    ]\\n)\\n\\nanswer_grader = answer_prompt | structured_llm_grader\\nanswer_grader.invoke({\"question\": question, \"generation\": generation}) GradeAnswer(binary_score=\\'yes\\') ### Question Re-writer \"\"\"You a question re-writer that converts an input question to a better version that is optimized for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\" re_write_prompt \"Here is the initial question: Formulate an improved question.\" question_rewriter ### Question Re-writer\\n\\n# LLM\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\\n\\n# Prompt\\nsystem = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\\\n \\n     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\\nre_write_prompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system),\\n        (\\n            \"human\",\\n            \"Here is the initial question: \\\\n\\\\n {question} \\\\n Formulate an improved question.\",\\n        ),\\n    ]\\n)\\n\\nquestion_rewriter = re_write_prompt | llm | StrOutputParser()\\nquestion_rewriter.invoke({\"question\": question}) \"What is the role of memory in an agent\\'s functioning?\" Web Search Tool langchain_community.tools.tavily_search TavilySearchResults web_search_tool ### Search\\n\\nfrom langchain_community.tools.tavily_search import TavilySearchResults\\n\\nweb_search_tool = TavilySearchResults(k=3) Capture the flow in as a graph. Graph state typing_extensions Represents the state of our graph. Attributes: question: question generation: LLM generation documents: list of documents from typing import List\\n\\nfrom typing_extensions import TypedDict\\n\\n\\nclass GraphState(TypedDict):\\n    \"\"\"\\n    Represents the state of our graph.\\n\\n    Attributes:\\n        question: question\\n        generation: LLM generation\\n        documents: list of documents\\n    \"\"\"\\n\\n    question: str\\n    generation: str\\n    documents: List[str] langchain.schema Retrieve documents state (dict): The current graph state state (dict): New key added to state, documents, that contains retrieved documents \"---RETRIEVE---\" # Retrieval Generate answer state (dict): New key added to state, generation, that contains LLM generation \"---GENERATE---\" # RAG generation grade_documents Determines whether the retrieved documents are relevant to the question. state (dict): Updates documents key with only filtered relevant documents \"---CHECK DOCUMENT RELEVANCE TO QUESTION---\" # Score each doc filtered_docs \"---GRADE: DOCUMENT RELEVANT---\" \"---GRADE: DOCUMENT NOT RELEVANT---\" transform_query Transform the query to produce a better question. state (dict): Updates question key with a re-phrased question \"---TRANSFORM QUERY---\" # Re-write question better_question Web search based on the re-phrased question. state (dict): Updates documents key with appended web results \"---WEB SEARCH---\" # Web search web_results ### Edges ### route_question Route question to web search or RAG. str: Next node to call \"---ROUTE QUESTION---\" \"---ROUTE QUESTION TO WEB SEARCH---\" \"---ROUTE QUESTION TO RAG---\" decide_to_generate Determines whether to generate an answer, or re-generate a question. str: Binary decision for next node to call \"---ASSESS GRADED DOCUMENTS---\" filtered_documents # All documents have been filtered check_relevance # We will re-generate a new query \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\" \"transform_query\" # We have relevant documents, so generate answer \"---DECISION: GENERATE---\" grade_generation_v_documents_and_question Determines whether the generation is grounded in the document and answers question. str: Decision for next node to call \"---CHECK HALLUCINATIONS---\" # Check hallucination \"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\" # Check question-answering \"---GRADE GENERATION vs QUESTION---\" \"---DECISION: GENERATION ADDRESSES QUESTION---\" \"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\" \"not useful\" \"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\" \"not supported\" from langchain.schema import Document\\n\\n\\ndef retrieve(state):\\n    \"\"\"\\n    Retrieve documents\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): New key added to state, documents, that contains retrieved documents\\n    \"\"\"\\n    print(\"---RETRIEVE---\")\\n    question = state[\"question\"]\\n\\n    # Retrieval\\n    documents = retriever.invoke(question)\\n    return {\"documents\": documents, \"question\": question}\\n\\n\\ndef generate(state):\\n    \"\"\"\\n    Generate answer\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): New key added to state, generation, that contains LLM generation\\n    \"\"\"\\n    print(\"---GENERATE---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # RAG generation\\n    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\\n    return {\"documents\": documents, \"question\": question, \"generation\": generation}\\n\\n\\ndef grade_documents(state):\\n    \"\"\"\\n    Determines whether the retrieved documents are relevant to the question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates documents key with only filtered relevant documents\\n    \"\"\"\\n\\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # Score each doc\\n    filtered_docs = []\\n    for d in documents:\\n        score = retrieval_grader.invoke(\\n            {\"question\": question, \"document\": d.page_content}\\n        )\\n        grade = score.binary_score\\n        if grade == \"yes\":\\n            print(\"---GRADE: DOCUMENT RELEVANT---\")\\n            filtered_docs.append(d)\\n        else:\\n            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\\n            continue\\n    return {\"documents\": filtered_docs, \"question\": question}\\n\\n\\ndef transform_query(state):\\n    \"\"\"\\n    Transform the query to produce a better question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates question key with a re-phrased question\\n    \"\"\"\\n\\n    print(\"---TRANSFORM QUERY---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # Re-write question\\n    better_question = question_rewriter.invoke({\"question\": question})\\n    return {\"documents\": documents, \"question\": better_question}\\n\\n\\ndef web_search(state):\\n    \"\"\"\\n    Web search based on the re-phrased question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates documents key with appended web results\\n    \"\"\"\\n\\n    print(\"---WEB SEARCH---\")\\n    question = state[\"question\"]\\n\\n    # Web search\\n    docs = web_search_tool.invoke({\"query\": question})\\n    web_results = \"\\\\n\".join([d[\"content\"] for d in docs])\\n    web_results = Document(page_content=web_results)\\n\\n    return {\"documents\": web_results, \"question\": question}\\n\\n\\n### Edges ###\\n\\n\\ndef route_question(state):\\n    \"\"\"\\n    Route question to web search or RAG.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        str: Next node to call\\n    \"\"\"\\n\\n    print(\"---ROUTE QUESTION---\")\\n    question = state[\"question\"]\\n    source = question_router.invoke({\"question\": question})\\n    if source.datasource == \"web_search\":\\n        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\\n        return \"web_search\"\\n    elif source.datasource == \"vectorstore\":\\n        print(\"---ROUTE QUESTION TO RAG---\")\\n        return \"vectorstore\"\\n\\n\\ndef decide_to_generate(state):\\n    \"\"\"\\n    Determines whether to generate an answer, or re-generate a question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        str: Binary decision for next node to call\\n    \"\"\"\\n\\n    print(\"---ASSESS GRADED DOCUMENTS---\")\\n    state[\"question\"]\\n    filtered_documents = state[\"documents\"]\\n\\n    if not filtered_documents:\\n        # All documents have been filtered check_relevance\\n        # We will re-generate a new query\\n        print(\\n            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\\n        )\\n        return \"transform_query\"\\n    else:\\n        # We have relevant documents, so generate answer\\n        print(\"---DECISION: GENERATE---\")\\n        return \"generate\"\\n\\n\\ndef grade_generation_v_documents_and_question(state):\\n    \"\"\"\\n    Determines whether the generation is grounded in the document and answers question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        str: Decision for next node to call\\n    \"\"\"\\n\\n    print(\"---CHECK HALLUCINATIONS---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n    generation = state[\"generation\"]\\n\\n    score = hallucination_grader.invoke(\\n        {\"documents\": documents, \"generation\": generation}\\n    )\\n    grade = score.binary_score\\n\\n    # Check hallucination\\n    if grade == \"yes\":\\n        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\\n        # Check question-answering\\n        print(\"---GRADE GENERATION vs QUESTION---\")\\n        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\\n        grade = score.binary_score\\n        if grade == \"yes\":\\n            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\\n            return \"useful\"\\n        else:\\n            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\\n            return \"not useful\"\\n    else:\\n        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\\n        return \"not supported\" Build Graph langgraph.graph # Define the nodes # web search \"grade_documents\" # grade documents # generatae # transform_query # Build graph add_conditional_edges from langgraph.graph import END, StateGraph, START\\n\\nworkflow = StateGraph(GraphState)\\n\\n# Define the nodes\\nworkflow.add_node(\"web_search\", web_search)  # web search\\nworkflow.add_node(\"retrieve\", retrieve)  # retrieve\\nworkflow.add_node(\"grade_documents\", grade_documents)  # grade documents\\nworkflow.add_node(\"generate\", generate)  # generatae\\nworkflow.add_node(\"transform_query\", transform_query)  # transform_query\\n\\n# Build graph\\nworkflow.add_conditional_edges(\\n    START,\\n    route_question,\\n    {\\n        \"web_search\": \"web_search\",\\n        \"vectorstore\": \"retrieve\",\\n    },\\n)\\nworkflow.add_edge(\"web_search\", \"generate\")\\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\\nworkflow.add_conditional_edges(\\n    \"grade_documents\",\\n    decide_to_generate,\\n    {\\n        \"transform_query\": \"transform_query\",\\n        \"generate\": \"generate\",\\n    },\\n)\\nworkflow.add_edge(\"transform_query\", \"retrieve\")\\nworkflow.add_conditional_edges(\\n    \"generate\",\\n    grade_generation_v_documents_and_question,\\n    {\\n        \"not supported\": \"generate\",\\n        \"useful\": END,\\n        \"not useful\": \"transform_query\",\\n    },\\n)\\n\\n# Compile\\napp = workflow.compile() \"What player at the Bears expected to draft first in the 2024 NFL draft?\" # Optional: print full state at each node # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None) # Final generation from pprint import pprint\\n\\n# Run\\ninputs = {\\n    \"question\": \"What player at the Bears expected to draft first in the 2024 NFL draft?\"\\n}\\nfor output in app.stream(inputs):\\n    for key, value in output.items():\\n        # Node\\n        pprint(f\"Node \\'{key}\\':\")\\n        # Optional: print full state at each node\\n        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\\n    pprint(\"\\\\n---\\\\n\")\\n\\n# Final generation\\npprint(value[\"generation\"]) ---ROUTE QUESTION---\\n---ROUTE QUESTION TO WEB SEARCH---\\n---WEB SEARCH---\\n\"Node \\'web_search\\':\"\\n\\'\\\\n---\\\\n\\'\\n---GENERATE---\\n---CHECK HALLUCINATIONS---\\n---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\\n---GRADE GENERATION vs QUESTION---\\n---DECISION: GENERATION ADDRESSES QUESTION---\\n\"Node \\'generate\\':\"\\n\\'\\\\n---\\\\n\\'\\n(\\'It is expected that the Chicago Bears could have the opportunity to draft \\'\\n \\'the first defensive player in the 2024 NFL draft. The Bears have the first \\'\\n \\'overall pick in the draft, giving them a prime position to select top \\'\\n \\'talent. The top wide receiver Marvin Harrison Jr. from Ohio State is also \\'\\n \\'mentioned as a potential pick for the Cardinals.\\') https://smith.langchain.com/public/7e3aa7e5-c51f-45c2-bc66-b34f17ff2263/r # Run\\ninputs = {\"question\": \"What are the types of agent memory?\"}\\nfor output in app.stream(inputs):\\n    for key, value in output.items():\\n        # Node\\n        pprint(f\"Node \\'{key}\\':\")\\n        # Optional: print full state at each node\\n        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\\n    pprint(\"\\\\n---\\\\n\")\\n\\n# Final generation\\npprint(value[\"generation\"]) ---ROUTE QUESTION---\\n---ROUTE QUESTION TO RAG---\\n---RETRIEVE---\\n\"Node \\'retrieve\\':\"\\n\\'\\\\n---\\\\n\\'\\n---CHECK DOCUMENT RELEVANCE TO QUESTION---\\n---GRADE: DOCUMENT RELEVANT---\\n---GRADE: DOCUMENT RELEVANT---\\n---GRADE: DOCUMENT NOT RELEVANT---\\n---GRADE: DOCUMENT RELEVANT---\\n---ASSESS GRADED DOCUMENTS---\\n---DECISION: GENERATE---\\n\"Node \\'grade_documents\\':\"\\n\\'\\\\n---\\\\n\\'\\n---GENERATE---\\n---CHECK HALLUCINATIONS---\\n---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\\n---GRADE GENERATION vs QUESTION---\\n---DECISION: GENERATION ADDRESSES QUESTION---\\n\"Node \\'generate\\':\"\\n\\'\\\\n---\\\\n\\'\\n(\\'The types of agent memory include Sensory Memory, Short-Term Memory (STM) or \\'\\n \\'Working Memory, and Long-Term Memory (LTM) with subtypes of Explicit / \\'\\n \\'declarative memory and Implicit / procedural memory. Sensory memory retains \\'\\n \\'sensory information briefly, STM stores information for cognitive tasks, and \\'\\n \\'LTM stores information for a long time with different types of memories.\\') https://smith.langchain.com/public/fdf0a180-6d15-4d09-bb92-f84f2105ca51/r Back to top Code Assistant Material for MkDocs\\n\\n    Access time: 2024-08-05 15:44:02.906191111-04:00\\n\\n    Web Page text: %capture --no-stderr\\n%pip install -U langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python nomic[local] Local Embeddings You can use GPT4AllEmbeddings() from Nomic, which can access use Nomic\\'s recently released embeddings. Follow the documentation (1) Download (2) Download a model from various Mistral versions and Mixtral versions available. Also, try one of the quantized command-R models ollama pull mistral # Ollama model name # Ollama model name\\nlocal_llm = \"mistral\" for tracing (shown at bottom) import os\\n\\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\\nos.environ[\"LANGCHAIN_API_KEY\"] = \" langchain_nomic.embeddings NomicEmbeddings # Add to vectorDB \"nomic-embed-text-v1.5\" inference_mode from langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain_community.vectorstores import Chroma\\nfrom langchain_nomic.embeddings import NomicEmbeddings\\n\\nurls = [\\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\\n]\\n\\ndocs = [WebBaseLoader(url).load() for url in urls]\\ndocs_list = [item for sublist in docs for item in sublist]\\n\\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\\n    chunk_size=250, chunk_overlap=0\\n)\\ndoc_splits = text_splitter.split_documents(docs_list)\\n\\n# Add to vectorDB\\nvectorstore = Chroma.from_documents(\\n    documents=doc_splits,\\n    collection_name=\"rag-chroma\",\\n    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\\n)\\nretriever = vectorstore.as_retriever() Note: tested cmd-R on Mac M2 32GB and latency is ~52 sec for RAG generation langchain.prompts PromptTemplate langchain_community.chat_models JsonOutputParser Use the vectorstore for questions on LLM  agents, prompt engineering, and adversarial attacks. You do not need to be stringent with the keywords in the question related to these topics. Otherwise, use web-search. Give a binary choice \\'web_search\\' or \\'vectorstore\\' based on the question. Return the a JSON with a single key \\'datasource\\' and no premable or explanation. Question to route: input_variables \"llm agent memory\" ### Router\\n\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_core.output_parsers import JsonOutputParser\\n\\n# LLM\\nllm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\\n\\nprompt = PromptTemplate(\\n    template=\"\"\"You are an expert at routing a user question to a vectorstore or web search. \\\\n\\n    Use the vectorstore for questions on LLM  agents, prompt engineering, and adversarial attacks. \\\\n\\n    You do not need to be stringent with the keywords in the question related to these topics. \\\\n\\n    Otherwise, use web-search. Give a binary choice \\'web_search\\' or \\'vectorstore\\' based on the question. \\\\n\\n    Return the a JSON with a single key \\'datasource\\' and no premable or explanation. \\\\n\\n    Question to route: {question}\"\"\",\\n    input_variables=[\"question\"],\\n)\\n\\nquestion_router = prompt | llm | JsonOutputParser()\\nquestion = \"llm agent memory\"\\ndocs = retriever.get_relevant_documents(question)\\ndoc_txt = docs[1].page_content\\nprint(question_router.invoke({\"question\": question})) {\\'datasource\\': \\'vectorstore\\'} Here is the retrieved document: Here is the user question: If the document contains keywords related to the user question, grade it as relevant. Give a binary score \\'yes\\' or \\'no\\' score to indicate whether the document is relevant to the question. Provide the binary score as a JSON with a single key \\'score\\' and no premable or explanation.\"\"\" ### Retrieval Grader\\n\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_core.output_parsers import JsonOutputParser\\n\\n# LLM\\nllm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\\n\\nprompt = PromptTemplate(\\n    template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\\\n \\n    Here is the retrieved document: \\\\n\\\\n {document} \\\\n\\\\n\\n    Here is the user question: {question} \\\\n\\n    If the document contains keywords related to the user question, grade it as relevant. \\\\n\\n    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\\\n\\n    Give a binary score \\'yes\\' or \\'no\\' score to indicate whether the document is relevant to the question. \\\\n\\n    Provide the binary score as a JSON with a single key \\'score\\' and no premable or explanation.\"\"\",\\n    input_variables=[\"question\", \"document\"],\\n)\\n\\nretrieval_grader = prompt | llm | JsonOutputParser()\\nquestion = \"agent memory\"\\ndocs = retriever.get_relevant_documents(question)\\ndoc_txt = docs[1].page_content\\nprint(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})) {\\'score\\': \\'yes\\'} ### Generate\\n\\nfrom langchain import hub\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n# Prompt\\nprompt = hub.pull(\"rlm/rag-prompt\")\\n\\n# LLM\\nllm = ChatOllama(model=local_llm, temperature=0)\\n\\n\\n# Post-processing\\ndef format_docs(docs):\\n    return \"\\\\n\\\\n\".join(doc.page_content for doc in docs)\\n\\n\\n# Chain\\nrag_chain = prompt | llm | StrOutputParser()\\n\\n# Run\\nquestion = \"agent memory\"\\ngeneration = rag_chain.invoke({\"context\": docs, \"question\": question})\\nprint(generation) In an LLM-powered autonomous agent system, the Large Language Model (LLM) functions as the agent\\'s brain. The agent has key components including memory, planning, and reflection mechanisms. The memory component is a long-term memory module that records a comprehensive list of agents’ experience in natural language. It includes a memory stream, which is an external database for storing past experiences. The reflection mechanism synthesizes memories into higher-level inferences over time and guides the agent\\'s future behavior. \"\"\"You are a grader assessing whether an answer is grounded in / supported by a set of facts. Here are the facts: Here is the answer: Give a binary score \\'yes\\' or \\'no\\' score to indicate whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a single key \\'score\\' and no preamble or explanation.\"\"\" ### Hallucination Grader\\n\\n# LLM\\nllm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\\n\\n# Prompt\\nprompt = PromptTemplate(\\n    template=\"\"\"You are a grader assessing whether an answer is grounded in / supported by a set of facts. \\\\n \\n    Here are the facts:\\n    \\\\n ------- \\\\n\\n    {documents} \\n    \\\\n ------- \\\\n\\n    Here is the answer: {generation}\\n    Give a binary score \\'yes\\' or \\'no\\' score to indicate whether the answer is grounded in / supported by a set of facts. \\\\n\\n    Provide the binary score as a JSON with a single key \\'score\\' and no preamble or explanation.\"\"\",\\n    input_variables=[\"generation\", \"documents\"],\\n)\\n\\nhallucination_grader = prompt | llm | JsonOutputParser()\\nhallucination_grader.invoke({\"documents\": docs, \"generation\": generation}) \"\"\"You are a grader assessing whether an answer is useful to resolve a question. Here is the question: Give a binary score \\'yes\\' or \\'no\\' to indicate whether the answer is useful to resolve a question. ### Answer Grader\\n\\n# LLM\\nllm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\\n\\n# Prompt\\nprompt = PromptTemplate(\\n    template=\"\"\"You are a grader assessing whether an answer is useful to resolve a question. \\\\n \\n    Here is the answer:\\n    \\\\n ------- \\\\n\\n    {generation} \\n    \\\\n ------- \\\\n\\n    Here is the question: {question}\\n    Give a binary score \\'yes\\' or \\'no\\' to indicate whether the answer is useful to resolve a question. \\\\n\\n    Provide the binary score as a JSON with a single key \\'score\\' and no preamble or explanation.\"\"\",\\n    input_variables=[\"generation\", \"question\"],\\n)\\n\\nanswer_grader = prompt | llm | JsonOutputParser()\\nanswer_grader.invoke({\"question\": question, \"generation\": generation}) for vectorstore retrieval. Look at the initial and formulate an improved question. Here is the initial question: . Improved question with no preamble: ### Question Re-writer\\n\\n# LLM\\nllm = ChatOllama(model=local_llm, temperature=0)\\n\\n# Prompt\\nre_write_prompt = PromptTemplate(\\n    template=\"\"\"You a question re-writer that converts an input question to a better version that is optimized \\\\n \\n     for vectorstore retrieval. Look at the initial and formulate an improved question. \\\\n\\n     Here is the initial question: \\\\n\\\\n {question}. Improved question with no preamble: \\\\n \"\"\",\\n    input_variables=[\"generation\", \"question\"],\\n)\\n\\nquestion_rewriter = re_write_prompt | llm | StrOutputParser()\\nquestion_rewriter.invoke({\"question\": question}) \\' What is agent memory and how can it be effectively utilized in vector database retrieval?\\' \"datasource\" ### Nodes\\n\\nfrom langchain.schema import Document\\n\\n\\ndef retrieve(state):\\n    \"\"\"\\n    Retrieve documents\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): New key added to state, documents, that contains retrieved documents\\n    \"\"\"\\n    print(\"---RETRIEVE---\")\\n    question = state[\"question\"]\\n\\n    # Retrieval\\n    documents = retriever.get_relevant_documents(question)\\n    return {\"documents\": documents, \"question\": question}\\n\\n\\ndef generate(state):\\n    \"\"\"\\n    Generate answer\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): New key added to state, generation, that contains LLM generation\\n    \"\"\"\\n    print(\"---GENERATE---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # RAG generation\\n    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\\n    return {\"documents\": documents, \"question\": question, \"generation\": generation}\\n\\n\\ndef grade_documents(state):\\n    \"\"\"\\n    Determines whether the retrieved documents are relevant to the question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates documents key with only filtered relevant documents\\n    \"\"\"\\n\\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # Score each doc\\n    filtered_docs = []\\n    for d in documents:\\n        score = retrieval_grader.invoke(\\n            {\"question\": question, \"document\": d.page_content}\\n        )\\n        grade = score[\"score\"]\\n        if grade == \"yes\":\\n            print(\"---GRADE: DOCUMENT RELEVANT---\")\\n            filtered_docs.append(d)\\n        else:\\n            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\\n            continue\\n    return {\"documents\": filtered_docs, \"question\": question}\\n\\n\\ndef transform_query(state):\\n    \"\"\"\\n    Transform the query to produce a better question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates question key with a re-phrased question\\n    \"\"\"\\n\\n    print(\"---TRANSFORM QUERY---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # Re-write question\\n    better_question = question_rewriter.invoke({\"question\": question})\\n    return {\"documents\": documents, \"question\": better_question}\\n\\n\\ndef web_search(state):\\n    \"\"\"\\n    Web search based on the re-phrased question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates documents key with appended web results\\n    \"\"\"\\n\\n    print(\"---WEB SEARCH---\")\\n    question = state[\"question\"]\\n\\n    # Web search\\n    docs = web_search_tool.invoke({\"query\": question})\\n    web_results = \"\\\\n\".join([d[\"content\"] for d in docs])\\n    web_results = Document(page_content=web_results)\\n\\n    return {\"documents\": web_results, \"question\": question}\\n\\n\\n### Edges ###\\n\\n\\ndef route_question(state):\\n    \"\"\"\\n    Route question to web search or RAG.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        str: Next node to call\\n    \"\"\"\\n\\n    print(\"---ROUTE QUESTION---\")\\n    question = state[\"question\"]\\n    print(question)\\n    source = question_router.invoke({\"question\": question})\\n    print(source)\\n    print(source[\"datasource\"])\\n    if source[\"datasource\"] == \"web_search\":\\n        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\\n        return \"web_search\"\\n    elif source[\"datasource\"] == \"vectorstore\":\\n        print(\"---ROUTE QUESTION TO RAG---\")\\n        return \"vectorstore\"\\n\\n\\ndef decide_to_generate(state):\\n    \"\"\"\\n    Determines whether to generate an answer, or re-generate a question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        str: Binary decision for next node to call\\n    \"\"\"\\n\\n    print(\"---ASSESS GRADED DOCUMENTS---\")\\n    state[\"question\"]\\n    filtered_documents = state[\"documents\"]\\n\\n    if not filtered_documents:\\n        # All documents have been filtered check_relevance\\n        # We will re-generate a new query\\n        print(\\n            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\\n        )\\n        return \"transform_query\"\\n    else:\\n        # We have relevant documents, so generate answer\\n        print(\"---DECISION: GENERATE---\")\\n        return \"generate\"\\n\\n\\ndef grade_generation_v_documents_and_question(state):\\n    \"\"\"\\n    Determines whether the generation is grounded in the document and answers question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        str: Decision for next node to call\\n    \"\"\"\\n\\n    print(\"---CHECK HALLUCINATIONS---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n    generation = state[\"generation\"]\\n\\n    score = hallucination_grader.invoke(\\n        {\"documents\": documents, \"generation\": generation}\\n    )\\n    grade = score[\"score\"]\\n\\n    # Check hallucination\\n    if grade == \"yes\":\\n        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\\n        # Check question-answering\\n        print(\"---GRADE GENERATION vs QUESTION---\")\\n        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\\n        grade = score[\"score\"]\\n        if grade == \"yes\":\\n            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\\n            return \"useful\"\\n        else:\\n            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\\n            return \"not useful\"\\n    else:\\n        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\\n        return \"not supported\" \"What is the AlphaCodium paper about?\" from pprint import pprint\\n\\n# Run\\ninputs = {\"question\": \"What is the AlphaCodium paper about?\"}\\nfor output in app.stream(inputs):\\n    for key, value in output.items():\\n        # Node\\n        pprint(f\"Node \\'{key}\\':\")\\n        # Optional: print full state at each node\\n        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\\n    pprint(\"\\\\n---\\\\n\")\\n\\n# Final generation\\npprint(value[\"generation\"]) ---ROUTE QUESTION---\\nWhat is the AlphaCodium paper about?\\n{\\'datasource\\': \\'web_search\\'}\\nweb_search\\n---ROUTE QUESTION TO WEB SEARCH---\\n---WEB SEARCH---\\n\"Node \\'web_search\\':\"\\n\\'\\\\n---\\\\n\\'\\n---GENERATE---\\n---CHECK HALLUCINATIONS---\\n---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\\n---GRADE GENERATION vs QUESTION---\\n---DECISION: GENERATION ADDRESSES QUESTION---\\n\"Node \\'generate\\':\"\\n\\'\\\\n---\\\\n\\'\\n(\\' The AlphaCodium paper introduces a new approach for code generation by \\'\\n \\'Large Language Models (LLMs). It presents AlphaCodium, an iterative process \\'\\n \\'that involves generating additional data to aid the flow, and testing it on \\'\\n \\'the CodeContests dataset. The results show that AlphaCodium outperforms \\'\\n \"DeepMind\\'s AlphaCode and AlphaCode2 without fine-tuning a model. The \"\\n \\'approach includes a pre-processing phase for problem reasoning in natural \\'\\n \\'language and an iterative code generation phase with runs and fixes against \\'\\n \\'tests.\\') https://smith.langchain.com/public/81813813-be53-403c-9877-afcd5786ca2e/r\\n\\n    Access time: 2024-08-05 15:44:07.620565891-04:00\\n\\n    Web Page text: Table of contents Corrective-RAG (CRAG) is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents. The paper follows this general flow: If at least one document exceeds the threshold for , then it proceeds to generation If all documents fall below the threshold or if the grader is unsure, then it uses web search to supplement retrieval Before generation, it performs knowledge refinement of the search or retrieved documents This partitions the document into knowledge strips It grades each strip, and filters out irrelevant ones We will implement some of these ideas from scratch using documents are irrelevant, we\\'ll supplement retrieval with web search. We\\'ll skip the knowledge refinement, but this can be added back as a node if desired. Tavily Search for web search. to access a local LLM: Pull your model of choice, e.g.: ollama pull llama3 We\\'ll use a vectorstore with Nomic local embeddings or, optionally, OpenAI embeddings. for tracing and evaluation. %%capture --no-stderr\\n%pip install -U langchain_community tiktoken langchainhub scikit-learn langchain langgraph tavily-python  nomic[local] langchain-nomic langchain_openai # Search\\nimport os\\n\\nos.environ[\"TAVILY_API_KEY\"] = \"xxx\" # Embedding (optional) # Embedding (optional)\\nos.environ[\"OPENAI_API_KEY\"] = \"xxx\" # Tracing and testing (optional) \"LANGCHAIN_PROJECT\" \"corrective-rag-agent-testing\" # Tracing and testing (optional)\\nos.environ[\"LANGCHAIN_API_KEY\"] = \"xxx\"\\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\\nos.environ[\"LANGCHAIN_PROJECT\"] = \"corrective-rag-agent-testing\" You can select from Ollama LLMs model_tested \"llama3-8b\" local_llm = \"llama3\"\\nmodel_tested = \"llama3-8b\"\\nmetadata = f\"CRAG, {model_tested}\" Let\\'s index 3 blog posts. SKLearnVectorStore # List of URLs to load documents from # Load documents from the URLs # Initialize a text splitter with specified chunk size and overlap # Split the documents into chunks # Embedding embedding=NomicEmbeddings( model=\"nomic-embed-text-v1.5\", inference_mode=\"local\", # Add the document chunks to the \"vector store\" from langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain_community.vectorstores import SKLearnVectorStore\\nfrom langchain_nomic.embeddings import NomicEmbeddings  # local\\nfrom langchain_openai import OpenAIEmbeddings  # api\\n\\n# List of URLs to load documents from\\nurls = [\\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\\n]\\n\\n# Load documents from the URLs\\ndocs = [WebBaseLoader(url).load() for url in urls]\\ndocs_list = [item for sublist in docs for item in sublist]\\n\\n# Initialize a text splitter with specified chunk size and overlap\\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\\n    chunk_size=250, chunk_overlap=0\\n)\\n\\n# Split the documents into chunks\\ndoc_splits = text_splitter.split_documents(docs_list)\\n\\n# Embedding\\n\"\"\"\\nembedding=NomicEmbeddings(\\n    model=\"nomic-embed-text-v1.5\",\\n    inference_mode=\"local\",\\n)\\n\"\"\"\\nembedding = OpenAIEmbeddings()\\n\\n# Add the document chunks to the \"vector store\"\\nvectorstore = SKLearnVectorStore.from_documents(\\n    documents=doc_splits,\\n    embedding=embedding,\\n)\\nretriever = vectorstore.as_retriever(k=4) USER_AGENT environment variable not set, consider setting it to identify your requests. langchain_mistralai.chat_models ChatMistralAI \"\"\"You are a teacher grading a quiz. You will be given: 1/ a QUESTION 2/ A FACT provided by the student You are grading RELEVANCE RECALL: A score of 1 means that ANY of the statements in the FACT are relevant to the QUESTION. A score of 0 means that NONE of the statements in the FACT are relevant to the QUESTION. 1 is the highest (best) score. 0 is the lowest score you can give. Explain your reasoning in a step-by-step manner. Ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset. Provide the binary score as a JSON with a single key \\'score\\' and no premable or explanation. ### Retrieval Grader\\n\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain_community.chat_models import ChatOllama\\nfrom langchain_core.output_parsers import JsonOutputParser\\nfrom langchain_mistralai.chat_models import ChatMistralAI\\n\\n# LLM\\nllm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\\n\\n# Prompt\\nprompt = PromptTemplate(\\n    template=\"\"\"You are a teacher grading a quiz. You will be given: \\n    1/ a QUESTION\\n    2/ A FACT provided by the student\\n    \\n    You are grading RELEVANCE RECALL:\\n    A score of 1 means that ANY of the statements in the FACT are relevant to the QUESTION. \\n    A score of 0 means that NONE of the statements in the FACT are relevant to the QUESTION. \\n    1 is the highest (best) score. 0 is the lowest score you can give. \\n    \\n    Explain your reasoning in a step-by-step manner. Ensure your reasoning and conclusion are correct. \\n    \\n    Avoid simply stating the correct answer at the outset.\\n    \\n    Question: {question} \\\\n\\n    Fact: \\\\n\\\\n {documents} \\\\n\\\\n\\n    \\n    Give a binary score \\'yes\\' or \\'no\\' score to indicate whether the document is relevant to the question. \\\\n\\n    Provide the binary score as a JSON with a single key \\'score\\' and no premable or explanation.\\n    \"\"\",\\n    input_variables=[\"question\", \"documents\"],\\n)\\n\\nretrieval_grader = prompt | llm | JsonOutputParser()\\nquestion = \"agent memory\"\\ndocs = retriever.invoke(question)\\ndoc_txt = docs[1].page_content\\nprint(retrieval_grader.invoke({\"question\": question, \"documents\": doc_txt})) {\\'score\\': \\'1\\'} \"\"\"You are an assistant for question-answering tasks. Use the following documents to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise: ### Generate\\n\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n# Prompt\\nprompt = PromptTemplate(\\n    template=\"\"\"You are an assistant for question-answering tasks. \\n    \\n    Use the following documents to answer the question. \\n    \\n    If you don\\'t know the answer, just say that you don\\'t know. \\n    \\n    Use three sentences maximum and keep the answer concise:\\n    Question: {question} \\n    Documents: {documents} \\n    Answer: \\n    \"\"\",\\n    input_variables=[\"question\", \"documents\"],\\n)\\n\\n# LLM\\nllm = ChatOllama(model=local_llm, temperature=0)\\n\\n# Chain\\nrag_chain = prompt | llm | StrOutputParser()\\n\\n# Run\\ngeneration = rag_chain.invoke({\"documents\": docs, \"question\": question})\\nprint(generation) The document mentions \"memory stream\" which is a long-term memory module that records a comprehensive list of agents\\' experience in natural language. It also discusses short-term memory and long-term memory, with the latter providing the agent with the capability to retain and recall information over extended periods. Additionally, it mentions planning and reflection mechanisms that enable agents to behave conditioned on past experience. Here we\\'ll explicitly define the majority of the control flow, only using an LLM to define a single branch point following grading. IPython.display search: whether to add search \"retrieve_documents\" \"generate_answer\" \"grade_document_retrieval\" custom_graph draw_mermaid_png from typing import List\\nfrom typing_extensions import TypedDict\\nfrom IPython.display import Image, display\\nfrom langchain.schema import Document\\nfrom langgraph.graph import START, END, StateGraph\\n\\n\\nclass GraphState(TypedDict):\\n    \"\"\"\\n    Represents the state of our graph.\\n\\n    Attributes:\\n        question: question\\n        generation: LLM generation\\n        search: whether to add search\\n        documents: list of documents\\n    \"\"\"\\n\\n    question: str\\n    generation: str\\n    search: str\\n    documents: List[str]\\n    steps: List[str]\\n\\n\\ndef retrieve(state):\\n    \"\"\"\\n    Retrieve documents\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): New key added to state, documents, that contains retrieved documents\\n    \"\"\"\\n    question = state[\"question\"]\\n    documents = retriever.invoke(question)\\n    steps = state[\"steps\"]\\n    steps.append(\"retrieve_documents\")\\n    return {\"documents\": documents, \"question\": question, \"steps\": steps}\\n\\n\\ndef generate(state):\\n    \"\"\"\\n    Generate answer\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): New key added to state, generation, that contains LLM generation\\n    \"\"\"\\n\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n    generation = rag_chain.invoke({\"documents\": documents, \"question\": question})\\n    steps = state[\"steps\"]\\n    steps.append(\"generate_answer\")\\n    return {\\n        \"documents\": documents,\\n        \"question\": question,\\n        \"generation\": generation,\\n        \"steps\": steps,\\n    }\\n\\n\\ndef grade_documents(state):\\n    \"\"\"\\n    Determines whether the retrieved documents are relevant to the question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates documents key with only filtered relevant documents\\n    \"\"\"\\n\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n    steps = state[\"steps\"]\\n    steps.append(\"grade_document_retrieval\")\\n    filtered_docs = []\\n    search = \"No\"\\n    for d in documents:\\n        score = retrieval_grader.invoke(\\n            {\"question\": question, \"documents\": d.page_content}\\n        )\\n        grade = score[\"score\"]\\n        if grade == \"yes\":\\n            filtered_docs.append(d)\\n        else:\\n            search = \"Yes\"\\n            continue\\n    return {\\n        \"documents\": filtered_docs,\\n        \"question\": question,\\n        \"search\": search,\\n        \"steps\": steps,\\n    }\\n\\n\\ndef web_search(state):\\n    \"\"\"\\n    Web search based on the re-phrased question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates documents key with appended web results\\n    \"\"\"\\n\\n    question = state[\"question\"]\\n    documents = state.get(\"documents\", [])\\n    steps = state[\"steps\"]\\n    steps.append(\"web_search\")\\n    web_results = web_search_tool.invoke({\"query\": question})\\n    documents.extend(\\n        [\\n            Document(page_content=d[\"content\"], metadata={\"url\": d[\"url\"]})\\n            for d in web_results\\n        ]\\n    )\\n    return {\"documents\": documents, \"question\": question, \"steps\": steps}\\n\\n\\ndef decide_to_generate(state):\\n    \"\"\"\\n    Determines whether to generate an answer, or re-generate a question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        str: Binary decision for next node to call\\n    \"\"\"\\n    search = state[\"search\"]\\n    if search == \"Yes\":\\n        return \"search\"\\n    else:\\n        return \"generate\"\\n\\n\\n# Graph\\nworkflow = StateGraph(GraphState)\\n\\n# Define the nodes\\nworkflow.add_node(\"retrieve\", retrieve)  # retrieve\\nworkflow.add_node(\"grade_documents\", grade_documents)  # grade documents\\nworkflow.add_node(\"generate\", generate)  # generatae\\nworkflow.add_node(\"web_search\", web_search)  # web search\\n\\n# Build graph\\nworkflow.add_edge(START, \"retrieve\")\\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\\nworkflow.add_conditional_edges(\\n    \"grade_documents\",\\n    decide_to_generate,\\n    {\\n        \"search\": \"web_search\",\\n        \"generate\": \"generate\",\\n    },\\n)\\nworkflow.add_edge(\"web_search\", \"generate\")\\nworkflow.add_edge(\"generate\", END)\\n\\ncustom_graph = workflow.compile()\\n\\ndisplay(Image(custom_graph.get_graph(xray=True).draw_mermaid_png())) predict_custom_agent_local_answer \"configurable\" \"thread_id\" import uuid\\n\\n\\ndef predict_custom_agent_local_answer(example: dict):\\n    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\\n    state_dict = custom_graph.invoke(\\n        {\"question\": example[\"input\"], \"steps\": []}, config\\n    )\\n    return {\"response\": state_dict[\"generation\"], \"steps\": state_dict[\"steps\"]}\\n\\n\\nexample = {\"input\": \"What are the types of agent memory?\"}\\nresponse = predict_custom_agent_local_answer(example)\\nresponse {\\'response\\': \\'According to the documents, there are two types of agent memory:\\\\n\\\\n* Short-term memory (STM): This is a data structure that holds information temporarily and allows the agent to process it when needed.\\\\n* Long-term memory (LTM): This provides the agent with the capability to retain and recall information over extended periods.\\\\n\\\\nThese types of memories allow the agent to learn, reason, and make decisions.\\',\\n \\'steps\\': [\\'retrieve_documents\\',\\n  \\'grade_document_retrieval\\',\\n  \\'web_search\\',\\n  \\'generate_answer\\']} https://smith.langchain.com/public/88e7579e-2571-4cf6-98d2-1f9ce3359967/r Now we\\'ve defined two different agent architectures that do roughly the same thing! We can evaluate them. See our conceptual guide for context on agent evaluation. First, we can assess how well our agent performs on a set of question-answer pairs We\\'ll create a dataset and save it in LangSmith. # Create a dataset \"How does the ReAct agent use self-reflection? \" \"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool outputs.\" \"What are the types of biases that can arise with few-shot prompting?\" \"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\" \"What are five types of adversarial attacks?\" \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\" \"Who did the Chicago Bears draft first in the 2024 NFL draft”?\" \"The Chicago Bears drafted Caleb Williams first in the 2024 NFL draft.\" \"Who won the 2024 NBA finals?\" \"The Boston Celtics on the 2024 NBA finals\" dataset_name \"Corrective RAG Agent Testing\" has_dataset create_dataset create_examples from langsmith import Client\\n\\nclient = Client()\\n\\n# Create a dataset\\nexamples = [\\n    (\\n        \"How does the ReAct agent use self-reflection? \",\\n        \"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool outputs.\",\\n    ),\\n    (\\n        \"What are the types of biases that can arise with few-shot prompting?\",\\n        \"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\",\\n    ),\\n    (\\n        \"What are five types of adversarial attacks?\",\\n        \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\",\\n    ),\\n    (\\n        \"Who did the Chicago Bears draft first in the 2024 NFL draft”?\",\\n        \"The Chicago Bears drafted Caleb Williams first in the 2024 NFL draft.\",\\n    ),\\n    (\"Who won the 2024 NBA finals?\", \"The Boston Celtics on the 2024 NBA finals\"),\\n]\\n\\n# Save it\\ndataset_name = \"Corrective RAG Agent Testing\"\\nif not client.has_dataset(dataset_name=dataset_name):\\n    dataset = client.create_dataset(dataset_name=dataset_name)\\n    inputs, outputs = zip(\\n        *[({\"input\": text}, {\"output\": label}) for text, label in examples]\\n    )\\n    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id) Now, we\\'ll use an LLM as a grader to compare both agent responses to our ground truth reference answer. is the default prompt that we can use. as our LLM grader. # Grade prompt grade_prompt_answer_accuracy \"langchain-ai/rag-answer-vs-reference\" answer_evaluator A simple evaluator for RAG answer accuracy # Get the question, the ground truth reference answer, RAG chain answer prediction input_question # Define an LLM grader # Run evaluator \"correct_answer\" \"student_answer\" \"answer_v_reference_score\" from langchain import hub\\nfrom langchain_openai import ChatOpenAI\\n\\n# Grade prompt\\ngrade_prompt_answer_accuracy = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\\n\\n\\ndef answer_evaluator(run, example) -> dict:\\n    \"\"\"\\n    A simple evaluator for RAG answer accuracy\\n    \"\"\"\\n\\n    # Get the question, the ground truth reference answer, RAG chain answer prediction\\n    input_question = example.inputs[\"input\"]\\n    reference = example.outputs[\"output\"]\\n    prediction = run.outputs[\"response\"]\\n\\n    # Define an LLM grader\\n    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\\n    answer_grader = grade_prompt_answer_accuracy | llm\\n\\n    # Run evaluator\\n    score = answer_grader.invoke(\\n        {\\n            \"question\": input_question,\\n            \"correct_answer\": reference,\\n            \"student_answer\": prediction,\\n        }\\n    )\\n    score = score[\"Score\"]\\n    return {\"key\": \"answer_v_reference_score\", \"score\": score} we can assess the list of tool calls that each agent makes relative to expected trajectories. This evaluates the specific reasoning traces taken by our agents! langsmith.schemas # Reasoning traces that we expect the agents to take expected_trajectory_1 expected_trajectory_2 check_trajectory_react Check if all expected tools are called in exact order and without any additional tool calls. find_tool_calls_react \"Tool calls ReAct agent: \"tool_calls_in_exact_order\" check_trajectory_custom \"Tool calls custom agent: from langsmith.schemas import Example, Run\\n\\n# Reasoning traces that we expect the agents to take\\nexpected_trajectory_1 = [\\n    \"retrieve_documents\",\\n    \"grade_document_retrieval\",\\n    \"web_search\",\\n    \"generate_answer\",\\n]\\nexpected_trajectory_2 = [\\n    \"retrieve_documents\",\\n    \"grade_document_retrieval\",\\n    \"generate_answer\",\\n]\\n\\n\\ndef check_trajectory_react(root_run: Run, example: Example) -> dict:\\n    \"\"\"\\n    Check if all expected tools are called in exact order and without any additional tool calls.\\n    \"\"\"\\n    messages = root_run.outputs[\"messages\"]\\n    tool_calls = find_tool_calls_react(messages)\\n    print(f\"Tool calls ReAct agent: {tool_calls}\")\\n    if tool_calls == expected_trajectory_1 or tool_calls == expected_trajectory_2:\\n        score = 1\\n    else:\\n        score = 0\\n\\n    return {\"score\": int(score), \"key\": \"tool_calls_in_exact_order\"}\\n\\n\\ndef check_trajectory_custom(root_run: Run, example: Example) -> dict:\\n    \"\"\"\\n    Check if all expected tools are called in exact order and without any additional tool calls.\\n    \"\"\"\\n    tool_calls = root_run.outputs[\"steps\"]\\n    print(f\"Tool calls custom agent: {tool_calls}\")\\n    if tool_calls == expected_trajectory_1 or tool_calls == expected_trajectory_2:\\n        score = 1\\n    else:\\n        score = 0\\n\\n    return {\"score\": int(score), \"key\": \"tool_calls_in_exact_order\"} langsmith.evaluation experiment_prefix \"custom-agent- experiment_results \"-answer-and-tool-use\" num_repetitions max_concurrency # Use when running locally from langsmith.evaluation import evaluate\\n\\nexperiment_prefix = f\"custom-agent-{model_tested}\"\\nexperiment_results = evaluate(\\n    predict_custom_agent_local_answer,\\n    data=dataset_name,\\n    evaluators=[answer_evaluator, check_trajectory_custom],\\n    experiment_prefix=experiment_prefix + \"-answer-and-tool-use\",\\n    num_repetitions=3,\\n    max_concurrency=1,  # Use when running locally\\n    metadata={\"version\": metadata},\\n) View the evaluation results for experiment: \\'custom-agent-llama3-8b-answer-and-tool-use-d6006159\\' at:\\nhttps://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/a8b9273b-ca33-4e2f-9f69-9bbc37f6f51b/compare?selectedSessions=83c60822-ef22-43e8-ac85-4488af279c6f 0it [00:00, ?it/s] Tool calls custom agent: [\\'retrieve_documents\\', \\'grade_document_retrieval\\', \\'web_search\\', \\'generate_answer\\']\\nTool calls custom agent: [\\'retrieve_documents\\', \\'grade_document_retrieval\\', \\'web_search\\', \\'generate_answer\\']\\nTool calls custom agent: [\\'retrieve_documents\\', \\'grade_document_retrieval\\', \\'web_search\\', \\'generate_answer\\']\\nTool calls custom agent: [\\'retrieve_documents\\', \\'grade_document_retrieval\\', \\'web_search\\', \\'generate_answer\\']\\nTool calls custom agent: [\\'retrieve_documents\\', \\'grade_document_retrieval\\', \\'web_search\\', \\'generate_answer\\']\\nTool calls custom agent: [\\'retrieve_documents\\', \\'grade_document_retrieval\\', \\'web_search\\', \\'generate_answer\\']\\nTool calls custom agent: [\\'retrieve_documents\\', \\'grade_document_retrieval\\', \\'web_search\\', \\'generate_answer\\']\\nTool calls custom agent: [\\'retrieve_documents\\', \\'grade_document_retrieval\\', \\'web_search\\', \\'generate_answer\\']\\nTool calls custom agent: [\\'retrieve_documents\\', \\'grade_document_retrieval\\', \\'web_search\\', \\'generate_answer\\']\\nTool calls custom agent: [\\'retrieve_documents\\', \\'grade_document_retrieval\\', \\'web_search\\', \\'generate_answer\\']\\nTool calls custom agent: [\\'retrieve_documents\\', \\'grade_document_retrieval\\', \\'web_search\\', \\'generate_answer\\']\\nTool calls custom agent: [\\'retrieve_documents\\', \\'grade_document_retrieval\\', \\'web_search\\', \\'generate_answer\\']\\nTool calls custom agent: [\\'retrieve_documents\\', \\'grade_document_retrieval\\', \\'web_search\\', \\'generate_answer\\']\\nTool calls custom agent: [\\'retrieve_documents\\', \\'grade_document_retrieval\\', \\'web_search\\', \\'generate_answer\\']\\nTool calls custom agent: [\\'retrieve_documents\\', \\'grade_document_retrieval\\', \\'web_search\\', \\'generate_answer\\'] We can see the results benchmarked against Llama-3-70b agent (as shown here) and ReAct. local custom agent performs well in terms of tool calling reliability: it follows the expected reasoning traces. However, the answer accuracy performance lags the larger models with custom agent implementations.\\n\\n    Access time: 2024-08-05 18:23:33.017527103-04:00\\n\\n    Web Page text: Please turn JavaScript on and reload the page. Please enable Cookies and reload the page.\\n\\n    Access time: 2024-08-06 13:43:45.099234104-04:00\\n\\n    Web Page text: Agent state Nodes and Edges Retrieval Agents are useful when we want to make decisions about whether to retrieve from an index. To implement a retrieval agent, we simple need to give an LLM access to a retriever tool. We can incorporate this into %%capture --no-stderr\\n%pip install -U --quiet langchain-community tiktoken langchain-openai langchainhub chromadb langchain langgraph langchain-text-splitters # (Optional) For tracing import getpass\\nimport os\\n\\n\\ndef _set_env(key: str):\\n    if key not in os.environ:\\n        os.environ[key] = getpass.getpass(f\"{key}:\")\\n\\n\\n_set_env(\"OPENAI_API_KEY\")\\n\\n# (Optional) For tracing\\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\\n_set_env(\"LANGCHAIN_API_KEY\") First, we index 3 blog posts. langchain_text_splitters from langchain_community.document_loaders import WebBaseLoader\\nfrom langchain_community.vectorstores import Chroma\\nfrom langchain_openai import OpenAIEmbeddings\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\nurls = [\\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\\n]\\n\\ndocs = [WebBaseLoader(url).load() for url in urls]\\ndocs_list = [item for sublist in docs for item in sublist]\\n\\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\\n    chunk_size=100, chunk_overlap=50\\n)\\ndoc_splits = text_splitter.split_documents(docs_list)\\n\\n# Add to vectorDB\\nvectorstore = Chroma.from_documents(\\n    documents=doc_splits,\\n    collection_name=\"rag-chroma\",\\n    embedding=OpenAIEmbeddings(),\\n)\\nretriever = vectorstore.as_retriever() Then we create a retriever tool. langchain.tools.retriever create_retriever_tool retriever_tool \"retrieve_blog_posts\" \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.\" from langchain.tools.retriever import create_retriever_tool\\n\\nretriever_tool = create_retriever_tool(\\n    retriever,\\n    \"retrieve_blog_posts\",\\n    \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.\",\\n)\\n\\ntools = [retriever_tool] We will defined a graph. object that it passes around to each node. Our state will be a list of Each node in our graph will append to it. langchain_core.messages BaseMessage langgraph.graph.message add_messages # The add_messages function defines how an update should be processed # Default is to replace. add_messages says \"append\" from typing import Annotated, Sequence, TypedDict\\n\\nfrom langchain_core.messages import BaseMessage\\n\\nfrom langgraph.graph.message import add_messages\\n\\n\\nclass AgentState(TypedDict):\\n    # The add_messages function defines how an update should be processed\\n    # Default is to replace. add_messages says \"append\"\\n    messages: Annotated[Sequence[BaseMessage], add_messages] We can lay out an agentic RAG graph like this: The state is a set of messages Each node will update (append to) state Conditional edges decide which node to visit next HumanMessage langgraph.prebuilt tools_condition state (messages): The current state str: A decision for whether the documents are relevant or not \"---CHECK RELEVANCE---\" \"\"\"Binary score for relevance check.\"\"\" \"Relevance score \\'yes\\' or \\'no\\'\" \"gpt-4-0125-preview\" # LLM with tool and validation llm_with_tool last_message scored_result \"---DECISION: DOCS RELEVANT---\" \"---DECISION: DOCS NOT RELEVANT---\" Invokes the agent model to generate a response based on the current state. Given the question, it will decide to retrieve using the retriever tool, or simply end. dict: The updated state with the agent response appended to messages \"---CALL AGENT---\" \"gpt-4-turbo\" # We return a list, because this will get added to the existing list dict: The updated state with re-phrased question Look at the input and try to reason about the underlying semantic intent / meaning. Formulate an improved question: \"\"\" \"Prompt[rlm/rag-prompt]\" pretty_print # Show what the prompt looks like from typing import Annotated, Literal, Sequence, TypedDict\\n\\nfrom langchain import hub\\nfrom langchain_core.messages import BaseMessage, HumanMessage\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\nfrom langchain_openai import ChatOpenAI\\n\\nfrom langgraph.prebuilt import tools_condition\\n\\n### Edges\\n\\n\\ndef grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\\n    \"\"\"\\n    Determines whether the retrieved documents are relevant to the question.\\n\\n    Args:\\n        state (messages): The current state\\n\\n    Returns:\\n        str: A decision for whether the documents are relevant or not\\n    \"\"\"\\n\\n    print(\"---CHECK RELEVANCE---\")\\n\\n    # Data model\\n    class grade(BaseModel):\\n        \"\"\"Binary score for relevance check.\"\"\"\\n\\n        binary_score: str = Field(description=\"Relevance score \\'yes\\' or \\'no\\'\")\\n\\n    # LLM\\n    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\\n\\n    # LLM with tool and validation\\n    llm_with_tool = model.with_structured_output(grade)\\n\\n    # Prompt\\n    prompt = PromptTemplate(\\n        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\\\n \\n        Here is the retrieved document: \\\\n\\\\n {context} \\\\n\\\\n\\n        Here is the user question: {question} \\\\n\\n        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\\\n\\n        Give a binary score \\'yes\\' or \\'no\\' score to indicate whether the document is relevant to the question.\"\"\",\\n        input_variables=[\"context\", \"question\"],\\n    )\\n\\n    # Chain\\n    chain = prompt | llm_with_tool\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n\\n    question = messages[0].content\\n    docs = last_message.content\\n\\n    scored_result = chain.invoke({\"question\": question, \"context\": docs})\\n\\n    score = scored_result.binary_score\\n\\n    if score == \"yes\":\\n        print(\"---DECISION: DOCS RELEVANT---\")\\n        return \"generate\"\\n\\n    else:\\n        print(\"---DECISION: DOCS NOT RELEVANT---\")\\n        print(score)\\n        return \"rewrite\"\\n\\n\\n### Nodes\\n\\n\\ndef agent(state):\\n    \"\"\"\\n    Invokes the agent model to generate a response based on the current state. Given\\n    the question, it will decide to retrieve using the retriever tool, or simply end.\\n\\n    Args:\\n        state (messages): The current state\\n\\n    Returns:\\n        dict: The updated state with the agent response appended to messages\\n    \"\"\"\\n    print(\"---CALL AGENT---\")\\n    messages = state[\"messages\"]\\n    model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4-turbo\")\\n    model = model.bind_tools(tools)\\n    response = model.invoke(messages)\\n    # We return a list, because this will get added to the existing list\\n    return {\"messages\": [response]}\\n\\n\\ndef rewrite(state):\\n    \"\"\"\\n    Transform the query to produce a better question.\\n\\n    Args:\\n        state (messages): The current state\\n\\n    Returns:\\n        dict: The updated state with re-phrased question\\n    \"\"\"\\n\\n    print(\"---TRANSFORM QUERY---\")\\n    messages = state[\"messages\"]\\n    question = messages[0].content\\n\\n    msg = [\\n        HumanMessage(\\n            content=f\"\"\" \\\\n \\n    Look at the input and try to reason about the underlying semantic intent / meaning. \\\\n \\n    Here is the initial question:\\n    \\\\n ------- \\\\n\\n    {question} \\n    \\\\n ------- \\\\n\\n    Formulate an improved question: \"\"\",\\n        )\\n    ]\\n\\n    # Grader\\n    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\\n    response = model.invoke(msg)\\n    return {\"messages\": [response]}\\n\\n\\ndef generate(state):\\n    \"\"\"\\n    Generate answer\\n\\n    Args:\\n        state (messages): The current state\\n\\n    Returns:\\n         dict: The updated state with re-phrased question\\n    \"\"\"\\n    print(\"---GENERATE---\")\\n    messages = state[\"messages\"]\\n    question = messages[0].content\\n    last_message = messages[-1]\\n\\n    question = messages[0].content\\n    docs = last_message.content\\n\\n    # Prompt\\n    prompt = hub.pull(\"rlm/rag-prompt\")\\n\\n    # LLM\\n    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True)\\n\\n    # Post-processing\\n    def format_docs(docs):\\n        return \"\\\\n\\\\n\".join(doc.page_content for doc in docs)\\n\\n    # Chain\\n    rag_chain = prompt | llm | StrOutputParser()\\n\\n    # Run\\n    response = rag_chain.invoke({\"context\": docs, \"question\": question})\\n    return {\"messages\": [response]}\\n\\n\\nprint(\"*\" * 20 + \"Prompt[rlm/rag-prompt]\" + \"*\" * 20)\\nprompt = hub.pull(\"rlm/rag-prompt\").pretty_print()  # Show what the prompt looks like ********************Prompt[rlm/rag-prompt]********************\\n================================ Human Message =================================\\n\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.\\nQuestion: Start with an agent, Agent make a decision to call a function If so, then to call tool (retriever) Then call agent with the tool output added to messages ( # Define a new graph # Define the nodes we will cycle between # retrieval # Re-writing the question # Generating a response after we know the documents are relevant # Call agent node to decide to retrieve or not # Decide whether to retrieve # Assess agent decision # Translate the condition outputs to nodes in our graph # Edges taken after the `action` node is called. from langgraph.graph import END, StateGraph, START\\nfrom langgraph.prebuilt import ToolNode\\n\\n# Define a new graph\\nworkflow = StateGraph(AgentState)\\n\\n# Define the nodes we will cycle between\\nworkflow.add_node(\"agent\", agent)  # agent\\nretrieve = ToolNode([retriever_tool])\\nworkflow.add_node(\"retrieve\", retrieve)  # retrieval\\nworkflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\\nworkflow.add_node(\\n    \"generate\", generate\\n)  # Generating a response after we know the documents are relevant\\n# Call agent node to decide to retrieve or not\\nworkflow.add_edge(START, \"agent\")\\n\\n# Decide whether to retrieve\\nworkflow.add_conditional_edges(\\n    \"agent\",\\n    # Assess agent decision\\n    tools_condition,\\n    {\\n        # Translate the condition outputs to nodes in our graph\\n        \"tools\": \"retrieve\",\\n        END: END,\\n    },\\n)\\n\\n# Edges taken after the `action` node is called.\\nworkflow.add_conditional_edges(\\n    \"retrieve\",\\n    # Assess agent decision\\n    grade_documents,\\n)\\nworkflow.add_edge(\"generate\", END)\\nworkflow.add_edge(\"rewrite\", \"agent\")\\n\\n# Compile\\ngraph = workflow.compile() # This requires some extra dependencies and is optional from IPython.display import Image, display\\n\\ntry:\\n    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\\nexcept Exception:\\n    # This requires some extra dependencies and is optional\\n    pass \"What does Lilian Weng say about the types of agent memory?\" \"Output from node \\' import pprint\\n\\ninputs = {\\n    \"messages\": [\\n        (\"user\", \"What does Lilian Weng say about the types of agent memory?\"),\\n    ]\\n}\\nfor output in graph.stream(inputs):\\n    for key, value in output.items():\\n        pprint.pprint(f\"Output from node \\'{key}\\':\")\\n        pprint.pprint(\"---\")\\n        pprint.pprint(value, indent=2, width=80, depth=None)\\n    pprint.pprint(\"\\\\n---\\\\n\") ---CALL AGENT---\\n\"Output from node \\'agent\\':\"\\n\\'---\\'\\n{ \\'messages\\': [ AIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'index\\': 0, \\'id\\': \\'call_z36oPZN8l1UC6raxrebqc1bH\\', \\'function\\': {\\'arguments\\': \\'{\"query\":\"types of agent memory\"}\\', \\'name\\': \\'retrieve_blog_posts\\'}, \\'type\\': \\'function\\'}]}, response_metadata={\\'finish_reason\\': \\'tool_calls\\'}, id=\\'run-2bad2518-8187-4d8f-8e23-2b9501becb6f-0\\', tool_calls=[{\\'name\\': \\'retrieve_blog_posts\\', \\'args\\': {\\'query\\': \\'types of agent memory\\'}, \\'id\\': \\'call_z36oPZN8l1UC6raxrebqc1bH\\'}])]}\\n\\'\\\\n---\\\\n\\'\\n---CHECK RELEVANCE---\\n---DECISION: DOCS RELEVANT---\\n\"Output from node \\'retrieve\\':\"\\n\\'---\\'\\n{ \\'messages\\': [ ToolMessage(content=\\'Table of Contents\\\\n\\\\n\\\\n\\\\nAgent System Overview\\\\n\\\\nComponent One: Planning\\\\n\\\\nTask Decomposition\\\\n\\\\nSelf-Reflection\\\\n\\\\n\\\\nComponent Two: Memory\\\\n\\\\nTypes of Memory\\\\n\\\\nMaximum Inner Product Search (MIPS)\\\\n\\\\n\\\\nComponent Three: Tool Use\\\\n\\\\nCase Studies\\\\n\\\\nScientific Discovery Agent\\\\n\\\\nGenerative Agents Simulation\\\\n\\\\nProof-of-Concept Examples\\\\n\\\\n\\\\nChallenges\\\\n\\\\nCitation\\\\n\\\\nReferences\\\\n\\\\nPlanning\\\\n\\\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\\\n\\\\n\\\\nMemory\\\\n\\\\nMemory\\\\n\\\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\\\n\\\\n\\\\nTool use\\\\n\\\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\', name=\\'retrieve_blog_posts\\', id=\\'d815f283-868c-4660-a1c6-5f6e5373ca06\\', tool_call_id=\\'call_z36oPZN8l1UC6raxrebqc1bH\\')]}\\n\\'\\\\n---\\\\n\\'\\n---GENERATE---\\n\"Output from node \\'generate\\':\"\\n\\'---\\'\\n{ \\'messages\\': [ \\'Lilian Weng discusses short-term and long-term memory in \\'\\n                \\'agent systems. Short-term memory is used for in-context \\'\\n                \\'learning, while long-term memory allows agents to retain and \\'\\n                \\'recall information over extended periods.\\']}\\n\\'\\\\n---\\\\n\\'\\n\\n    Access time: 2024-08-06 13:46:04.360167980-04:00\\n\\n    Web Page text: Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. , a few decisions are made: Should I retrieve from retriever, x (question) y (generation) Decides when to retrieve chunks with yes, no, continue Are the retrieved passages relevant to the question provides useful information to solve relevant, irrelevant Are the LLM generation from each chunk in is relevant to the chunk (hallucinations, etc)  - All of the verification-worthy statements in are supported by {fully supported, partially supported, no support The LLM generation from each chunk in is a useful response to {5, 4, 3, 2, 1} ! pip install -U langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph import os\\n\\nos.environ[\"OPENAI_API_KEY\"] = \" os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\\nos.environ[\"LANGCHAIN_API_KEY\"] = \" from langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain_community.vectorstores import Chroma\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nurls = [\\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\\n]\\n\\ndocs = [WebBaseLoader(url).load() for url in urls]\\ndocs_list = [item for sublist in docs for item in sublist]\\n\\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\\n    chunk_size=250, chunk_overlap=0\\n)\\ndoc_splits = text_splitter.split_documents(docs_list)\\n\\n# Add to vectorDB\\nvectorstore = Chroma.from_documents(\\n    documents=doc_splits,\\n    collection_name=\"rag-chroma\",\\n    embedding=OpenAIEmbeddings(),\\n)\\nretriever = vectorstore.as_retriever() ### Retrieval Grader\\n\\n\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.pydantic_v1 import BaseModel, Field\\nfrom langchain_openai import ChatOpenAI\\n\\n\\n# Data model\\nclass GradeDocuments(BaseModel):\\n    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\\n\\n    binary_score: str = Field(\\n        description=\"Documents are relevant to the question, \\'yes\\' or \\'no\\'\"\\n    )\\n\\n\\n# LLM with function call\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\\nstructured_llm_grader = llm.with_structured_output(GradeDocuments)\\n\\n# Prompt\\nsystem = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\\\n \\n    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\\\n\\n    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\\\n\\n    Give a binary score \\'yes\\' or \\'no\\' score to indicate whether the document is relevant to the question.\"\"\"\\ngrade_prompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system),\\n        (\"human\", \"Retrieved document: \\\\n\\\\n {document} \\\\n\\\\n User question: {question}\"),\\n    ]\\n)\\n\\nretrieval_grader = grade_prompt | structured_llm_grader\\nquestion = \"agent memory\"\\ndocs = retriever.get_relevant_documents(question)\\ndoc_txt = docs[1].page_content\\nprint(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})) /Users/rlm/miniforge3/envs/llama2/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\\n  warn_deprecated( binary_score=\\'yes\\' The design of generative agents combines LLM with memory, planning, and reflection mechanisms to enable agents to behave conditioned on past experience and interact with other agents. Long-term memory provides the agent with the capability to retain and recall infinite information over extended periods. Short-term memory is utilized for in-context learning. ### Nodes\\n\\n\\ndef retrieve(state):\\n    \"\"\"\\n    Retrieve documents\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): New key added to state, documents, that contains retrieved documents\\n    \"\"\"\\n    print(\"---RETRIEVE---\")\\n    question = state[\"question\"]\\n\\n    # Retrieval\\n    documents = retriever.get_relevant_documents(question)\\n    return {\"documents\": documents, \"question\": question}\\n\\n\\ndef generate(state):\\n    \"\"\"\\n    Generate answer\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): New key added to state, generation, that contains LLM generation\\n    \"\"\"\\n    print(\"---GENERATE---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # RAG generation\\n    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\\n    return {\"documents\": documents, \"question\": question, \"generation\": generation}\\n\\n\\ndef grade_documents(state):\\n    \"\"\"\\n    Determines whether the retrieved documents are relevant to the question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates documents key with only filtered relevant documents\\n    \"\"\"\\n\\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # Score each doc\\n    filtered_docs = []\\n    for d in documents:\\n        score = retrieval_grader.invoke(\\n            {\"question\": question, \"document\": d.page_content}\\n        )\\n        grade = score.binary_score\\n        if grade == \"yes\":\\n            print(\"---GRADE: DOCUMENT RELEVANT---\")\\n            filtered_docs.append(d)\\n        else:\\n            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\\n            continue\\n    return {\"documents\": filtered_docs, \"question\": question}\\n\\n\\ndef transform_query(state):\\n    \"\"\"\\n    Transform the query to produce a better question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates question key with a re-phrased question\\n    \"\"\"\\n\\n    print(\"---TRANSFORM QUERY---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # Re-write question\\n    better_question = question_rewriter.invoke({\"question\": question})\\n    return {\"documents\": documents, \"question\": better_question}\\n\\n\\n### Edges\\n\\n\\ndef decide_to_generate(state):\\n    \"\"\"\\n    Determines whether to generate an answer, or re-generate a question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        str: Binary decision for next node to call\\n    \"\"\"\\n\\n    print(\"---ASSESS GRADED DOCUMENTS---\")\\n    state[\"question\"]\\n    filtered_documents = state[\"documents\"]\\n\\n    if not filtered_documents:\\n        # All documents have been filtered check_relevance\\n        # We will re-generate a new query\\n        print(\\n            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\\n        )\\n        return \"transform_query\"\\n    else:\\n        # We have relevant documents, so generate answer\\n        print(\"---DECISION: GENERATE---\")\\n        return \"generate\"\\n\\n\\ndef grade_generation_v_documents_and_question(state):\\n    \"\"\"\\n    Determines whether the generation is grounded in the document and answers question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        str: Decision for next node to call\\n    \"\"\"\\n\\n    print(\"---CHECK HALLUCINATIONS---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n    generation = state[\"generation\"]\\n\\n    score = hallucination_grader.invoke(\\n        {\"documents\": documents, \"generation\": generation}\\n    )\\n    grade = score.binary_score\\n\\n    # Check hallucination\\n    if grade == \"yes\":\\n        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\\n        # Check question-answering\\n        print(\"---GRADE GENERATION vs QUESTION---\")\\n        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\\n        grade = score.binary_score\\n        if grade == \"yes\":\\n            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\\n            return \"useful\"\\n        else:\\n            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\\n            return \"not useful\"\\n    else:\\n        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\\n        return \"not supported\" The just follows the flow we outlined in the figure above. from langgraph.graph import END, StateGraph, START\\n\\nworkflow = StateGraph(GraphState)\\n\\n# Define the nodes\\nworkflow.add_node(\"retrieve\", retrieve)  # retrieve\\nworkflow.add_node(\"grade_documents\", grade_documents)  # grade documents\\nworkflow.add_node(\"generate\", generate)  # generatae\\nworkflow.add_node(\"transform_query\", transform_query)  # transform_query\\n\\n# Build graph\\nworkflow.add_edge(START, \"retrieve\")\\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\\nworkflow.add_conditional_edges(\\n    \"grade_documents\",\\n    decide_to_generate,\\n    {\\n        \"transform_query\": \"transform_query\",\\n        \"generate\": \"generate\",\\n    },\\n)\\nworkflow.add_edge(\"transform_query\", \"retrieve\")\\nworkflow.add_conditional_edges(\\n    \"generate\",\\n    grade_generation_v_documents_and_question,\\n    {\\n        \"not supported\": \"generate\",\\n        \"useful\": END,\\n        \"not useful\": \"transform_query\",\\n    },\\n)\\n\\n# Compile\\napp = workflow.compile() \"Explain how the different types of agent memory work?\" from pprint import pprint\\n\\n# Run\\ninputs = {\"question\": \"Explain how the different types of agent memory work?\"}\\nfor output in app.stream(inputs):\\n    for key, value in output.items():\\n        # Node\\n        pprint(f\"Node \\'{key}\\':\")\\n        # Optional: print full state at each node\\n        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\\n    pprint(\"\\\\n---\\\\n\")\\n\\n# Final generation\\npprint(value[\"generation\"]) ---RETRIEVE---\\n\"Node \\'retrieve\\':\"\\n\\'\\\\n---\\\\n\\'\\n---CHECK DOCUMENT RELEVANCE TO QUESTION---\\n---GRADE: DOCUMENT NOT RELEVANT---\\n---GRADE: DOCUMENT RELEVANT---\\n---GRADE: DOCUMENT NOT RELEVANT---\\n---GRADE: DOCUMENT RELEVANT---\\n---ASSESS GRADED DOCUMENTS---\\n---DECISION: GENERATE---\\n\"Node \\'grade_documents\\':\"\\n\\'\\\\n---\\\\n\\'\\n---GENERATE---\\n---CHECK HALLUCINATIONS---\\n---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\\n---GRADE GENERATION vs QUESTION---\\n---DECISION: GENERATION ADDRESSES QUESTION---\\n\"Node \\'generate\\':\"\\n\\'\\\\n---\\\\n\\'\\n(\\'Short-term memory is used for in-context learning in agents, allowing them \\'\\n \\'to learn quickly. Long-term memory enables agents to retain and recall vast \\'\\n \\'amounts of information over extended periods. Agents can also utilize \\'\\n \\'external tools like APIs to access additional information beyond what is \\'\\n \\'stored in their memory.\\') \"Explain how chain of thought prompting works?\" inputs = {\"question\": \"Explain how chain of thought prompting works?\"}\\nfor output in app.stream(inputs):\\n    for key, value in output.items():\\n        # Node\\n        pprint(f\"Node \\'{key}\\':\")\\n        # Optional: print full state at each node\\n        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\\n    pprint(\"\\\\n---\\\\n\")\\n\\n# Final generation\\npprint(value[\"generation\"]) ---RETRIEVE---\\n\"Node \\'retrieve\\':\"\\n\\'\\\\n---\\\\n\\'\\n---CHECK DOCUMENT RELEVANCE TO QUESTION---\\n---GRADE: DOCUMENT RELEVANT---\\n---GRADE: DOCUMENT NOT RELEVANT---\\n---GRADE: DOCUMENT RELEVANT---\\n---GRADE: DOCUMENT RELEVANT---\\n---ASSESS GRADED DOCUMENTS---\\n---DECISION: GENERATE---\\n\"Node \\'grade_documents\\':\"\\n\\'\\\\n---\\\\n\\'\\n---GENERATE---\\n---CHECK HALLUCINATIONS---\\n---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\\n---GRADE GENERATION vs QUESTION---\\n---DECISION: GENERATION ADDRESSES QUESTION---\\n\"Node \\'generate\\':\"\\n\\'\\\\n---\\\\n\\'\\n(\\'Chain of thought prompting works by repeatedly prompting the model to ask \\'\\n \\'follow-up questions to construct the thought process iteratively. This \\'\\n \\'method can be combined with queries to search for relevant entities and \\'\\n \\'content to add back into the context. It extends the thought process by \\'\\n \\'exploring multiple reasoning possibilities at each step, creating a tree \\'\\n \\'structure of thoughts.\\') LangSmith Traces - https://smith.langchain.com/public/55d6180f-aab8-42bc-8799-dadce6247d9b/r https://smith.langchain.com/public/1c6bf654-61b2-4fc5-9889-054b020c78aa/r\\n\\n    Access time: 2024-08-06 13:46:21.157686949-04:00\\n\\n    Web Page text: Self RAG using local LLMs %capture --no-stderr\\n%pip install -U langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph nomic[local] ### Generate\\n\\nfrom langchain import hub\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\n# Prompt\\nprompt = hub.pull(\"rlm/rag-prompt\")\\n\\n# LLM\\nllm = ChatOllama(model=local_llm, temperature=0)\\n\\n\\n# Post-processing\\ndef format_docs(docs):\\n    return \"\\\\n\\\\n\".join(doc.page_content for doc in docs)\\n\\n\\n# Chain\\nrag_chain = prompt | llm | StrOutputParser()\\n\\n# Run\\ngeneration = rag_chain.invoke({\"context\": docs, \"question\": question})\\nprint(generation) ### Nodes\\n\\n\\ndef retrieve(state):\\n    \"\"\"\\n    Retrieve documents\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): New key added to state, documents, that contains retrieved documents\\n    \"\"\"\\n    print(\"---RETRIEVE---\")\\n    question = state[\"question\"]\\n\\n    # Retrieval\\n    documents = retriever.get_relevant_documents(question)\\n    return {\"documents\": documents, \"question\": question}\\n\\n\\ndef generate(state):\\n    \"\"\"\\n    Generate answer\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): New key added to state, generation, that contains LLM generation\\n    \"\"\"\\n    print(\"---GENERATE---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # RAG generation\\n    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\\n    return {\"documents\": documents, \"question\": question, \"generation\": generation}\\n\\n\\ndef grade_documents(state):\\n    \"\"\"\\n    Determines whether the retrieved documents are relevant to the question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates documents key with only filtered relevant documents\\n    \"\"\"\\n\\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # Score each doc\\n    filtered_docs = []\\n    for d in documents:\\n        score = retrieval_grader.invoke(\\n            {\"question\": question, \"document\": d.page_content}\\n        )\\n        grade = score[\"score\"]\\n        if grade == \"yes\":\\n            print(\"---GRADE: DOCUMENT RELEVANT---\")\\n            filtered_docs.append(d)\\n        else:\\n            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\\n            continue\\n    return {\"documents\": filtered_docs, \"question\": question}\\n\\n\\ndef transform_query(state):\\n    \"\"\"\\n    Transform the query to produce a better question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        state (dict): Updates question key with a re-phrased question\\n    \"\"\"\\n\\n    print(\"---TRANSFORM QUERY---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n\\n    # Re-write question\\n    better_question = question_rewriter.invoke({\"question\": question})\\n    return {\"documents\": documents, \"question\": better_question}\\n\\n\\n### Edges\\n\\n\\ndef decide_to_generate(state):\\n    \"\"\"\\n    Determines whether to generate an answer, or re-generate a question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        str: Binary decision for next node to call\\n    \"\"\"\\n\\n    print(\"---ASSESS GRADED DOCUMENTS---\")\\n    state[\"question\"]\\n    filtered_documents = state[\"documents\"]\\n\\n    if not filtered_documents:\\n        # All documents have been filtered check_relevance\\n        # We will re-generate a new query\\n        print(\\n            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\\n        )\\n        return \"transform_query\"\\n    else:\\n        # We have relevant documents, so generate answer\\n        print(\"---DECISION: GENERATE---\")\\n        return \"generate\"\\n\\n\\ndef grade_generation_v_documents_and_question(state):\\n    \"\"\"\\n    Determines whether the generation is grounded in the document and answers question.\\n\\n    Args:\\n        state (dict): The current graph state\\n\\n    Returns:\\n        str: Decision for next node to call\\n    \"\"\"\\n\\n    print(\"---CHECK HALLUCINATIONS---\")\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n    generation = state[\"generation\"]\\n\\n    score = hallucination_grader.invoke(\\n        {\"documents\": documents, \"generation\": generation}\\n    )\\n    grade = score[\"score\"]\\n\\n    # Check hallucination\\n    if grade == \"yes\":\\n        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\\n        # Check question-answering\\n        print(\"---GRADE GENERATION vs QUESTION---\")\\n        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\\n        grade = score[\"score\"]\\n        if grade == \"yes\":\\n            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\\n            return \"useful\"\\n        else:\\n            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\\n            return \"not useful\"\\n    else:\\n        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\\n        return \"not supported\" This just follows the flow we outlined in the figure above. ---RETRIEVE---\\n\"Node \\'retrieve\\':\"\\n\\'\\\\n---\\\\n\\'\\n---CHECK DOCUMENT RELEVANCE TO QUESTION---\\n---GRADE: DOCUMENT RELEVANT---\\n---GRADE: DOCUMENT RELEVANT---\\n---GRADE: DOCUMENT RELEVANT---\\n---GRADE: DOCUMENT RELEVANT---\\n\"Node \\'grade_documents\\':\"\\n\\'\\\\n---\\\\n\\'\\n---ASSESS GRADED DOCUMENTS---\\n---DECISION: GENERATE---\\n---GENERATE---\\n\"Node \\'generate\\':\"\\n\\'\\\\n---\\\\n\\'\\n---CHECK HALLUCINATIONS---\\n---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\\n---GRADE GENERATION vs QUESTION---\\n---DECISION: GENERATION ADDRESSES QUESTION---\\n\"Node \\'__end__\\':\"\\n\\'\\\\n---\\\\n\\'\\n(\\' In a LLM-powered autonomous agent system, memory is a key component that \\'\\n \\'enables agents to store and retrieve information. There are different types \\'\\n \\'of memory in human brains, such as sensory memory which retains impressions \\'\\n \\'of sensory information for a few seconds, and long-term memory which records \\'\\n \"experiences for extended periods (Lil\\'Log, 2023). In the context of LLM \"\\n \\'agents, memory is often implemented as an external database or memory stream \\'\\n \"(Lil\\'Log, 2023). The agent can consult this memory to inform its behavior \"\\n \\'based on relevance, recency, and importance. Additionally, reflection \\'\\n \\'mechanisms synthesize memories into higher-level inferences over time and \\'\\n \"guide the agent\\'s future behavior (Lil\\'Log, 2023).\") https://smith.langchain.com/public/4163a342-5260-4852-8602-bda3f95177e7/r\\n\\n    Access time: 2024-08-13 11:20:05.493707895-04:00\\n\\n    Web Page text: Extension of Langchain for RAG. Easy benchmarking, multiple retrievals, reranker, time-aware RAG, and so on... Apache-2.0 license Marker-Inc-Korea/RAGchain This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository. 956 Commits .env.template CONTRIBUTING.md dev_requirements.txt pytest_template.ini Repository files navigation RAGchain is a framework for developing advanced RAG(Retrieval Augmented Generation) workflow powered by LLM (Large Language Model).\\nWhile existing frameworks like Langchain or LlamaIndex allow you to build simple RAG workflows, they have limitations when it comes to building complex and high-accuracy RAG workflows. RAGchain is designed to overcome these limitations by providing powerful features for building advanced RAG workflow easily.\\nAlso, it is partially compatible with Langchain, allowing you to leverage many of its integrations for vector storage,\\nembeddings, document loaders, and LLM models. Quick Install pip install RAGchain Why RAGchain? RAGchain offers several powerful features for building high-quality RAG workflows: OCR Loaders Simple file loaders may not be sufficient when trying to enhance accuracy or ingest real-world documents. OCR models can scan documents and convert them into text with high accuracy, improving the quality of responses from LLMs. Reranking is a popular method used in many research projects to improve retrieval accuracy in RAG workflows. Unlike LangChain, which doesn\\'t include reranking as a default feature, RAGChain comes with various rerankers. Great to use multiple retrievers In real-world scenarios, you may need multiple retrievers depending on your requirements. RAGchain is highly optimized for using multiple retrievers. It divides retrieval and DB. Retrieval saves vector representation of contents, and DB saves contents. We connect both with Linker, so it is really easy to use multiple retrievers and DBs. pre-made RAG pipelines We provide pre-made pipelines that let you quickly set up RAG workflow. We are planning to make much complex pipelines, which hard to make but powerful. With pipelines, you can build really powerful RAG system quickly and easily. Easy benchmarking It is crucial to benchmark and test your RAG workflows. We have easy benchmarking module for evaluation. Support your\\nown questions and various datasets. Installation simply install at pypi. From source First, clone this git repository to your local machine. git clone https://github.com/Marker-Inc-Korea/RAGchain.git Then, install RAGchain module. python3 setup.py develop For using files at root folder and test, run dev requirements. pip install dev_requirements.txt Supporting Features Advanced RAG features Time-Aware RAG Importance-Aware RAG Deepdoctection Google Search Bing Search Workflows (pipeline) Extra utils Query Decomposition Evidence Extractor Search Detector Semantic Clustering Cluster Time Compressor Dataset Evaluators KoStrategyQA DSTC11-Track5 Contributing We welcome any contributions. Please feel free to raise issues and submit pull requests. Acknowledgement This project is an early version, so it can be unstable. The project is licensed under the Apache 2.0 License. Custom properties Report repository Jan 9, 2024 + 11 releases No packages published Contributors\\n\\n    Access time: 2024-08-13 14:08:45.815433979-04:00\\n\\n    Web Page text: bm25_retrieval.py vectordb_retrieval.py\\n\\n    Access time: 2024-08-13 15:53:08.265459061-04:00\\n\\n    Web Page text: Server error We have encountered an error. Please try again later.\\n\\n    Create a summary of the users\\'s research on the topic.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='pbcopy', returncode=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(\"pbcopy\", text=True, input=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_history['thumbnail_url'] = results_history.apply(lambda row: utils.get_thumbnail_url(row['url'], row['html']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try summarizing using Local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/connorparish/miniconda3/envs/hindsight_exp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "MLX_LLM_MODEL = \"mlx-community/Meta-Llama-3.1-8B-Instruct-8bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load(MLX_LLM_MODEL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarize_prompt(html_text):\n",
    "    # prompt = f\"\"\"Below is text from a webpage.\\n {html_text}\\n \n",
    "    # Extract the key points from the webpage in relation to {topic}. Key Points:\\n\"\"\"\n",
    "    prompt = f\"\"\"Below is text from a webpage.\\n {html_text}\\n \n",
    "        Create a short bullet-point TLDR summary in relation to {topic}. Only use the \n",
    "        text provided. Summary:\\n\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_history['summary'] = results_history['html_text'].apply(lambda x: generate(model, tokenizer, prompt=get_summarize_prompt(x), max_tokens=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-13\n",
      "2024-08-06\n",
      "2024-08-05\n",
      "2024-08-03\n",
      "2024-08-02\n",
      "2024-07-30\n",
      "2024-07-02\n",
      "2024-05-01\n"
     ]
    }
   ],
   "source": [
    "def get_summary_html(row):\n",
    "    # Check for thumbnail and adjust HTML accordingly\n",
    "    thumbnail_html = \"\"\n",
    "    if row['thumbnail_url']:\n",
    "        thumbnail_html = f\"\"\"\n",
    "            <div class=\"thumbnail-container\">\n",
    "                <a href=\"{row['url']}\" target=\"_blank\">\n",
    "                    <img src=\"{row['thumbnail_url']}\" alt=\"Thumbnail for {row['title']}\" class=\"content-thumbnail\">\n",
    "                </a>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "\n",
    "    # Escape HTML special characters in the summary and convert newlines to HTML breaks\n",
    "    escaped_summary = html.escape(row['summary']).replace('\\n', '<br>')\n",
    "\n",
    "    # Combine all parts\n",
    "    html_content = f\"\"\"\n",
    "        <div class=\"content-container\" data-content-id=\"{row['id']}\">\n",
    "            {thumbnail_html}\n",
    "            <div class=\"text-container\">\n",
    "                <a href=\"{row['url']}\" target=\"_blank\" onclick=\"trackClick({row['id']});\" class=\"content-title\">{row['title']}</a>\n",
    "                <div class=\"summary\">{escaped_summary}</div>\n",
    "            </div>\n",
    "        </div>\n",
    "    \"\"\"\n",
    "    return html_content\n",
    "\n",
    "def generate_full_html(results_history, topic):\n",
    "    styles = '''\n",
    "    <style>\n",
    "        .content-container {\n",
    "            display: flex; /* Flexbox layout to align image and text side by side */\n",
    "            border-bottom: 1px solid #ccc; /* Adds a border between entries */\n",
    "            padding-bottom: 10px; /* Spacing below each item */\n",
    "            margin-bottom: 10px; /* Spacing between items */\n",
    "        }\n",
    "        .thumbnail-container {\n",
    "            flex: 0 0 auto; /* Flex item does not grow or shrink */\n",
    "            margin-right: 10px; /* Space between the image and the text */\n",
    "        }\n",
    "        .text-container {\n",
    "            flex: 1; /* Allows the text container to take up remaining space */\n",
    "        }\n",
    "        .content-thumbnail {\n",
    "            width: 100px; /* Sets a fixed width */\n",
    "            height: 100px; /* Sets a fixed height */\n",
    "            object-fit: contain; /* Ensures the image fits within dimensions without cropping */\n",
    "        }\n",
    "        .summary {\n",
    "            white-space: pre-wrap; /* Maintains whitespace formatting */\n",
    "        }\n",
    "        .date-header {\n",
    "            font-size: 18px; /* Size of date header */\n",
    "            font-weight: bold; /* Make date header bold */\n",
    "            margin-top: 20px; /* Top margin for spacing */\n",
    "            margin-bottom: 10px; /* Bottom margin before content starts */\n",
    "        }\n",
    "        .header {\n",
    "            font-size: 24px; /* Larger font size for header */\n",
    "            text-align: center; /* Center-align the header text */\n",
    "            margin: 20px 0; /* Top and bottom margin for spacing */\n",
    "        }\n",
    "    </style>\n",
    "    '''\n",
    "    header_html = f'<div class=\"header\">Topic: {topic}</div>'\n",
    "\n",
    "    summaries_html = styles + header_html\n",
    "    \n",
    "    results_history = results_history.sort_values('datetime_local', ascending=False)\n",
    "    results_history['day_accessed'] = results_history['datetime_local'].dt.date\n",
    "\n",
    "    for date in results_history.day_accessed.unique():\n",
    "        date_df = results_history.loc[results_history['day_accessed'] == date]\n",
    "        summaries_html += f'<div class=\"date-header\">Accessed on: {date}</div>'\n",
    "        for i, row in date_df.iterrows():\n",
    "            summaries_html += get_summary_html(row)\n",
    "\n",
    "    return summaries_html\n",
    "\n",
    "html_output = generate_full_html(results_history, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../data/output_summaries/\"\n",
    "with open(os.path.join(output_dir, \"agentic_rag_summary.html\"), 'w') as outfile:\n",
    "    outfile.write(html_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create summary of summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_text(row):\n",
    "    return f\"\"\"Access time: {row['datetime_local']}\\n\n",
    "    Web Page summary: {row['summary']}\\n\n",
    "    \"\"\"\n",
    "\n",
    "def get_summaries_summary_prompt(df):\n",
    "    prompt = f\"Below are summaries extracted from different webpages related to the topic {topic}. \\n\"\n",
    "    for i, row in df.iterrows():\n",
    "        prompt += get_url_text(row)\n",
    "    prompt += \"Create a summary of the below summaries focusing on {topic}. Answer: \\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = get_summaries_summary_prompt(results_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>rev_host</th>\n",
       "      <th>visit_count</th>\n",
       "      <th>hidden</th>\n",
       "      <th>typed</th>\n",
       "      <th>frecency</th>\n",
       "      <th>last_visit_date</th>\n",
       "      <th>guid</th>\n",
       "      <th>...</th>\n",
       "      <th>datetime_utc</th>\n",
       "      <th>datetime_local</th>\n",
       "      <th>html</th>\n",
       "      <th>html_text</th>\n",
       "      <th>html_f</th>\n",
       "      <th>html_f_test</th>\n",
       "      <th>thumbnail_url</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_o</th>\n",
       "      <th>day_accessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9518</th>\n",
       "      <td>10883.0</td>\n",
       "      <td>https://www.reddit.com/r/LocalLLaMA/comments/1...</td>\n",
       "      <td>What Embedding Models Are You Using For RAG? :...</td>\n",
       "      <td>moc.tidder.www.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>1.723579e+15</td>\n",
       "      <td>wU57JKVMb3nl</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-13 19:53:08.265459061+00:00</td>\n",
       "      <td>2024-08-13 15:53:08.265459061-04:00</td>\n",
       "      <td>\\n    &lt;!DOCTYPE html&gt;\\n    &lt;html lang=\"en-US\" ...</td>\n",
       "      <td>Server error We have encountered an error. Ple...</td>\n",
       "      <td>../data/history_pages/472063029123780562.html</td>\n",
       "      <td>../data/test_dir/472063029123780562.html</td>\n",
       "      <td>None</td>\n",
       "      <td>• The server encountered an error.\\n  ...</td>\n",
       "      <td>The error message indicates that the s...</td>\n",
       "      <td>2024-08-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6250</th>\n",
       "      <td>7479.0</td>\n",
       "      <td>https://github.com/Marker-Inc-Korea/RAGchain/t...</td>\n",
       "      <td>RAGchain/RAGchain/retrieval at main · Marker-I...</td>\n",
       "      <td>moc.buhtig.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>1.723573e+15</td>\n",
       "      <td>9HFVC4Dobsu0</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-13 18:08:45.815433979+00:00</td>\n",
       "      <td>2024-08-13 14:08:45.815433979-04:00</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n&lt;!DOCTYPE html&gt;\\n&lt;html\\n  lang=\"en...</td>\n",
       "      <td>bm25_retrieval.py vectordb_retrieval.py</td>\n",
       "      <td>../data/history_pages/1563417946622143631.html</td>\n",
       "      <td>../data/test_dir/1563417946622143631.html</td>\n",
       "      <td>https://opengraph.githubassets.com/455d2b4e1e9...</td>\n",
       "      <td>• The code is for two different retrie...</td>\n",
       "      <td>- The code is used for retrieval-based...</td>\n",
       "      <td>2024-08-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6233</th>\n",
       "      <td>7462.0</td>\n",
       "      <td>https://github.com/NomaDamas/RAGchain</td>\n",
       "      <td>Marker-Inc-Korea/RAGchain: Extension of Langch...</td>\n",
       "      <td>moc.buhtig.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.723562e+15</td>\n",
       "      <td>vAlqpL-OYHlL</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-13 15:20:05.493707895+00:00</td>\n",
       "      <td>2024-08-13 11:20:05.493707895-04:00</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n&lt;!DOCTYPE html&gt;\\n&lt;html\\n  lang=\"...</td>\n",
       "      <td>Extension of Langchain for RAG. Easy benchmark...</td>\n",
       "      <td>../data/history_pages/888551556676856608.html</td>\n",
       "      <td>../data/test_dir/888551556676856608.html</td>\n",
       "      <td>https://opengraph.githubassets.com/455d2b4e1e9...</td>\n",
       "      <td>RAGchain is a framework for developing...</td>\n",
       "      <td>RAGchain is a framework for developing advance...</td>\n",
       "      <td>2024-08-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11371</th>\n",
       "      <td>12736.0</td>\n",
       "      <td>https://langchain-ai.github.io/langgraph/tutor...</td>\n",
       "      <td>Self-RAG using local LLMs</td>\n",
       "      <td>oi.buhtig.ia-niahcgnal.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.722966e+15</td>\n",
       "      <td>sGf0hx8Etis-</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-06 17:46:21.157686949+00:00</td>\n",
       "      <td>2024-08-06 13:46:21.157686949-04:00</td>\n",
       "      <td>\\n&lt;!doctype html&gt;\\n&lt;html lang=\"en\" class=\"no-j...</td>\n",
       "      <td>Self RAG using local LLMs %capture --no-stderr...</td>\n",
       "      <td>../data/history_pages/1905697565505754408.html</td>\n",
       "      <td>../data/test_dir/1905697565505754408.html</td>\n",
       "      <td>https://langchain-ai.github.io/langgraph/stati...</td>\n",
       "      <td>• Agentic RAG is a framework for build...</td>\n",
       "      <td>* Agentic RAG is a framework for build...</td>\n",
       "      <td>2024-08-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11266</th>\n",
       "      <td>12631.0</td>\n",
       "      <td>https://langchain-ai.github.io/langgraph/tutor...</td>\n",
       "      <td>Self-RAG</td>\n",
       "      <td>oi.buhtig.ia-niahcgnal.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>1.722966e+15</td>\n",
       "      <td>KheTWfX378HV</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-06 17:46:04.360167980+00:00</td>\n",
       "      <td>2024-08-06 13:46:04.360167980-04:00</td>\n",
       "      <td>\\n&lt;!doctype html&gt;\\n&lt;html lang=\"en\" class=\"no-j...</td>\n",
       "      <td>Self-RAG is a strategy for RAG that incorporat...</td>\n",
       "      <td>../data/history_pages/1303557538656901825.html</td>\n",
       "      <td>../data/test_dir/1303557538656901825.html</td>\n",
       "      <td>https://langchain-ai.github.io/langgraph/stati...</td>\n",
       "      <td>• Agentic RAG is a strategy that incor...</td>\n",
       "      <td>• Agentic RAG is a strategy that incorporates ...</td>\n",
       "      <td>2024-08-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11169</th>\n",
       "      <td>12534.0</td>\n",
       "      <td>https://langchain-ai.github.io/langgraph/tutor...</td>\n",
       "      <td>Agentic RAG</td>\n",
       "      <td>oi.buhtig.ia-niahcgnal.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2039.0</td>\n",
       "      <td>1.722966e+15</td>\n",
       "      <td>6817PCBNxPht</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-06 17:43:45.099234104+00:00</td>\n",
       "      <td>2024-08-06 13:43:45.099234104-04:00</td>\n",
       "      <td>\\n&lt;!doctype html&gt;\\n&lt;html lang=\"en\" class=\"no-j...</td>\n",
       "      <td>Agent state Nodes and Edges Retrieval Agents a...</td>\n",
       "      <td>../data/history_pages/783017007441617444.html</td>\n",
       "      <td>../data/test_dir/783017007441617444.html</td>\n",
       "      <td>https://langchain-ai.github.io/langgraph/stati...</td>\n",
       "      <td>• Agentic RAG is a type of RAG that us...</td>\n",
       "      <td>• Agentic RAG is a type of RAG that us...</td>\n",
       "      <td>2024-08-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11245</th>\n",
       "      <td>12610.0</td>\n",
       "      <td>https://www.youtube.com/watch?v=fkBkNWivq-s</td>\n",
       "      <td>Autonomous RAG | The next evolution of RAG AI ...</td>\n",
       "      <td>moc.ebutuoy.www.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.722886e+15</td>\n",
       "      <td>j60uoBJTDtCJ</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-05 19:25:43.749269962+00:00</td>\n",
       "      <td>2024-08-05 15:25:43.749269962-04:00</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;html style=\"font-size: 10px;fo...</td>\n",
       "      <td>Policy &amp; Safety How YouTube works Test new fea...</td>\n",
       "      <td>../data/history_pages/1941920220060162641.html</td>\n",
       "      <td>../data/test_dir/1941920220060162641.html</td>\n",
       "      <td>https://i.ytimg.com/vi/fkBkNWivq-s/hqdefault.jpg</td>\n",
       "      <td>• YouTube is a platform that works in ...</td>\n",
       "      <td>\\n  • YouTube is a video-sharing platform own...</td>\n",
       "      <td>2024-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11251</th>\n",
       "      <td>12616.0</td>\n",
       "      <td>https://www.perplexity.ai/search/i-want-to-bui...</td>\n",
       "      <td>I want to build an agentic rag system allowing...</td>\n",
       "      <td>ia.ytixelprep.www.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.722886e+15</td>\n",
       "      <td>E9ki-qZGGx0h</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-05 19:29:57.355839014+00:00</td>\n",
       "      <td>2024-08-05 15:29:57.355839014-04:00</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;html lang=\"en-US\"&gt;&lt;head&gt;&lt;title...</td>\n",
       "      <td>Enable JavaScript and cookies to continue</td>\n",
       "      <td>../data/history_pages/888927757549760350.html</td>\n",
       "      <td>../data/test_dir/888927757549760350.html</td>\n",
       "      <td>None</td>\n",
       "      <td>Enable JavaScript and cookies to conti...</td>\n",
       "      <td>TLDR: Agentic rag is a type of rag tha...</td>\n",
       "      <td>2024-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11264</th>\n",
       "      <td>12629.0</td>\n",
       "      <td>https://langchain-ai.github.io/langgraph/tutor...</td>\n",
       "      <td>Adaptive RAG</td>\n",
       "      <td>oi.buhtig.ia-niahcgnal.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.722887e+15</td>\n",
       "      <td>elF-rJO3D9BK</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-05 19:43:58.414294004+00:00</td>\n",
       "      <td>2024-08-05 15:43:58.414294004-04:00</td>\n",
       "      <td>\\n&lt;!doctype html&gt;\\n&lt;html lang=\"en\" class=\"no-j...</td>\n",
       "      <td>Adaptive RAG Initializing search How-to Guides...</td>\n",
       "      <td>../data/history_pages/1584516194633218520.html</td>\n",
       "      <td>../data/test_dir/1584516194633218520.html</td>\n",
       "      <td>https://langchain-ai.github.io/langgraph/stati...</td>\n",
       "      <td>Agentic RAG is a strategy for RAG that...</td>\n",
       "      <td>* Agentic RAG is a strategy for RAG th...</td>\n",
       "      <td>2024-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11265</th>\n",
       "      <td>12630.0</td>\n",
       "      <td>https://langchain-ai.github.io/langgraph/tutor...</td>\n",
       "      <td>Adaptive RAG using local LLMs</td>\n",
       "      <td>oi.buhtig.ia-niahcgnal.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.722887e+15</td>\n",
       "      <td>54FlEPfqKrTg</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-05 19:44:02.906191111+00:00</td>\n",
       "      <td>2024-08-05 15:44:02.906191111-04:00</td>\n",
       "      <td>\\n&lt;!doctype html&gt;\\n&lt;html lang=\"en\" class=\"no-j...</td>\n",
       "      <td>%capture --no-stderr\\n%pip install -U langchai...</td>\n",
       "      <td>../data/history_pages/1397372920769757798.html</td>\n",
       "      <td>../data/test_dir/1397372920769757798.html</td>\n",
       "      <td>https://langchain-ai.github.io/langgraph/stati...</td>\n",
       "      <td>• AlphaCodium is a new approach for co...</td>\n",
       "      <td>The AlphaCodium paper introduces a new...</td>\n",
       "      <td>2024-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11263</th>\n",
       "      <td>12628.0</td>\n",
       "      <td>https://langchain-ai.github.io/langgraph/tutor...</td>\n",
       "      <td>Corrective RAG (CRAG) using local LLMs</td>\n",
       "      <td>oi.buhtig.ia-niahcgnal.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>1.722887e+15</td>\n",
       "      <td>3cr-900hUgw2</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-05 19:44:07.620565891+00:00</td>\n",
       "      <td>2024-08-05 15:44:07.620565891-04:00</td>\n",
       "      <td>\\n&lt;!doctype html&gt;\\n&lt;html lang=\"en\" class=\"no-j...</td>\n",
       "      <td>Table of contents Corrective-RAG (CRAG) is a s...</td>\n",
       "      <td>../data/history_pages/1447630539588103606.html</td>\n",
       "      <td>../data/test_dir/1447630539588103606.html</td>\n",
       "      <td>https://langchain-ai.github.io/langgraph/stati...</td>\n",
       "      <td>• Agentic RAG is a framework for build...</td>\n",
       "      <td>\\n        *   Agentic RAG is a framework for ...</td>\n",
       "      <td>2024-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11196</th>\n",
       "      <td>12561.0</td>\n",
       "      <td>https://chatgpt.com/c/68874f72-7a98-4417-a7d9-...</td>\n",
       "      <td>Agentic RAG System Setup</td>\n",
       "      <td>moc.tpgtahc.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1.722897e+15</td>\n",
       "      <td>JOs1YNX4MWO7</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-05 22:23:33.017527103+00:00</td>\n",
       "      <td>2024-08-05 18:23:33.017527103-04:00</td>\n",
       "      <td>&lt;html&gt;\\n  &lt;head&gt;\\n    &lt;meta name=\"viewport\" co...</td>\n",
       "      <td>Please turn JavaScript on and reload the page....</td>\n",
       "      <td>../data/history_pages/694291293255622745.html</td>\n",
       "      <td>../data/test_dir/694291293255622745.html</td>\n",
       "      <td>None</td>\n",
       "      <td>• JavaScript needs to be enabled to vi...</td>\n",
       "      <td>Agentic rag is a type of rag that is c...</td>\n",
       "      <td>2024-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11191</th>\n",
       "      <td>12556.0</td>\n",
       "      <td>https://medium.com/@infiniflowai/agentic-rag-d...</td>\n",
       "      <td>Agentic RAG: Definition and Low-code Implement...</td>\n",
       "      <td>moc.muidem.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.722712e+15</td>\n",
       "      <td>JVNBhZoZHXcN</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-03 19:11:54.438225985+00:00</td>\n",
       "      <td>2024-08-03 15:11:54.438225985-04:00</td>\n",
       "      <td>&lt;!doctype html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;title da...</td>\n",
       "      <td>Agentic RAG: Definition and Low-code Implement...</td>\n",
       "      <td>../data/history_pages/1668270784507309558.html</td>\n",
       "      <td>../data/test_dir/1668270784507309558.html</td>\n",
       "      <td>https://miro.medium.com/v2/resize:fit:1200/1*J...</td>\n",
       "      <td>• Agentic RAG is an agent-based RAG th...</td>\n",
       "      <td>Agentic RAG is an advanced form of RAG...</td>\n",
       "      <td>2024-08-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11170</th>\n",
       "      <td>12535.0</td>\n",
       "      <td>https://medium.com/@bijit211987/agentic-rag-81...</td>\n",
       "      <td>Agentic RAG. Alright, let’s get straight to th...</td>\n",
       "      <td>moc.muidem.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.722628e+15</td>\n",
       "      <td>2uh1U6qLrD5I</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-02 19:41:51.501085043+00:00</td>\n",
       "      <td>2024-08-02 15:41:51.501085043-04:00</td>\n",
       "      <td>&lt;!doctype html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;title da...</td>\n",
       "      <td>Open in app Bijit Ghosh Apr 14, 2024 Alright, ...</td>\n",
       "      <td>../data/history_pages/1128041076671617201.html</td>\n",
       "      <td>../data/test_dir/1128041076671617201.html</td>\n",
       "      <td>https://miro.medium.com/v2/resize:fit:1039/1*W...</td>\n",
       "      <td>Agentic RAG is a paradigm shift in lan...</td>\n",
       "      <td>\\n  • Agentic RAG is a paradigm shift in lang...</td>\n",
       "      <td>2024-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11168</th>\n",
       "      <td>12533.0</td>\n",
       "      <td>https://www.leewayhertz.com/agentic-rag/</td>\n",
       "      <td>Agentic RAG: What it is, its types, applicatio...</td>\n",
       "      <td>moc.ztrehyaweel.www.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.722627e+15</td>\n",
       "      <td>OVt3xSP1T-kK</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-08-02 19:30:41.140996933+00:00</td>\n",
       "      <td>2024-08-02 15:30:41.140996933-04:00</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en-US\"&gt;\\n&lt;head&gt;&lt;m...</td>\n",
       "      <td>AI PRODUCTS Enterprise GenAI Platform AI Copil...</td>\n",
       "      <td>../data/history_pages/1388658885853707355.html</td>\n",
       "      <td>../data/test_dir/1388658885853707355.html</td>\n",
       "      <td>https://d3lkc3n5th01x7.cloudfront.net/wp-conte...</td>\n",
       "      <td>\\n        Agentic RAG is a transformative app...</td>\n",
       "      <td>\\n        Agentic RAG is a transformative app...</td>\n",
       "      <td>2024-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10942</th>\n",
       "      <td>12307.0</td>\n",
       "      <td>https://docs.exa.ai/reference/getting-started-...</td>\n",
       "      <td>Getting Started with RAG in Python</td>\n",
       "      <td>ia.axe.scod.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.722376e+15</td>\n",
       "      <td>QTegvH5imxX4</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-07-30 21:48:34.856149912+00:00</td>\n",
       "      <td>2024-07-30 17:48:34.856149912-04:00</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;html lang=\"en\" style=\"\" data-c...</td>\n",
       "      <td>Jump to Content Getting Started with RAG in Py...</td>\n",
       "      <td>../data/history_pages/1110325031299530837.html</td>\n",
       "      <td>../data/test_dir/1110325031299530837.html</td>\n",
       "      <td>https://files.readme.io/39ec0bb-small-logo.png</td>\n",
       "      <td>\\n        This is a summary of the top AI sta...</td>\n",
       "      <td>\\n        *   Set up OpenAI and pass the Exa ...</td>\n",
       "      <td>2024-07-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9519</th>\n",
       "      <td>10884.0</td>\n",
       "      <td>https://www.reddit.com/r/LocalLLaMA/comments/1...</td>\n",
       "      <td>Best 7B LLMs / Embeddings for RAG? : r/LocalLLaMA</td>\n",
       "      <td>moc.tidder.www.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.719934e+15</td>\n",
       "      <td>hDaANnNmx8_F</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-07-02 15:30:15.643182039+00:00</td>\n",
       "      <td>2024-07-02 11:30:15.643182039-04:00</td>\n",
       "      <td>\\n    &lt;!DOCTYPE html&gt;\\n    &lt;html lang=\"en-US\" ...</td>\n",
       "      <td>Skip to main content Open navigation Go to Red...</td>\n",
       "      <td>../data/history_pages/1045585297866113232.html</td>\n",
       "      <td>../data/test_dir/1045585297866113232.html</td>\n",
       "      <td>None</td>\n",
       "      <td>The user is asking for recommendations...</td>\n",
       "      <td>\\n  • The user is using RAG with a 7B LLM (Mi...</td>\n",
       "      <td>2024-07-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6249</th>\n",
       "      <td>7478.0</td>\n",
       "      <td>https://github.com/Marker-Inc-Korea/RAGchain/t...</td>\n",
       "      <td>RAGchain/RAGchain at main · Marker-Inc-Korea/R...</td>\n",
       "      <td>moc.buhtig.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.714593e+15</td>\n",
       "      <td>3mZ5u4ZqrMtq</td>\n",
       "      <td>...</td>\n",
       "      <td>2024-05-01 19:45:13.067468882+00:00</td>\n",
       "      <td>2024-05-01 15:45:13.067468882-04:00</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n&lt;!DOCTYPE html&gt;\\n&lt;html\\n  lang=\"en...</td>\n",
       "      <td>Skip to content Navigation Menu Toggle navigat...</td>\n",
       "      <td>../data/history_pages/419566124378497773.html</td>\n",
       "      <td>../data/test_dir/419566124378497773.html</td>\n",
       "      <td>https://opengraph.githubassets.com/455d2b4e1e9...</td>\n",
       "      <td>This repository has been archived by t...</td>\n",
       "      <td>\\n  • Agentic rag is a type of ragtime music ...</td>\n",
       "      <td>2024-05-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                                url  \\\n",
       "9518   10883.0  https://www.reddit.com/r/LocalLLaMA/comments/1...   \n",
       "6250    7479.0  https://github.com/Marker-Inc-Korea/RAGchain/t...   \n",
       "6233    7462.0              https://github.com/NomaDamas/RAGchain   \n",
       "11371  12736.0  https://langchain-ai.github.io/langgraph/tutor...   \n",
       "11266  12631.0  https://langchain-ai.github.io/langgraph/tutor...   \n",
       "11169  12534.0  https://langchain-ai.github.io/langgraph/tutor...   \n",
       "11245  12610.0        https://www.youtube.com/watch?v=fkBkNWivq-s   \n",
       "11251  12616.0  https://www.perplexity.ai/search/i-want-to-bui...   \n",
       "11264  12629.0  https://langchain-ai.github.io/langgraph/tutor...   \n",
       "11265  12630.0  https://langchain-ai.github.io/langgraph/tutor...   \n",
       "11263  12628.0  https://langchain-ai.github.io/langgraph/tutor...   \n",
       "11196  12561.0  https://chatgpt.com/c/68874f72-7a98-4417-a7d9-...   \n",
       "11191  12556.0  https://medium.com/@infiniflowai/agentic-rag-d...   \n",
       "11170  12535.0  https://medium.com/@bijit211987/agentic-rag-81...   \n",
       "11168  12533.0           https://www.leewayhertz.com/agentic-rag/   \n",
       "10942  12307.0  https://docs.exa.ai/reference/getting-started-...   \n",
       "9519   10884.0  https://www.reddit.com/r/LocalLLaMA/comments/1...   \n",
       "6249    7478.0  https://github.com/Marker-Inc-Korea/RAGchain/t...   \n",
       "\n",
       "                                                   title  \\\n",
       "9518   What Embedding Models Are You Using For RAG? :...   \n",
       "6250   RAGchain/RAGchain/retrieval at main · Marker-I...   \n",
       "6233   Marker-Inc-Korea/RAGchain: Extension of Langch...   \n",
       "11371                          Self-RAG using local LLMs   \n",
       "11266                                           Self-RAG   \n",
       "11169                                        Agentic RAG   \n",
       "11245  Autonomous RAG | The next evolution of RAG AI ...   \n",
       "11251  I want to build an agentic rag system allowing...   \n",
       "11264                                       Adaptive RAG   \n",
       "11265                      Adaptive RAG using local LLMs   \n",
       "11263             Corrective RAG (CRAG) using local LLMs   \n",
       "11196                           Agentic RAG System Setup   \n",
       "11191  Agentic RAG: Definition and Low-code Implement...   \n",
       "11170  Agentic RAG. Alright, let’s get straight to th...   \n",
       "11168  Agentic RAG: What it is, its types, applicatio...   \n",
       "10942                 Getting Started with RAG in Python   \n",
       "9519   Best 7B LLMs / Embeddings for RAG? : r/LocalLLaMA   \n",
       "6249   RAGchain/RAGchain at main · Marker-Inc-Korea/R...   \n",
       "\n",
       "                      rev_host  visit_count  hidden  typed  frecency  \\\n",
       "9518           moc.tidder.www.          2.0     0.0    0.0     127.0   \n",
       "6250               moc.buhtig.          2.0     0.0    0.0     107.0   \n",
       "6233               moc.buhtig.          3.0     0.0    0.0      37.0   \n",
       "11371  oi.buhtig.ia-niahcgnal.          1.0     0.0    0.0      94.0   \n",
       "11266  oi.buhtig.ia-niahcgnal.          2.0     0.0    0.0     185.0   \n",
       "11169  oi.buhtig.ia-niahcgnal.          3.0     0.0    1.0    2039.0   \n",
       "11245         moc.ebutuoy.www.          1.0     0.0    0.0      94.0   \n",
       "11251       ia.ytixelprep.www.          1.0     0.0    0.0      94.0   \n",
       "11264  oi.buhtig.ia-niahcgnal.          1.0     0.0    0.0      94.0   \n",
       "11265  oi.buhtig.ia-niahcgnal.          1.0     0.0    0.0      94.0   \n",
       "11263  oi.buhtig.ia-niahcgnal.          2.0     0.0    0.0     185.0   \n",
       "11196             moc.tpgtahc.          2.0     0.0    0.0     131.0   \n",
       "11191              moc.muidem.          1.0     0.0    0.0      92.0   \n",
       "11170              moc.muidem.          1.0     0.0    0.0      92.0   \n",
       "11168     moc.ztrehyaweel.www.          1.0     0.0    0.0      92.0   \n",
       "10942             ia.axe.scod.          1.0     0.0    0.0      88.0   \n",
       "9519           moc.tidder.www.          1.0     0.0    0.0      20.0   \n",
       "6249               moc.buhtig.          2.0     0.0    0.0      52.0   \n",
       "\n",
       "       last_visit_date          guid  ...                        datetime_utc  \\\n",
       "9518      1.723579e+15  wU57JKVMb3nl  ... 2024-08-13 19:53:08.265459061+00:00   \n",
       "6250      1.723573e+15  9HFVC4Dobsu0  ... 2024-08-13 18:08:45.815433979+00:00   \n",
       "6233      1.723562e+15  vAlqpL-OYHlL  ... 2024-08-13 15:20:05.493707895+00:00   \n",
       "11371     1.722966e+15  sGf0hx8Etis-  ... 2024-08-06 17:46:21.157686949+00:00   \n",
       "11266     1.722966e+15  KheTWfX378HV  ... 2024-08-06 17:46:04.360167980+00:00   \n",
       "11169     1.722966e+15  6817PCBNxPht  ... 2024-08-06 17:43:45.099234104+00:00   \n",
       "11245     1.722886e+15  j60uoBJTDtCJ  ... 2024-08-05 19:25:43.749269962+00:00   \n",
       "11251     1.722886e+15  E9ki-qZGGx0h  ... 2024-08-05 19:29:57.355839014+00:00   \n",
       "11264     1.722887e+15  elF-rJO3D9BK  ... 2024-08-05 19:43:58.414294004+00:00   \n",
       "11265     1.722887e+15  54FlEPfqKrTg  ... 2024-08-05 19:44:02.906191111+00:00   \n",
       "11263     1.722887e+15  3cr-900hUgw2  ... 2024-08-05 19:44:07.620565891+00:00   \n",
       "11196     1.722897e+15  JOs1YNX4MWO7  ... 2024-08-05 22:23:33.017527103+00:00   \n",
       "11191     1.722712e+15  JVNBhZoZHXcN  ... 2024-08-03 19:11:54.438225985+00:00   \n",
       "11170     1.722628e+15  2uh1U6qLrD5I  ... 2024-08-02 19:41:51.501085043+00:00   \n",
       "11168     1.722627e+15  OVt3xSP1T-kK  ... 2024-08-02 19:30:41.140996933+00:00   \n",
       "10942     1.722376e+15  QTegvH5imxX4  ... 2024-07-30 21:48:34.856149912+00:00   \n",
       "9519      1.719934e+15  hDaANnNmx8_F  ... 2024-07-02 15:30:15.643182039+00:00   \n",
       "6249      1.714593e+15  3mZ5u4ZqrMtq  ... 2024-05-01 19:45:13.067468882+00:00   \n",
       "\n",
       "                           datetime_local  \\\n",
       "9518  2024-08-13 15:53:08.265459061-04:00   \n",
       "6250  2024-08-13 14:08:45.815433979-04:00   \n",
       "6233  2024-08-13 11:20:05.493707895-04:00   \n",
       "11371 2024-08-06 13:46:21.157686949-04:00   \n",
       "11266 2024-08-06 13:46:04.360167980-04:00   \n",
       "11169 2024-08-06 13:43:45.099234104-04:00   \n",
       "11245 2024-08-05 15:25:43.749269962-04:00   \n",
       "11251 2024-08-05 15:29:57.355839014-04:00   \n",
       "11264 2024-08-05 15:43:58.414294004-04:00   \n",
       "11265 2024-08-05 15:44:02.906191111-04:00   \n",
       "11263 2024-08-05 15:44:07.620565891-04:00   \n",
       "11196 2024-08-05 18:23:33.017527103-04:00   \n",
       "11191 2024-08-03 15:11:54.438225985-04:00   \n",
       "11170 2024-08-02 15:41:51.501085043-04:00   \n",
       "11168 2024-08-02 15:30:41.140996933-04:00   \n",
       "10942 2024-07-30 17:48:34.856149912-04:00   \n",
       "9519  2024-07-02 11:30:15.643182039-04:00   \n",
       "6249  2024-05-01 15:45:13.067468882-04:00   \n",
       "\n",
       "                                                    html  \\\n",
       "9518   \\n    <!DOCTYPE html>\\n    <html lang=\"en-US\" ...   \n",
       "6250   \\n\\n\\n\\n\\n\\n<!DOCTYPE html>\\n<html\\n  lang=\"en...   \n",
       "6233   \\n\\n\\n\\n\\n\\n\\n<!DOCTYPE html>\\n<html\\n  lang=\"...   \n",
       "11371  \\n<!doctype html>\\n<html lang=\"en\" class=\"no-j...   \n",
       "11266  \\n<!doctype html>\\n<html lang=\"en\" class=\"no-j...   \n",
       "11169  \\n<!doctype html>\\n<html lang=\"en\" class=\"no-j...   \n",
       "11245  <!DOCTYPE html><html style=\"font-size: 10px;fo...   \n",
       "11251  <!DOCTYPE html><html lang=\"en-US\"><head><title...   \n",
       "11264  \\n<!doctype html>\\n<html lang=\"en\" class=\"no-j...   \n",
       "11265  \\n<!doctype html>\\n<html lang=\"en\" class=\"no-j...   \n",
       "11263  \\n<!doctype html>\\n<html lang=\"en\" class=\"no-j...   \n",
       "11196  <html>\\n  <head>\\n    <meta name=\"viewport\" co...   \n",
       "11191  <!doctype html><html lang=\"en\"><head><title da...   \n",
       "11170  <!doctype html><html lang=\"en\"><head><title da...   \n",
       "11168  <!DOCTYPE html>\\n<html lang=\"en-US\">\\n<head><m...   \n",
       "10942  <!DOCTYPE html><html lang=\"en\" style=\"\" data-c...   \n",
       "9519   \\n    <!DOCTYPE html>\\n    <html lang=\"en-US\" ...   \n",
       "6249   \\n\\n\\n\\n\\n\\n<!DOCTYPE html>\\n<html\\n  lang=\"en...   \n",
       "\n",
       "                                               html_text  \\\n",
       "9518   Server error We have encountered an error. Ple...   \n",
       "6250             bm25_retrieval.py vectordb_retrieval.py   \n",
       "6233   Extension of Langchain for RAG. Easy benchmark...   \n",
       "11371  Self RAG using local LLMs %capture --no-stderr...   \n",
       "11266  Self-RAG is a strategy for RAG that incorporat...   \n",
       "11169  Agent state Nodes and Edges Retrieval Agents a...   \n",
       "11245  Policy & Safety How YouTube works Test new fea...   \n",
       "11251          Enable JavaScript and cookies to continue   \n",
       "11264  Adaptive RAG Initializing search How-to Guides...   \n",
       "11265  %capture --no-stderr\\n%pip install -U langchai...   \n",
       "11263  Table of contents Corrective-RAG (CRAG) is a s...   \n",
       "11196  Please turn JavaScript on and reload the page....   \n",
       "11191  Agentic RAG: Definition and Low-code Implement...   \n",
       "11170  Open in app Bijit Ghosh Apr 14, 2024 Alright, ...   \n",
       "11168  AI PRODUCTS Enterprise GenAI Platform AI Copil...   \n",
       "10942  Jump to Content Getting Started with RAG in Py...   \n",
       "9519   Skip to main content Open navigation Go to Red...   \n",
       "6249   Skip to content Navigation Menu Toggle navigat...   \n",
       "\n",
       "                                               html_f  \\\n",
       "9518    ../data/history_pages/472063029123780562.html   \n",
       "6250   ../data/history_pages/1563417946622143631.html   \n",
       "6233    ../data/history_pages/888551556676856608.html   \n",
       "11371  ../data/history_pages/1905697565505754408.html   \n",
       "11266  ../data/history_pages/1303557538656901825.html   \n",
       "11169   ../data/history_pages/783017007441617444.html   \n",
       "11245  ../data/history_pages/1941920220060162641.html   \n",
       "11251   ../data/history_pages/888927757549760350.html   \n",
       "11264  ../data/history_pages/1584516194633218520.html   \n",
       "11265  ../data/history_pages/1397372920769757798.html   \n",
       "11263  ../data/history_pages/1447630539588103606.html   \n",
       "11196   ../data/history_pages/694291293255622745.html   \n",
       "11191  ../data/history_pages/1668270784507309558.html   \n",
       "11170  ../data/history_pages/1128041076671617201.html   \n",
       "11168  ../data/history_pages/1388658885853707355.html   \n",
       "10942  ../data/history_pages/1110325031299530837.html   \n",
       "9519   ../data/history_pages/1045585297866113232.html   \n",
       "6249    ../data/history_pages/419566124378497773.html   \n",
       "\n",
       "                                     html_f_test  \\\n",
       "9518    ../data/test_dir/472063029123780562.html   \n",
       "6250   ../data/test_dir/1563417946622143631.html   \n",
       "6233    ../data/test_dir/888551556676856608.html   \n",
       "11371  ../data/test_dir/1905697565505754408.html   \n",
       "11266  ../data/test_dir/1303557538656901825.html   \n",
       "11169   ../data/test_dir/783017007441617444.html   \n",
       "11245  ../data/test_dir/1941920220060162641.html   \n",
       "11251   ../data/test_dir/888927757549760350.html   \n",
       "11264  ../data/test_dir/1584516194633218520.html   \n",
       "11265  ../data/test_dir/1397372920769757798.html   \n",
       "11263  ../data/test_dir/1447630539588103606.html   \n",
       "11196   ../data/test_dir/694291293255622745.html   \n",
       "11191  ../data/test_dir/1668270784507309558.html   \n",
       "11170  ../data/test_dir/1128041076671617201.html   \n",
       "11168  ../data/test_dir/1388658885853707355.html   \n",
       "10942  ../data/test_dir/1110325031299530837.html   \n",
       "9519   ../data/test_dir/1045585297866113232.html   \n",
       "6249    ../data/test_dir/419566124378497773.html   \n",
       "\n",
       "                                           thumbnail_url  \\\n",
       "9518                                                None   \n",
       "6250   https://opengraph.githubassets.com/455d2b4e1e9...   \n",
       "6233   https://opengraph.githubassets.com/455d2b4e1e9...   \n",
       "11371  https://langchain-ai.github.io/langgraph/stati...   \n",
       "11266  https://langchain-ai.github.io/langgraph/stati...   \n",
       "11169  https://langchain-ai.github.io/langgraph/stati...   \n",
       "11245   https://i.ytimg.com/vi/fkBkNWivq-s/hqdefault.jpg   \n",
       "11251                                               None   \n",
       "11264  https://langchain-ai.github.io/langgraph/stati...   \n",
       "11265  https://langchain-ai.github.io/langgraph/stati...   \n",
       "11263  https://langchain-ai.github.io/langgraph/stati...   \n",
       "11196                                               None   \n",
       "11191  https://miro.medium.com/v2/resize:fit:1200/1*J...   \n",
       "11170  https://miro.medium.com/v2/resize:fit:1039/1*W...   \n",
       "11168  https://d3lkc3n5th01x7.cloudfront.net/wp-conte...   \n",
       "10942     https://files.readme.io/39ec0bb-small-logo.png   \n",
       "9519                                                None   \n",
       "6249   https://opengraph.githubassets.com/455d2b4e1e9...   \n",
       "\n",
       "                                                 summary  \\\n",
       "9518           • The server encountered an error.\\n  ...   \n",
       "6250           • The code is for two different retrie...   \n",
       "6233           RAGchain is a framework for developing...   \n",
       "11371          • Agentic RAG is a framework for build...   \n",
       "11266          • Agentic RAG is a strategy that incor...   \n",
       "11169          • Agentic RAG is a type of RAG that us...   \n",
       "11245          • YouTube is a platform that works in ...   \n",
       "11251          Enable JavaScript and cookies to conti...   \n",
       "11264          Agentic RAG is a strategy for RAG that...   \n",
       "11265          • AlphaCodium is a new approach for co...   \n",
       "11263          • Agentic RAG is a framework for build...   \n",
       "11196          • JavaScript needs to be enabled to vi...   \n",
       "11191          • Agentic RAG is an agent-based RAG th...   \n",
       "11170          Agentic RAG is a paradigm shift in lan...   \n",
       "11168   \\n        Agentic RAG is a transformative app...   \n",
       "10942   \\n        This is a summary of the top AI sta...   \n",
       "9519           The user is asking for recommendations...   \n",
       "6249           This repository has been archived by t...   \n",
       "\n",
       "                                               summary_o  day_accessed  \n",
       "9518           The error message indicates that the s...    2024-08-13  \n",
       "6250           - The code is used for retrieval-based...    2024-08-13  \n",
       "6233   RAGchain is a framework for developing advance...    2024-08-13  \n",
       "11371          * Agentic RAG is a framework for build...    2024-08-06  \n",
       "11266  • Agentic RAG is a strategy that incorporates ...    2024-08-06  \n",
       "11169          • Agentic RAG is a type of RAG that us...    2024-08-06  \n",
       "11245   \\n  • YouTube is a video-sharing platform own...    2024-08-05  \n",
       "11251          TLDR: Agentic rag is a type of rag tha...    2024-08-05  \n",
       "11264          * Agentic RAG is a strategy for RAG th...    2024-08-05  \n",
       "11265          The AlphaCodium paper introduces a new...    2024-08-05  \n",
       "11263   \\n        *   Agentic RAG is a framework for ...    2024-08-05  \n",
       "11196          Agentic rag is a type of rag that is c...    2024-08-05  \n",
       "11191          Agentic RAG is an advanced form of RAG...    2024-08-03  \n",
       "11170   \\n  • Agentic RAG is a paradigm shift in lang...    2024-08-02  \n",
       "11168   \\n        Agentic RAG is a transformative app...    2024-08-02  \n",
       "10942   \\n        *   Set up OpenAI and pass the Exa ...    2024-07-30  \n",
       "9519    \\n  • The user is using RAG with a 7B LLM (Mi...    2024-07-02  \n",
       "6249    \\n  • Agentic rag is a type of ragtime music ...    2024-05-01  \n",
       "\n",
       "[18 rows x 32 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hindsight_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
